{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2a0453",
   "metadata": {},
   "source": [
    "# 5.1 - Introdução ao BERT: Pré-treinamento e Arquitetura\n",
    "\n",
    "Na aula anterior, exploramos detalhadamente o **Transformer**, arquitetura proposta no artigo *Attention is All You Need (Vaswani et al., 2017)*.  \n",
    "Vimos como o modelo substitui as recorrências por **atenção auto-regressiva**, resultando em paralelização e aprendizado mais eficiente de dependências de longo alcance.\n",
    "\n",
    "O **BERT (Bidirectional Encoder Representations from Transformers)**, proposto por **Devlin et al. (2018)**, é um passo além dessa ideia.  \n",
    "Ele se baseia **exclusivamente na parte do encoder do Transformer**, mas introduz um novo paradigma: **aprendizado de representações de linguagem profundas e bidirecionais**, usando **pré-treinamento auto-supervisionado**.\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/bert_pt_ft.png\" width=\"50%%\">\n",
    "\n",
    "Fonte: Devlin et al. (2018)\n",
    "\n",
    "---\n",
    "\n",
    "## Por que BERT?\n",
    "\n",
    "Antes do BERT, os modelos de linguagem típicos (como GPT-1 ou ELMo) processavam texto **em apenas uma direção**:\n",
    "\n",
    "- Modelos *left-to-right* (como GPT): preveem o próximo token a partir dos anteriores.  \n",
    "- Modelos *right-to-left* (como LM reversos): preveem o token anterior.  \n",
    "- ELMo (2018) combinava duas redes LSTM unidirecionais, mas de forma ainda limitada.\n",
    "\n",
    "Essas abordagens **não capturavam o contexto completo** de uma palavra em sua sentença, já que a representação dependia apenas do passado ou do futuro, mas não dos dois **ao mesmo tempo**.\n",
    "\n",
    "O BERT resolve isso ao:\n",
    "- Treinar **bidirecionalmente**, olhando para a **sentença inteira**;  \n",
    "- Usar um esquema de pré-treinamento **auto-supervisionado** (sem rótulos humanos);  \n",
    "- E depois **ajustar** o modelo (*fine-tuning*) para tarefas específicas de NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## Transfer Learning em NLP\n",
    "\n",
    "Assim como o *ImageNet* revolucionou a visão computacional com o conceito de **pré-treinamento + fine-tuning**, o BERT trouxe essa revolução para o **Processamento de Linguagem Natural (PLN)**.\n",
    "\n",
    "O processo segue duas fases:\n",
    "\n",
    "1. **Pré-treinamento (Pretraining)**  \n",
    "   - O modelo aprende conhecimento linguístico geral a partir de **grandes corpora** (Wikipedia + BooksCorpus).  \n",
    "   - Duas tarefas auto-supervisionadas são usadas:\n",
    "     - **Masked Language Modeling (MLM)**: prever palavras mascaradas no texto.  \n",
    "     - **Next Sentence Prediction (NSP)**: prever se uma sentença B segue naturalmente uma sentença A.\n",
    "\n",
    "2. **Ajuste fino (Fine-tuning)**  \n",
    "   - O modelo é adaptado para uma tarefa específica: classificação, QA, NER, etc.  \n",
    "   - Os mesmos pesos são reutilizados, e apenas algumas camadas finais são ajustadas.\n",
    "\n",
    "Essa combinação tornou o BERT um **modelo universal de linguagem**, facilmente adaptável a diversas tarefas de NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuição: O que o BERT aprende?\n",
    "\n",
    "Durante o pré-treinamento, o BERT **aprende as relações contextuais** entre as palavras:\n",
    "- Como palavras diferentes se associam em frases;  \n",
    "- Que estrutura sintática e semântica definem o sentido;  \n",
    "- Que palavras são prováveis em determinados contextos.\n",
    "\n",
    "Com isso, ele gera **embeddings contextuais**, ou seja, o vetor de uma palavra depende da **sentença em que ela aparece**.\n",
    "\n",
    "> Exemplo:  \n",
    "> - Em “Ele foi ao **banco** sacar dinheiro”, “banco” → instituição financeira.  \n",
    "> - Em “Ela sentou no **banco** do parque”, “banco” → assento.  \n",
    ">\n",
    "> O BERT gera embeddings diferentes para cada uso.\n",
    "\n",
    "---\n",
    "\n",
    "## Arquitetura Geral\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/bert_size_architecture.png\" width=\"50%%\">\n",
    "\n",
    "Fonte: [Huggingface](https://huggingface.co/blog/bert-101)\n",
    "\n",
    "O BERT baseia-se **no encoder do Transformer**, repetido *N* vezes.  \n",
    "Cada camada contém:\n",
    "\n",
    "1. **Multi-Head Self-Attention**  \n",
    "   - Permite que cada token atenda a todos os outros tokens da sentença, em ambas as direções.  \n",
    "   - É aqui que o “Bidirectional” de BERT acontece.\n",
    "\n",
    "2. **Feed-Forward Network (FFN)**  \n",
    "   - Aplica transformações não lineares em cada posição, aprendendo representações mais ricas.\n",
    "\n",
    "3. **Residual Connections + Layer Normalization**  \n",
    "   - Facilitam o fluxo de gradientes e a estabilidade do treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "## Configurações originais\n",
    "\n",
    "| Modelo | Camadas (L) | Heads | Dimensão (`d_model`) | Dim. Feedforward | Parâmetros |\n",
    "|:--------|:------------:|:------:|:--------------------:|:----------------:|:-----------:|\n",
    "| **BERT Base** | 12 | 12 | 768 | 3072 | 110M |\n",
    "| **BERT Large** | 24 | 16 | 1024 | 4096 | 340M |\n",
    "\n",
    "---\n",
    "\n",
    "## Entrada do BERT\n",
    "\n",
    "O BERT processa o texto em forma de tokens especiais:\n",
    "\n",
    "- **[CLS]** → marca o início da sequência (usado para classificação).  \n",
    "- **[SEP]** → separa sentenças (usado em tarefas de pares).  \n",
    "- **[MASK]** → substitui tokens que o modelo deve prever.\n",
    "\n",
    "> Exemplo:  \n",
    "> Entrada: `[CLS] O gato [MASK] no tapete [SEP]`  \n",
    "> Saída: prever a palavra “dormiu”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0df512",
   "metadata": {},
   "source": [
    "# 5.2 - Objetivos de Pré-Treinamento do BERT\n",
    "\n",
    "O sucesso do BERT vem de seu **pré-treinamento auto-supervisionado** em larga escala.  \n",
    "Ele não usa rótulos humanos: aprende diretamente com **texto cru**, aplicando duas tarefas artificiais que forçam o modelo a entender a estrutura e o sentido da linguagem.\n",
    "\n",
    "Essas duas tarefas são:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**  \n",
    "2. **Next Sentence Prediction (NSP)**\n",
    "\n",
    "Vamos entender cada uma delas com muito detalhe.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2.1. Masked Language Modeling (MLM)\n",
    "\n",
    "O objetivo do **MLM** é ensinar o modelo a *prever palavras faltantes* (ou mascaradas) a partir do contexto.\n",
    "\n",
    "### Intuição\n",
    "\n",
    "Imagine a seguinte frase:\n",
    "\n",
    "> “O cachorro **[MASK]** o carteiro.”\n",
    "\n",
    "O modelo deve prever a palavra mais provável para **[MASK]**, considerando **todas as palavras** da sentença — tanto antes quanto depois.\n",
    "\n",
    "Dessa forma, o BERT aprende **representações bidirecionais**, já que olha para a esquerda e para a direita ao mesmo tempo.\n",
    "\n",
    "---\n",
    "\n",
    "### Como o processo funciona\n",
    "\n",
    "1. **Tokenização e inserção de tokens especiais**  \n",
    "   O texto é dividido em *subtokens* (usando WordPiece) e recebe marcadores especiais:\n",
    "\n",
    "> [CLS] O cachorro [MASK] o carteiro [SEP]\n",
    "\n",
    "\n",
    "2. **Escolha aleatória dos tokens a mascarar**  \n",
    "Em cada sequência, **15% dos tokens** são selecionados para mascaramento.\n",
    "\n",
    "3. **Substituições aplicadas (segundo o paper original):**\n",
    "- 80% → substituídos por `[MASK]`  \n",
    "  > Ex: “O cachorro **[MASK]** o carteiro”\n",
    "- 10% → substituídos por uma palavra **aleatória**  \n",
    "  > Ex: “O cachorro **comeu** o carteiro”  \n",
    "- 10% → **mantidos iguais**, mas ainda contam como alvos de predição  \n",
    "  > Ex: “O cachorro **mordeu** o carteiro”\n",
    "\n",
    "Isso força o modelo a não se apoiar apenas no token `[MASK]` e a construir **representações contextuais robustas**.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo Prático\n",
    "\n",
    "Entrada:\n",
    "\n",
    "> [CLS] O cachorro [MASK] o carteiro [SEP]\n",
    "\n",
    "Saída esperada:\n",
    "\n",
    "> [CLS] O cachorro mordeu o carteiro [SEP]\n",
    "\n",
    "Durante o treinamento, o modelo gera uma distribuição de probabilidade sobre todo o vocabulário e tenta **maximizar a probabilidade do token correto**.\n",
    "\n",
    "---\n",
    "\n",
    "### Resultado\n",
    "\n",
    "Ao final do pré-treino, o BERT aprende vetores de embeddings altamente contextuais:\n",
    "- A palavra “mordeu” está associada a “cachorro” e “carteiro”.\n",
    "- Se a mesma palavra aparecer em outro contexto (“Ele **mordeu** a língua”), o vetor muda, refletindo o novo sentido.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2.2. Next Sentence Prediction (NSP)\n",
    "\n",
    "Além de entender palavras, o BERT também precisa compreender **relações entre sentenças**.  \n",
    "Essa é a função do **Next Sentence Prediction (NSP)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuição\n",
    "\n",
    "O NSP ensina o modelo a entender **coerência e continuidade textual**.\n",
    "\n",
    "Ele recebe **dois segmentos de texto (A e B)** e precisa responder:\n",
    "\n",
    "> “A sentença B realmente vem depois da sentença A no texto original?”\n",
    "\n",
    "---\n",
    "\n",
    "### Como o processo funciona\n",
    "\n",
    "Durante o pré-treinamento:\n",
    "\n",
    "- 50% dos pares (A,B) são **verdadeiros** → B realmente segue A.  \n",
    "- 50% dos pares são **falsos** → B vem de uma parte aleatória do corpus.\n",
    "\n",
    "O modelo deve classificar entre:\n",
    "\n",
    "| Rótulo | Significado | Exemplo |\n",
    "|:-------|:-------------|:---------|\n",
    "| `IsNext` | B segue A | A: “Ele pegou o guarda-chuva.”<br>B: “Saiu para a rua.” |\n",
    "| `NotNext` | B é aleatória | A: “Ele pegou o guarda-chuva.”<br>B: “O bolo estava delicioso.” |\n",
    "\n",
    "---\n",
    "\n",
    "### Entrada típica do BERT para NSP\n",
    "\n",
    "O modelo recebe os dois segmentos concatenados, separados por tokens especiais:\n",
    "\n",
    "> [CLS] Ele pegou o guarda-chuva. [SEP] Saiu para a rua. [SEP]\n",
    "\n",
    "Além disso, ele adiciona **embeddings segmentais** (também chamados *token type embeddings*), que indicam se cada token pertence à sentença A ou B.\n",
    "\n",
    "| Tipo de embedding | Função |\n",
    "|:------------------|:--------|\n",
    "| **Token embeddings** | Representa cada palavra ou subtoken. |\n",
    "| **Segment embeddings** | Indicam a qual sentença o token pertence (A ou B). |\n",
    "| **Positional embeddings** | Indicam a posição de cada token na sequência. |\n",
    "\n",
    "O vetor final de entrada de cada token é a **soma** desses três componentes.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo Prático\n",
    "\n",
    "Entrada:\n",
    "\n",
    "> [CLS] O cachorro latiu. [SEP] O carteiro correu. [SEP]\n",
    "\n",
    "Rótulo: `IsNext`\n",
    "\n",
    "Durante o pré-treino, o BERT aprende:\n",
    "- Que “latiu” e “carteiro” ocorrem juntos com frequência;  \n",
    "- Que há relação causal ou temporal entre as sentenças.\n",
    "\n",
    "Isso o prepara para tarefas posteriores como *Question Answering*, *Natural Language Inference* e *Paraphrase Detection*.\n",
    "\n",
    "---\n",
    "\n",
    "## Saída do Pré-Treinamento\n",
    "\n",
    "Durante o pré-treino, o BERT otimiza **duas funções de perda simultaneamente**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{MLM} + \\mathcal{L}_{NSP}\n",
    "$$\n",
    "\n",
    "- **$\\mathcal{L}_{MLM}$** → erro de predição das palavras mascaradas.  \n",
    "- **$\\mathcal{L}_{NSP}$** → erro de classificação entre `IsNext` e `NotNext`.\n",
    "\n",
    "A combinação dessas perdas permite que o modelo aprenda tanto **relações locais (entre palavras)** quanto **globais (entre sentenças)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumo Visual\n",
    "\n",
    "O BERT é, portanto, um **modelo de linguagem bidirecional e contextual**, treinado de forma auto-supervisionada, capaz de ser ajustado para praticamente qualquer tarefa de PLN.\n",
    "\n",
    "\n",
    "<pre>\n",
    "Entrada:\n",
    "[CLS] A sentença A ... [SEP] sentença B ... [SEP]\n",
    "\n",
    "↓ Embeddings = Token + Segment + Position\n",
    "\n",
    "↓ Encoder do Transformer (12 camadas)\n",
    "\n",
    "↓\n",
    "Saídas:\n",
    " - Vetores de cada token (para MLM)\n",
    " - Vetor [CLS] (para NSP)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1db883",
   "metadata": {},
   "source": [
    "# 5.3 Os três tipos de embeddings no BERT\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/bert_embeddings.png\" width=\"50%%\">\n",
    "\n",
    "Fonte: Devlin et al. (2018)\n",
    "\n",
    "O BERT não usa apenas *word embeddings*.  \n",
    "Cada token de entrada é representado pela **soma de três vetores** diferentes:\n",
    "\n",
    "$$\n",
    "\\text{InputEmbedding}(t_i) = \\text{TokenEmb}(t_i) + \\text{SegmentEmb}(s_i) + \\text{PositionEmb}(p_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3.1 Token Embeddings\n",
    "São os embeddings “normais” de palavras, como nos modelos anteriores (Word2Vec, GloVe, etc).  \n",
    "Cada palavra (ou subpalavra, no caso do BERT) é convertida em um vetor denso.\n",
    "\n",
    "Exemplo:\n",
    "```text\n",
    "[CLS] o filme foi ótimo [SEP]\n",
    "```\n",
    "\n",
    "Os tokens podem ser:\n",
    "```text\n",
    "[CLS], o, fil, ##me, foi, ót, ##imo, [SEP]\n",
    "```\n",
    "\n",
    "Cada um recebe um vetor aprendido — esses vetores formam a **base semântica** da frase.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3.2 Position Embeddings\n",
    "Transformers não têm noção de ordem (eles não são recorrentes).  \n",
    "Por isso, o BERT adiciona **positional embeddings fixos** que indicam a posição de cada token na sequência.\n",
    "\n",
    "Esses vetores são somados ao embedding de cada token e permitem que o modelo diferencie, por exemplo:\n",
    "\n",
    "```text\n",
    "o cachorro mordeu o homem\n",
    "o homem mordeu o cachorro\n",
    "```\n",
    "\n",
    "Apesar das mesmas palavras, as posições diferentes mudam completamente o significado.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3.3 Segment (ou Sentence) Embeddings\n",
    "\n",
    "Durante o pré-treinamento, o BERT usa o **Next Sentence Prediction (NSP)**, que exige lidar com **pares de sentenças**.  \n",
    "Para que o modelo saiba **qual token pertence a qual sentença**, o BERT adiciona embeddings de segmento:\n",
    "\n",
    "| Token | Segment Embedding |\n",
    "|:-------|:-----------------:|\n",
    "| `[CLS]` | A |\n",
    "| `O` | A |\n",
    "| `filme` | A |\n",
    "| `foi` | A |\n",
    "| `ótimo` | A |\n",
    "| `[SEP]` | A |\n",
    "| `Não` | B |\n",
    "| `gostei` | B |\n",
    "| `.` | B |\n",
    "| `[SEP]` | B |\n",
    "\n",
    "→ Tokens da **primeira sentença** recebem `Segment A`  \n",
    "→ Tokens da **segunda sentença** recebem `Segment B`\n",
    "\n",
    "Esses vetores de segmento são aprendidos junto com o resto da rede.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3.4 Combinação Final\n",
    "Para cada token, o vetor final de entrada é a soma:\n",
    "\n",
    "$$\n",
    "\\text{EmbeddingFinal}(t_i) =\n",
    "\\text{TokenEmb}(t_i)\n",
    "+ \\text{SegmentEmb}(s_i)\n",
    "+ \\text{PositionEmb}(p_i)\n",
    "$$\n",
    "\n",
    "\n",
    "Essa combinação fornece ao modelo:\n",
    "\n",
    "- **significado** da palavra (Token),\n",
    "- **ordem** na sequência (Position),\n",
    "- **identificação** da sentença (Segment).\n",
    "\n",
    "Essa soma é o **ponto de partida do aprendizado contextual** no BERT.  \n",
    "Os três tipos de embeddings são aprendidos (ou fixos, no caso posicional) e evoluem durante o pré-treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d12d7",
   "metadata": {},
   "source": [
    "## 5.4 — BERT na prática: Masked Language Modeling (MLM) + Next Sentence Prediction (NSP)\n",
    "\n",
    "Nesta seção vamos **reproduzir em pequena escala** os objetivos de pré-treinamento do BERT usando `BertForPreTraining` (que já inclui as duas cabeças: **MLM** e **NSP**).\n",
    "\n",
    "### O que vamos fazer\n",
    "\n",
    "1. **Mini-corpus**: definimos algumas sentenças curtas (você pode substituir pelo seu conjunto).\n",
    "2. **Pares para NSP**: criamos pares (A,B) rotulados:\n",
    "   - `IsNext` quando B realmente segue A no texto original;\n",
    "   - `NotNext` quando B é uma sentença aleatória.\n",
    "3. **Tokenização**: `[CLS] A [SEP] B [SEP]`, com `token_type_ids` (A=0, B=1).\n",
    "4. **Máscaras para MLM (80/10/10)** sobre ~15% dos tokens “previstáveis”.\n",
    "5. **Treino curto**: otimizamos **uma única loss conjunta**:  \n",
    "   \\(\\mathcal{L} = \\mathcal{L}_{MLM} + \\mathcal{L}_{NSP}\\)\n",
    "6. **Inspeção de saídas**:\n",
    "   - Top-k predições para `[MASK]`;\n",
    "   - Probabilidades `IsNext/NotNext` do NSP para um par de teste.\n",
    "\n",
    "### Dicas\n",
    "\n",
    "- **MLM** força o BERT a aprender **contexto bidirecional** (olhando esquerda e direita).  \n",
    "- **NSP** ensina **coerência entre sentenças** (embora trabalhos posteriores mostrem que remover NSP às vezes ajuda; aqui mantemos por fidelidade ao BERT original).  \n",
    "- Em produção, corpora são massivos (Wikipedia + BooksCorpus). Aqui, **mini-setup** para visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffd171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "[epoch 1 step 020] loss=2.0470\n",
      "[epoch 1 step 040] loss=0.4673\n",
      "\n",
      "=== INSPEÇÃO DE MLM (top-5 para um [MASK]) ===\n",
      "\n",
      "A: the cat sat on the mat.\n",
      "B: it [MASK] out the window.\n",
      "  posição mascarada: 9\n",
      "  token original (se disponível): 'it'\n",
      "   -> it           p=0.9780\n",
      "   -> cat          p=0.0073\n",
      "   -> eyes         p=0.0070\n",
      "   -> looking      p=0.0015\n",
      "   -> window       p=0.0015\n",
      "\n",
      "A: paris is the capital of france.\n",
      "B: the eiffel tower is in paris.\n",
      "  posição mascarada: 9\n",
      "  token original (se disponível): 'the'\n",
      "   -> the          p=0.9995\n",
      "   -> paris        p=0.0002\n",
      "   -> its          p=0.0001\n",
      "   -> france       p=0.0001\n",
      "   -> french       p=0.0000\n",
      "\n",
      "A: deep learning changed natural language processing.\n",
      "B: bert learns bidirectional context.\n",
      "  posição mascarada: 9\n",
      "  token original (se disponível): 'bert'\n",
      "   -> bert         p=0.9926\n",
      "   -> modeling     p=0.0035\n",
      "   -> learning     p=0.0008\n",
      "   -> learn        p=0.0002\n",
      "   -> context      p=0.0002\n",
      "\n",
      "A: paris is the capital of france.\n",
      "B: masked language modeling is powerful.\n",
      "  posição mascarada: 9\n",
      "  token original (se disponível): 'masked'\n",
      "   -> natural      p=0.5313\n",
      "   -> masked       p=0.4486\n",
      "   -> modeling     p=0.0116\n",
      "   -> deep         p=0.0036\n",
      "   -> parallel     p=0.0012\n",
      "\n",
      "=== INSPEÇÃO DE NSP (probabilidades) ===\n",
      "\n",
      "A: the cat sat on the mat.\n",
      "B: it [MASK] out the window.\n",
      "  P(IsNext)=1.000  |  P(NotNext)=0.000\n",
      "\n",
      "A: paris is the capital of france.\n",
      "B: the eiffel tower is in paris.\n",
      "  P(IsNext)=1.000  |  P(NotNext)=0.000\n",
      "\n",
      "A: deep learning changed natural language processing.\n",
      "B: bert learns bidirectional context.\n",
      "  P(IsNext)=1.000  |  P(NotNext)=0.000\n",
      "\n",
      "A: paris is the capital of france.\n",
      "B: masked language modeling is powerful.\n",
      "  P(IsNext)=0.000  |  P(NotNext)=1.000\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# BERT Pretraining Demo: MLM + NSP (didático)\n",
    "# ===========================================\n",
    "import math, random, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForPreTraining,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Setup\n",
    "# ---------------------------\n",
    "SEED = 7\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Mini-corpus (edite à vontade)\n",
    "#    Cada \"documento\" é uma lista de sentenças em ordem.\n",
    "# ---------------------------\n",
    "documents = [\n",
    "    [\n",
    "        \"the cat sat on the mat.\",\n",
    "        \"it looked out the window.\",\n",
    "        \"then it jumped down.\",\n",
    "        \"the dog barked loudly.\"\n",
    "    ],\n",
    "    [\n",
    "        \"deep learning changed natural language processing.\",\n",
    "        \"transformers enable parallel training.\",\n",
    "        \"bert learns bidirectional context.\",\n",
    "        \"masked language modeling is powerful.\"\n",
    "    ],\n",
    "    [\n",
    "        \"paris is the capital of france.\",\n",
    "        \"the eiffel tower is in paris.\",\n",
    "        \"tourists visit the louvre.\",\n",
    "        \"french cuisine is famous.\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Gerar pares (A,B) para NSP\n",
    "#    - 50% IsNext (B segue A no mesmo doc)\n",
    "#    - 50% NotNext (B aleatória de outro lugar)\n",
    "# ---------------------------\n",
    "def build_nsp_pairs(docs, num_pairs=300):\n",
    "    pairs = []\n",
    "    for _ in range(num_pairs):\n",
    "        doc = random.choice(docs)\n",
    "        if len(doc) >= 2 and random.random() < 0.5:\n",
    "            # IsNext: escolher A e B consecutivos no mesmo doc\n",
    "            i = random.randint(0, len(doc) - 2)\n",
    "            A, B = doc[i], doc[i+1]\n",
    "            label = 0  # IsNext\n",
    "        else:\n",
    "            # NotNext: A de um doc e B aleatória de outro doc\n",
    "            docA = random.choice(docs)\n",
    "            A = random.choice(docA)\n",
    "            # garanta B de outro doc para ser (em geral) NotNext\n",
    "            docB = random.choice([d for d in docs if d is not docA])\n",
    "            B = random.choice(docB)\n",
    "            label = 1  # NotNext\n",
    "        pairs.append((A, B, label))\n",
    "    return pairs\n",
    "\n",
    "pairs = build_nsp_pairs(documents, num_pairs=400)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Tokenizer e encoding de pares\n",
    "# ---------------------------\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "MAX_LEN = 64\n",
    "def encode_pair(a, b):\n",
    "    enc = tokenizer(\n",
    "        a, b,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # enc: input_ids, token_type_ids, attention_mask\n",
    "    return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# 4) MLM: selecionar posições a mascarar (15%) e aplicar 80/10/10\n",
    "#    - ignorar especiais [CLS]/[SEP]/PAD na seleção\n",
    "# ---------------------------\n",
    "CLS_ID = tokenizer.cls_token_id\n",
    "SEP_ID = tokenizer.sep_token_id\n",
    "PAD_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "MASK_ID = tokenizer.mask_token_id\n",
    "\n",
    "def apply_mlm_masking(input_ids, mlm_prob=0.15):\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # candidatos \"previstáveis\" (não especiais)\n",
    "    special_ids = {CLS_ID, SEP_ID, PAD_ID}\n",
    "    can_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "    for sid in special_ids:\n",
    "        can_mask &= (input_ids != sid)\n",
    "\n",
    "    # amostra binária de quais posições mascarar\n",
    "    probs = torch.full(input_ids.shape, mlm_prob, device=input_ids.device)\n",
    "    mask_positions = (torch.bernoulli(probs).bool()) & can_mask\n",
    "\n",
    "    # 80% -> [MASK]\n",
    "    mask80 = mask_positions & (torch.rand_like(input_ids, dtype=torch.float) < 0.8)\n",
    "    input_ids[mask80] = MASK_ID\n",
    "\n",
    "    # 10% -> token aleatório\n",
    "    # (onde foi selecionado p/ máscara mas não entrou no 80%)\n",
    "    remaining = mask_positions & (~mask80)\n",
    "    rand10 = remaining & (torch.rand_like(input_ids, dtype=torch.float) < 0.5)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    random_tokens = torch.randint(low=0, high=vocab_size, size=input_ids.shape, device=input_ids.device)\n",
    "    input_ids[rand10] = random_tokens[rand10]\n",
    "\n",
    "    # 10% -> deixam igual (implicitly: remaining & ~rand10)\n",
    "\n",
    "    # posições não-mascaradas não entram no loss (=-100)\n",
    "    labels[~mask_positions] = -100\n",
    "    return input_ids, labels, mask_positions\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Dataset + DataLoader\n",
    "# ---------------------------\n",
    "class BertPretrainDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        a, b, nsp_label = self.pairs[idx]\n",
    "        enc = encode_pair(a, b)\n",
    "        # aplica MLM no input_ids\n",
    "        ids, labels, mask_pos = apply_mlm_masking(enc[\"input_ids\"].clone())\n",
    "        item = {\n",
    "            \"input_ids\": ids,\n",
    "            \"token_type_ids\": enc[\"token_type_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"labels\": labels,  # para MLM\n",
    "            \"next_sentence_label\": torch.tensor(nsp_label, dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "dataset = BertPretrainDataset(pairs)\n",
    "split = int(0.9 * len(dataset))\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [split, len(dataset)-split])\n",
    "\n",
    "BATCH = 8\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Modelo (MLM + NSP) e otimizador\n",
    "# ---------------------------\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Loop de treino curto (didático)\n",
    "# ---------------------------\n",
    "EPOCHS = 1\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        out = model(**batch)  # loss = MLM + NSP\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        running += loss.item()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(f\"[epoch {epoch} step {step:03d}] loss={running/20:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Funções de inspeção: MLM top-k e NSP probs\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def inspect_mlm(tokenizer, model, a, b, topk=5):\n",
    "    model.eval()\n",
    "    enc = encode_pair(a, b)\n",
    "    # força UM [MASK] manualmente para demonstrar (na primeira palavra de B se possível)\n",
    "    ids = enc[\"input_ids\"].clone()\n",
    "    # procurar um token \"previstável\" em B (token_type_id==1) que não seja especial\n",
    "    ttypes = enc[\"token_type_ids\"]\n",
    "    chosen_idx = None\n",
    "    for i in range(ids.size(0)):\n",
    "        if (ttypes[i] == 1) and (ids[i] not in {CLS_ID, SEP_ID, PAD_ID}):\n",
    "            chosen_idx = i; break\n",
    "    if chosen_idx is None:\n",
    "        # fallback: primeiro token não-especial\n",
    "        for i in range(ids.size(0)):\n",
    "            if ids[i] not in {CLS_ID, SEP_ID, PAD_ID}:\n",
    "                chosen_idx = i; break\n",
    "    original = ids[chosen_idx].item()\n",
    "    ids[chosen_idx] = MASK_ID\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": ids.unsqueeze(0).to(device),\n",
    "        \"token_type_ids\": enc[\"token_type_ids\"].unsqueeze(0).to(device),\n",
    "        \"attention_mask\": enc[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "    }\n",
    "    out = model(**batch)\n",
    "    logits = out.prediction_logits[0, chosen_idx]  # (V,)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    vals, idxs = torch.topk(probs, k=topk)\n",
    "    toks = [tokenizer.decode([i]) for i in idxs.tolist()]\n",
    "    return {\n",
    "        \"masked_position\": chosen_idx,\n",
    "        \"original_token\": tokenizer.decode([original]),\n",
    "        \"topk\": list(zip(toks, [float(v) for v in vals]))\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def inspect_nsp(tokenizer, model, a, b):\n",
    "    model.eval()\n",
    "    enc = encode_pair(a, b)\n",
    "    batch = {\n",
    "        \"input_ids\": enc[\"input_ids\"].unsqueeze(0).to(device),\n",
    "        \"token_type_ids\": enc[\"token_type_ids\"].unsqueeze(0).to(device),\n",
    "        \"attention_mask\": enc[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "    }\n",
    "    out = model(**batch)\n",
    "    # out.seq_relationship_logits: (B, 2) -> [IsNext, NotNext]\n",
    "    logits = out.seq_relationship_logits[0]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return {\"IsNext\": float(probs[0]), \"NotNext\": float(probs[1])}\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Testes: ver predições após o treino curto\n",
    "# ---------------------------\n",
    "tests = [\n",
    "    (\"the cat sat on the mat.\", \"it [MASK] out the window.\"),\n",
    "    (\"paris is the capital of france.\", \"the eiffel tower is in paris.\"),\n",
    "    (\"deep learning changed natural language processing.\", \"bert learns bidirectional context.\"),\n",
    "    (\"paris is the capital of france.\", \"masked language modeling is powerful.\"),  # NotNext provável\n",
    "]\n",
    "\n",
    "print(\"\\n=== INSPEÇÃO DE MLM (top-5 para um [MASK]) ===\")\n",
    "for a, b in tests:\n",
    "    r = inspect_mlm(tokenizer, model, a, b, topk=5)\n",
    "    print(f\"\\nA: {a}\\nB: {b}\")\n",
    "    print(f\"  posição mascarada: {r['masked_position']}\")\n",
    "    print(f\"  token original (se disponível): {r['original_token']!r}\")\n",
    "    for tok, p in r[\"topk\"]:\n",
    "        print(f\"   -> {tok:<12} p={p:.4f}\")\n",
    "\n",
    "print(\"\\n=== INSPEÇÃO DE NSP (probabilidades) ===\")\n",
    "for a, b in tests:\n",
    "    p = inspect_nsp(tokenizer, model, a, b)\n",
    "    print(f\"\\nA: {a}\\nB: {b}\")\n",
    "    print(f\"  P(IsNext)={p['IsNext']:.3f}  |  P(NotNext)={p['NotNext']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7df5a",
   "metadata": {},
   "source": [
    "# 5.5 — Fine-Tuning do BERT e o papel do token `[CLS]`\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/bert_ft_tasks.png\" width=\"50%%\">\n",
    "\n",
    "Fonte: Devlin et al. (2018)\n",
    "\n",
    "Após o pré-treinamento, o BERT é uma poderosa **base de conhecimento linguístico geral**, mas ainda **não sabe realizar tarefas específicas** (como classificar sentimentos ou responder perguntas).  \n",
    "O **fine-tuning** é o processo que adapta esse conhecimento para **tarefas supervisionadas específicas**.\n",
    "\n",
    "---\n",
    "\n",
    "## Conceito de Fine-Tuning\n",
    "\n",
    "O *fine-tuning* consiste em:\n",
    "\n",
    "1. **Reaproveitar os pesos do modelo pré-treinado**, que já aprenderam relações semânticas e sintáticas da linguagem.  \n",
    "2. **Acrescentar uma pequena camada de saída** (geralmente linear) adaptada à tarefa desejada.  \n",
    "3. **Treinar o modelo completo novamente**, mas em um **novo conjunto de dados rotulado**, com uma **taxa de aprendizado menor** — para ajustar suavemente os pesos sem “destruir” o conhecimento linguístico aprendido.\n",
    "\n",
    "Essa abordagem faz parte do paradigma de **transfer learning**:\n",
    "- o pré-treinamento fornece **conhecimento geral** sobre linguagem;\n",
    "- o fine-tuning **especializa** esse conhecimento para um contexto ou tarefa.\n",
    "\n",
    "---\n",
    "\n",
    "## Como o Fine-Tuning é Feito\n",
    "\n",
    "O processo segue o mesmo pipeline básico:\n",
    "\n",
    "> Texto → Tokenização → Embeddings → Camadas do BERT → Saída Adaptada → Função de Perda\n",
    "\n",
    "Dependendo da tarefa, usamos diferentes **estruturas de saída**:\n",
    "\n",
    "| Tipo de Tarefa | Saída Esperada | Cabeça Adicional | Exemplo |\n",
    "|----------------|----------------|------------------|----------|\n",
    "| Classificação de Sentença | 1 vetor (logits) | Linear + Softmax | Análise de Sentimento |\n",
    "| Classificação de Token | 1 vetor por token | Linear + Softmax | NER, POS Tagging |\n",
    "| Perguntas e Respostas | Vetores de início e fim | Linear (dupla saída) | SQuAD |\n",
    "| Similaridade entre sentenças | 2 embeddings | Camada de similaridade (cosine) | STS |\n",
    "\n",
    "---\n",
    "\n",
    "## O Papel do Token `[CLS]`\n",
    "\n",
    "O token especial `[CLS]` (*classification token*) é adicionado **no início de toda sequência**.  \n",
    "Durante o pré-treinamento, ele é associado à tarefa **Next Sentence Prediction (NSP)**, ou seja, o modelo já aprendeu a representar **o significado global da sentença ou do par de sentenças** nele.\n",
    "\n",
    "Por isso, no *fine-tuning* para tarefas de **classificação**, usamos **somente o vetor final de `[CLS]`** como representação da entrada completa.\n",
    "\n",
    "### Exemplo:\n",
    "\n",
    "> Entrada: [CLS] O filme foi excelente ! [SEP]\n",
    "\n",
    "O BERT processa todos os tokens por 12 camadas de atenção.  \n",
    "Ao final, o vetor correspondente a `[CLS]` contém uma **representação contextual global** do significado da frase.\n",
    "\n",
    "> Esse vetor é passado para uma **camada linear de classificação** que gera os logits finais (ex: positivo / negativo).\n",
    "\n",
    "Matematicamente:\n",
    "\n",
    "$$\n",
    "\\text{logits} = W \\cdot h_{[CLS]} + b\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- $h_{[CLS]}$ é o embedding final do token `[CLS]`;  \n",
    "- $W$ e $b$ são parâmetros da nova camada linear.\n",
    "\n",
    "---\n",
    "\n",
    "## Fine-Tuning em Diferentes Tarefas\n",
    "\n",
    "### (a) Classificação de Sentenças\n",
    "- Entrada: `[CLS] sentença [SEP]`\n",
    "- Saída: vetor `[CLS]` → camada linear → softmax.\n",
    "\n",
    "### (b) Classificação de Tokens (ex: NER)\n",
    "- Entrada: `[CLS] frase [SEP]`\n",
    "- Saída: vetor contextual **de cada token**, passando cada um por uma camada linear.\n",
    "\n",
    "### (c) Perguntas e Respostas\n",
    "- Entrada: `[CLS] pergunta [SEP] contexto [SEP]`\n",
    "- Saída: duas camadas lineares preveem índices de início e fim no contexto.\n",
    "\n",
    "---\n",
    "\n",
    "## Treinamento Suave (Small Learning Rate)\n",
    "\n",
    "Durante o *fine-tuning*, usamos:\n",
    "- **taxas de aprendizado pequenas (ex: 2e-5 a 5e-5)**;\n",
    "- **poucas épocas (2–4)**;\n",
    "- **batch sizes pequenos (16–32)**;\n",
    "- **métodos de regularização leves** (como dropout).\n",
    "\n",
    "A ideia é **ajustar levemente** o modelo, mantendo a “gramática geral” da linguagem que o BERT já aprendeu.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualização Conceitual\n",
    "\n",
    "<pre>\n",
    "[Texto de Entrada] → [Tokenização]\n",
    "          ↓\n",
    "     [CLS]  token1  token2  ...  [SEP]\n",
    "          ↓\n",
    "     Embeddings + Posições\n",
    "          ↓\n",
    "      Camadas BERT\n",
    "          ↓\n",
    "      Vetor [CLS]  ← representa a sentença\n",
    "          ↓\n",
    "   Camada Linear (Nova)\n",
    "          ↓\n",
    "   Saída final (ex: sentimento positivo)\n",
    "</pre>\n",
    "\n",
    "O BERT já aprendeu “como o idioma funciona”; o fine-tuning ensina “como resolver o seu problema específico”.\n",
    "    \n",
    "- O token [CLS] é o resumo semântico da sentença — ele condensa, em um vetor, as informações que fluíram por toda a rede de atenção.\n",
    "    \n",
    "- Essa adaptação é tão eficiente que, muitas vezes, basta menos de 1 hora de treino para atingir desempenho de ponta em diversas tarefas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620b30",
   "metadata": {},
   "source": [
    "## Como o `[CLS]` é atualizado durante o Fine-Tuning\n",
    "\n",
    "O token especial `[CLS]` é adicionado **no início da sequência** de entrada:\n",
    "\n",
    "> [CLS] O filme foi ótimo [SEP]\n",
    "\n",
    "Durante o **forward pass** do BERT:\n",
    "\n",
    "1. Ele recebe um embedding inicial aprendido (como qualquer palavra).\n",
    "2. Passa por todas as camadas do Transformer — o `[CLS]` “observa” todas as outras palavras via *self-attention*.\n",
    "3. O vetor final do `[CLS]` (geralmente de 768 dimensões) é interpretado como o **resumo contextual da sentença**.\n",
    "4. Esse vetor é então enviado a uma camada linear + *softmax* para produzir as probabilidades das classes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{Softmax}(W_{cls} \\cdot h_{cls} + b)\n",
    "$$\n",
    "\n",
    "onde \\( h_{cls} \\) é o embedding final do `[CLS]`.\n",
    "\n",
    "---\n",
    "\n",
    "### A Função de Custo e o Gradiente\n",
    "\n",
    "Para uma tarefa de classificação binária, usamos a **entropia cruzada**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_i y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Durante o **backpropagation**, o gradiente da perda em relação a \\( h_{cls} \\) é:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_{cls}} = W_{cls}^\\top (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "Esse gradiente é propagado de volta, atualizando:\n",
    "- o vetor `[CLS]`;\n",
    "- as projeções da atenção e camadas feedforward;\n",
    "- e até mesmo os embeddings iniciais.\n",
    "\n",
    "Assim, o `[CLS]` aprende a **codificar padrões discriminativos** úteis para a tarefa.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuição\n",
    "\n",
    "Imagine duas frases:\n",
    "\n",
    "| Sentença | Rótulo |\n",
    "|-----------|---------|\n",
    "| “O filme foi maravilhoso!” | 1 (positivo) |\n",
    "| “O filme foi péssimo.” | 0 (negativo) |\n",
    "\n",
    "Inicialmente, os vetores `[CLS]` dessas frases são aleatórios ou neutros.  \n",
    "Após algumas iterações:\n",
    "\n",
    "- O `[CLS]` das frases **positivas** é “puxado” para uma região do espaço vetorial associada a altas probabilidades de classe 1.\n",
    "- O `[CLS]` das frases **negativas** é empurrado para outra região, favorecendo a classe 0.\n",
    "\n",
    "O espaço vetorial se organiza assim:\n",
    "\n",
    "Espaço vetorial após fine-tuning\n",
    "\n",
    "<pre>\n",
    "↑ Classe 1 (positivo)\n",
    "│\n",
    "│      o    o   o\n",
    "│        o\n",
    "│\n",
    "│\n",
    "│\n",
    "│     x\n",
    "│  x     x\n",
    "└────────────────────→ Classe 0 (negativo)\n",
    "</pre>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo Numérico Simplificado\n",
    "\n",
    "O exemplo abaixo mostra **como o gradiente atua no vetor `[CLS]`**.\n",
    "\n",
    "- h_cls = torch.tensor([0.2, 0.1, 0.4])   # embedding [CLS]\n",
    "- W_cls = torch.tensor([[ 0.5, -0.3, 0.1],\n",
    "- [-0.2,  0.4, 0.6]])   # pesos do classificador\n",
    "- b = torch.tensor([0.0, 0.0])\n",
    "\n",
    "A saída antes do ajuste pode ser algo como:\n",
    "\n",
    "> tensor([0.63, 0.37])   # predição: classe 0\n",
    "\n",
    "Se o rótulo verdadeiro é 1, a perda:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.37)\n",
    "$$\n",
    "\n",
    "gera um gradiente que:\n",
    "\n",
    "- diminui a confiança na classe 0;\n",
    "- aumenta as direções em $ h_{cls} $ que favorecem a classe 1.\n",
    "\n",
    "Após algumas iterações, o vetor `[CLS]` é ajustado de modo que:\n",
    "\n",
    "$ Softmax(W_cls ⋅ h_cls) ≈ [0.1, 0.9] $\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretação Final\n",
    "\n",
    "Depois do *fine-tuning*:\n",
    "- O vetor `[CLS]` é um **resumo contextual** ajustado à tarefa.\n",
    "- A camada linear \\( W_{cls} \\) aprende um **hiperplano de decisão** que separa `[CLS]` por classe.\n",
    "- Por isso, podemos usar apenas os vetores `[CLS]` (sem o resto do BERT) para visualizar como o modelo organiza o espaço semântico — o que faremos a seguir com **t-SNE/UMAP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb51354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades iniciais: [0.46754572 0.5324543 ]\n",
      "Perda inicial: 0.6302582025527954\n",
      "\n",
      "Gradiente em relação a h_cls: tensor([ 0.3273, -0.3273, -0.2338])\n",
      "Gradiente em relação a W_cls: tensor([[ 0.0935,  0.0468,  0.1870],\n",
      "        [-0.0935, -0.0468, -0.1870]])\n",
      "\n",
      "Probabilidades após atualização: [0.37053224 0.6294677 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Vetor [CLS] e pesos iniciais (3 dimensões → 2 classes)\n",
    "h_cls = torch.tensor([0.2, 0.1, 0.4], requires_grad=True)\n",
    "W_cls = torch.tensor([[ 0.5, -0.3, 0.1],\n",
    "                      [-0.2,  0.4, 0.6]], requires_grad=True)\n",
    "b = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "logits = W_cls @ h_cls + b\n",
    "probs = F.softmax(logits, dim=0)\n",
    "print(\"Probabilidades iniciais:\", probs.detach().numpy())\n",
    "\n",
    "# Rótulo verdadeiro: classe 1 (positiva)\n",
    "target = torch.tensor([0., 1.])\n",
    "\n",
    "# Perda (entropia cruzada)\n",
    "loss = -torch.sum(target * torch.log(probs))\n",
    "print(\"Perda inicial:\", loss.item())\n",
    "\n",
    "# Backprop\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradiente em relação a h_cls:\", h_cls.grad)\n",
    "print(\"Gradiente em relação a W_cls:\", W_cls.grad)\n",
    "\n",
    "# Atualiza manualmente (1 passo de SGD)\n",
    "lr = 0.5\n",
    "with torch.no_grad():\n",
    "    h_cls -= lr * h_cls.grad\n",
    "    W_cls -= lr * W_cls.grad\n",
    "\n",
    "logits_new = W_cls @ h_cls + b\n",
    "probs_new = F.softmax(logits_new, dim=0)\n",
    "print(\"\\nProbabilidades após atualização:\", probs_new.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba8c6f",
   "metadata": {},
   "source": [
    "## 5.6 — Fine-tuning do BERT na prática (Classificação de Sentenças)\n",
    "\n",
    "Nesta seção, vamos **ajustar** (fine-tune) um BERT pré-treinado para uma tarefa supervisionada de **classificação de sentenças** (sentimento).  \n",
    "Usaremos o modelo `BertForSequenceClassification`, que adiciona automaticamente uma **cabeça linear** sobre o vetor do **token `[CLS]`** (na prática, o \"pooled output\" = `tanh(W * h_[CLS] + b)`).\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "1. **Carregar dados** (GLUE/SST-2; se indisponível, usamos um fallback em memória).\n",
    "2. **Tokenizar** com `BertTokenizerFast` (`[CLS] ... [SEP]` automaticamente).\n",
    "3. **Instanciar o modelo** `BertForSequenceClassification(num_labels=2)`.\n",
    "4. **Treinar** com `Trainer` (taxa pequena, poucas épocas).\n",
    "5. **Avaliar** (accuracy, F1) e **inspecionar previsões**.\n",
    "\n",
    "> Intuição: durante o fine-tuning, o vetor contextual do `[CLS]` passa por uma camada linear + softmax.  \n",
    "> A perda supervisionada **empurra** o embedding do `[CLS]` para organizar o espaço semântico de modo a separar as classes (positivo/negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bc75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "transformers: 4.57.1\n",
      "GLUE/SST-2 carregado: train=67349  val=872\n",
      "Textos saneados: train=67349  val=872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aviso] parâmetros ignorados: ['evaluation_strategy']\n",
      "\n",
      "=== Iniciando fine-tuning do BERT ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  89/8420 06:28 < 10:20:23, 0.22 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BERT Fine-tuning — Classificação (SST-2 com fallback) + SANEAMENTO ULTRARROBUSTO\n",
    "# ============================================================\n",
    "import os, random, math, inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Setup determinístico\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) (Colab) instalar libs\n",
    "# ---------------------------\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -U \"transformers>=4.39\" \"datasets>=2.14\" \"accelerate>=0.28\" \"evaluate>=0.4\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Imports principais\n",
    "# ---------------------------\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizerFast, BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "# Helper: TrainingArguments compatível\n",
    "def build_training_arguments(**kwargs) -> TrainingArguments:\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    allowed = set(sig.parameters.keys()); allowed.discard(\"self\")\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n",
    "    dropped = [k for k in kwargs if k not in allowed]\n",
    "    if dropped:\n",
    "        print(\"[Aviso] parâmetros ignorados:\", dropped)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Dataset: GLUE/SST-2 → fallback\n",
    "# ---------------------------\n",
    "USE_FALLBACK = False\n",
    "train_texts, train_labels = [], []\n",
    "val_texts,   val_labels   = [], []\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    glue = load_dataset(\"glue\", \"sst2\")\n",
    "    train_texts = glue[\"train\"][\"sentence\"]\n",
    "    train_labels= glue[\"train\"][\"label\"]\n",
    "    val_texts   = glue[\"validation\"][\"sentence\"]\n",
    "    val_labels  = glue[\"validation\"][\"label\"]\n",
    "    print(f\"GLUE/SST-2 carregado: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "except Exception as e:\n",
    "    print(\"[AVISO] Falha ao carregar GLUE/SST-2:\", repr(e))\n",
    "    USE_FALLBACK = True\n",
    "    train_pairs = [\n",
    "        (\"i loved the movie, it was fantastic and touching.\", 1),\n",
    "        (\"what a terrible waste of time.\", 0),\n",
    "        (\"an excellent script and strong performances.\", 1),\n",
    "        (\"the film is boring and predictable.\", 0),\n",
    "        (\"absolutely wonderful and inspiring!\", 1),\n",
    "        (\"bad acting and poor dialogue.\", 0),\n",
    "        (\"smart, funny, and well paced.\", 1),\n",
    "        (\"i hated every minute of it.\", 0),\n",
    "        (\"a delightful surprise with great music.\", 1),\n",
    "        (\"the plot makes no sense at all.\", 0),\n",
    "        (\"remarkably good for a low budget.\", 1),\n",
    "        (\"painfully slow and unoriginal.\", 0),\n",
    "    ]\n",
    "    random.shuffle(train_pairs)\n",
    "    train_texts = [t for t,_ in train_pairs]\n",
    "    train_labels= [y for _,y in train_pairs]\n",
    "    val_pairs = [\n",
    "        (\"wonderful direction and amazing visuals.\", 1),\n",
    "        (\"awful script and worse execution.\", 0),\n",
    "        (\"i really enjoyed this film.\", 1),\n",
    "        (\"it was not enjoyable at all.\", 0),\n",
    "    ]\n",
    "    val_texts  = [t for t,_ in val_pairs]\n",
    "    val_labels = [y for _,y in val_pairs]\n",
    "    print(f\"Fallback: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4) SANEAMENTO ULTRARROBUSTO\n",
    "# ---------------------------\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def _is_nan(x):\n",
    "    try:\n",
    "        return bool(np.isnan(x))  # cobre floats numpy e python\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _to_plain_str(x):\n",
    "    # bytes → str\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", \"ignore\")\n",
    "        except Exception:\n",
    "            x = str(x)\n",
    "    # numpy types / outros\n",
    "    if isinstance(x, (np.generic,)):\n",
    "        x = x.item()\n",
    "    # dict comum com chave 'text' ou 'sentence'\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"text\", \"sentence\", \"content\"):\n",
    "            if k in x and isinstance(x[k], (str, bytes, bytearray)):\n",
    "                return _to_plain_str(x[k])\n",
    "        # último recurso: string da representação\n",
    "        return str(x)\n",
    "    # número → str\n",
    "    if isinstance(x, (int, float, np.integer, np.floating)) and not _is_nan(x):\n",
    "        return str(x)\n",
    "    # strings “normais”\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    # iteráveis de strings → junta\n",
    "    if isinstance(x, Iterable) and not isinstance(x, (str, bytes, bytearray)):\n",
    "        # achata e junta com espaço\n",
    "        flat = []\n",
    "        for item in x:\n",
    "            s = _to_plain_str(item)\n",
    "            if s is not None and s.strip():\n",
    "                flat.append(s.strip())\n",
    "        return \" \".join(flat) if flat else None\n",
    "    # fallback\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def clean_xy(texts, labels, max_bad_print=5, name=\"train\"):\n",
    "    X, y = [], []\n",
    "    bad_samples = []\n",
    "    # Se vier numpy array / pandas Series, transforma\n",
    "    if hasattr(texts, \"tolist\"):\n",
    "        texts = texts.tolist()\n",
    "    if hasattr(labels, \"tolist\"):\n",
    "        labels = labels.tolist()\n",
    "\n",
    "    for t, l in zip(texts, labels):\n",
    "        # remover None/NaN\n",
    "        if t is None: \n",
    "            bad_samples.append((\"None\", t)); continue\n",
    "        if _is_nan(t):\n",
    "            bad_samples.append((\"NaN\", t)); continue\n",
    "\n",
    "        s = _to_plain_str(t)\n",
    "        if s is None:\n",
    "            bad_samples.append((\"unconvertible\", t)); continue\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            bad_samples.append((\"empty after strip\", t)); continue\n",
    "\n",
    "        try:\n",
    "            lbl = int(l)\n",
    "        except Exception:\n",
    "            # alguns datasets retornam np.int64 etc.\n",
    "            try:\n",
    "                lbl = int(np.array(l).item())\n",
    "            except Exception:\n",
    "                bad_samples.append((\"bad label\", (t, l))); continue\n",
    "\n",
    "        X.append(s); y.append(lbl)\n",
    "\n",
    "    if bad_samples:\n",
    "        print(f\"[{name}] {len(bad_samples)} amostra(s) removida(s) por formato inválido.\")\n",
    "        for i, (reason, sample) in enumerate(bad_samples[:max_bad_print], 1):\n",
    "            print(f\"  #{i} motivo={reason}  exemplo={repr(sample)[:120]}\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "train_texts, train_labels = clean_xy(train_texts, train_labels, name=\"train\")\n",
    "val_texts,   val_labels   = clean_xy(val_texts,   val_labels,   name=\"val\")\n",
    "\n",
    "print(f\"Textos saneados: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "assert len(train_texts) == len(train_labels) and len(val_texts) == len(val_labels)\n",
    "\n",
    "# checagem final (se ainda falhar, imprime tipos “estranhos”)\n",
    "def _debug_types(name, xs, n=5):\n",
    "    weird = [(i, type(x).__name__, repr(x)[:80]) for i, x in enumerate(xs) if not isinstance(x, str)]\n",
    "    if weird:\n",
    "        print(f\"[DEBUG] {name}: itens não-string após saneamento:\", weird[:n])\n",
    "\n",
    "_debug_types(\"train_texts\", train_texts)\n",
    "_debug_types(\"val_texts\",   val_texts)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Tokenizer e tokenização\n",
    "# ---------------------------\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    # segurança extra: garante list[str]\n",
    "    assert isinstance(texts, (list, tuple)), \"tokenize_batch espera list/tuple de strings.\"\n",
    "    assert all(isinstance(t, str) for t in texts), \"tokenize_batch recebeu itens não-string.\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_enc = tokenize_batch(train_texts)\n",
    "val_enc   = tokenize_batch(val_texts)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Dataset PyTorch\n",
    "# ---------------------------\n",
    "class TorchTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels    = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TorchTextDataset(train_enc, train_labels)\n",
    "val_ds   = TorchTextDataset(val_enc,   val_labels)\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Modelo (usa pooled_output = [CLS])\n",
    "# ---------------------------\n",
    "num_labels = 2\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Métricas\n",
    "# ---------------------------\n",
    "try:\n",
    "    import evaluate\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric  = evaluate.load(\"f1\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        r1 = acc_metric.compute(predictions=preds, references=labels)\n",
    "        r2 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "        return {\"accuracy\": r1[\"accuracy\"], \"f1\": r2[\"f1\"]}\n",
    "except Exception:\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = (preds == labels).mean()\n",
    "        return {\"accuracy\": float(acc)}\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Treinamento com Trainer\n",
    "# ---------------------------\n",
    "EPOCHS = 2 if not USE_FALLBACK else 4\n",
    "BATCH  = 16 if not USE_FALLBACK else 8\n",
    "\n",
    "args = build_training_arguments(\n",
    "    output_dir=\"bert-ft-sst2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=SEED,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Iniciando fine-tuning do BERT ===\")\n",
    "train_out = trainer.train()\n",
    "eval_out  = trainer.evaluate()\n",
    "print(\"\\nResultados de validação:\", eval_out)\n",
    "\n",
    "# ---------------------------\n",
    "# 10) Teste prático — previsões e inspeção\n",
    "# ---------------------------\n",
    "def tokenize_one(text):\n",
    "    return tokenizer(\n",
    "        text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "def predict(texts):\n",
    "    model.eval()\n",
    "    assert isinstance(texts, (list, tuple)), \"predict espera list/tuple de strings.\"\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    return preds, probs\n",
    "\n",
    "samples = [\n",
    "    \"i absolutely loved this movie!\",\n",
    "    \"this was a complete waste of time.\",\n",
    "    \"the performances are strong and convincing.\",\n",
    "    \"the plot is weak and the pacing is terrible.\",\n",
    "]\n",
    "\n",
    "preds, probs = predict(samples)\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "print(\"\\n=== Amostras de previsão ===\")\n",
    "for s, p, pr in zip(samples, preds, probs):\n",
    "    print(f\"- {s}\\n  -> pred: {label_names[p]}  probs={pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81373235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para inspecionar manualmente o vetor [CLS] (pooled_output):\n",
    "# with torch.no_grad():\n",
    "#     enc = tokenize_one(\"a great movie.\").to(device)\n",
    "#     outputs = model.bert(**enc, return_dict=True)\n",
    "#     pooled = outputs.pooler_output\n",
    "#     print(\"pooled_output shape:\", pooled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9312d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
