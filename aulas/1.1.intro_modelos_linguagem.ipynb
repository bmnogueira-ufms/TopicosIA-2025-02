{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9713c578",
   "metadata": {
    "id": "9713c578"
   },
   "source": [
    "# Instalação dos pacotes necessários / Configuração da máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ddc7b",
   "metadata": {
    "id": "281ddc7b"
   },
   "outputs": [],
   "source": [
    "# Instalação das dependências localmente\n",
    "# Se estiver rodando localmente, descomente a linha abaixo para instalar as dependências\n",
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264ce1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35871,
     "status": "ok",
     "timestamp": 1754430090748,
     "user": {
      "displayName": "Bruno Magalhaes Nogueira",
      "userId": "18320277366917905276"
     },
     "user_tz": 240
    },
    "id": "c264ce1a",
    "outputId": "39ca3594-21d5-43c7-893e-c3a9d3298bfe"
   },
   "outputs": [],
   "source": [
    "# Se rodando no Google Colab, descomente a linha abaixo para montar o Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dUQdJBN-LKeQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1754430030538,
     "user": {
      "displayName": "Bruno Magalhaes Nogueira",
      "userId": "18320277366917905276"
     },
     "user_tz": 240
    },
    "id": "dUQdJBN-LKeQ",
    "outputId": "1c62fce2-9665-4e82-e9c0-0e4890d3ca83"
   },
   "outputs": [],
   "source": [
    "# Instalação das dependências no Google Colab\n",
    "# Mude CAMINHO_PARA_REPO para o caminho correto do seu repositório no seu Google Drive\n",
    "# ! pip install -r /content/drive/MyDrive/CAMINHO_PARA_REPO/TopicosIA-2025-02/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7988806",
   "metadata": {
    "id": "a7988806"
   },
   "source": [
    "# Introdução aos modelos de linguagem\n",
    "\n",
    "Imagine um computador que consegue:\n",
    "- Entender quando você escreve uma frase em português\n",
    "- Prever qual palavra você vai digitar em seguida\n",
    "- Traduzir um texto de inglês para português\n",
    "- Escrever um texto que parece ter sido escrito por um humano\n",
    "\n",
    "Isso tudo é possível graças aos **modelos de linguagem**! Eles são programas especiais que \"aprendem\" como a linguagem humana funciona.\n",
    "\n",
    "### Por que isso é importante?\n",
    "\n",
    "Os modelos de linguagem estão por trás de muitas tecnologias que você usa no dia a dia:\n",
    "- O corretor automático do seu celular\n",
    "- O Google Tradutor\n",
    "- Assistentes virtuais como Siri e Alexa\n",
    "- O ChatGPT que tem feito tanto sucesso\n",
    "\n",
    "### O que faremos nesta aula?\n",
    "\n",
    "1. Vamos entender **o que são** modelos de linguagem de forma simples\n",
    "2. Conhecer a **história** dessa tecnologia (é mais antiga do que você imagina!)\n",
    "3. Implementar nossos **primeiros modelos** em Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042e867",
   "metadata": {
    "id": "f042e867"
   },
   "source": [
    "## Sumário\n",
    "1. [Modelos de linguagem: definição e aplicações](#Modelos-de-linguagem:-definição-e-aplicações)\n",
    "2. [Linha do tempo e evolução histórica](#Linha-do-tempo-e-evolução-histórica)\n",
    "3. [Representação Bag‑of‑Words](#Representação-Bag‑of‑Words)\n",
    "4. [Modelos n‑gramas](#Modelos-n‑gramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d651dc",
   "metadata": {
    "id": "c4d651dc"
   },
   "source": [
    "## Modelos de linguagem: definição e aplicações\n",
    "\n",
    "### O que é um modelo de linguagem?\n",
    "\n",
    "Vamos começar com uma **definição simples**:\n",
    "\n",
    "> Um modelo de linguagem é como um \"estudante\" muito dedicado que leu milhões de textos e aprendeu a prever quais palavras fazem sentido juntas.\n",
    "\n",
    "**Analogia prática**: Imagine que você está escrevendo a frase \"O gato subiu no...\" - sua mente automaticamente pensa em palavras como \"telhado\", \"muro\" ou \"sofá\". Você sabe que \"O gato subiu no elefante\" seria estranho. Um modelo de linguagem faz exatamente isso, mas de forma matemática!\n",
    "\n",
    "### Definição técnica (mais precisa)\n",
    "\n",
    "Tecnicamente, um modelo de linguagem é uma função matemática que **calcula a probabilidade** de uma sequência de palavras aparecer em um idioma. Por exemplo:\n",
    "\n",
    "- \"Bom dia\" tem alta probabilidade (frase comum)\n",
    "- \"Dia bom\" tem menor probabilidade (menos comum)\n",
    "- \"Azul comer\" tem probabilidade quase zero (não faz sentido)\n",
    "\n",
    "### Onde encontramos modelos de linguagem?\n",
    "\n",
    "Você provavelmente já usou modelos de linguagem hoje, mesmo sem saber:\n",
    "\n",
    "**No seu celular:**\n",
    "- Corretor automático que sugere palavras\n",
    "- Teclado que completa suas frases\n",
    "- Tradução instantânea de mensagens\n",
    "\n",
    "**Na internet:**\n",
    "- Busca do Google que entende suas perguntas\n",
    "- Legendas automáticas do YouTube\n",
    "- Sugestões de email (\"Obrigado pela mensagem...\")\n",
    "\n",
    "**Assistentes virtuais:**\n",
    "- Siri, Google Assistant, Alexa\n",
    "- ChatGPT e outros chatbots\n",
    "\n",
    "### Por que são importantes?\n",
    "\n",
    "Os modelos de linguagem são fundamentais porque permitem que computadores:\n",
    "1. **Compreendam** textos escritos por humanos\n",
    "2. **Gerem** textos que parecem naturais\n",
    "3. **Traduzam** entre diferentes idiomas\n",
    "4. **Respondam** perguntas sobre textos\n",
    "\n",
    "Isso abriu as portas para uma revolução na forma como interagimos com computadores!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a43431",
   "metadata": {
    "id": "63a43431"
   },
   "source": [
    "## Linha do tempo e evolução histórica\n",
    "\n",
    "A história dos modelos de linguagem é fascinante! Vamos fazer uma viagem no tempo para entender como chegamos até aqui.\n",
    "\n",
    "### Era dos Pioneiros (1950-1960)\n",
    "\n",
    "**1950 - O Teste de Turing**\n",
    "Alan Turing fez uma pergunta revolucionária: \"Máquinas podem pensar?\" Ele propôs um teste: se um computador conseguir conversar de forma indistinguível de um humano, poderíamos dizer que ele \"pensa\".\n",
    "\n",
    "**1954 - Primeira Tradução Automática**\n",
    "Cientistas americanos conseguiram traduzir 60 frases do russo para o inglês usando um computador. Era muito simples, mas funcionava. Eles achavam que em poucos anos resolveriam a tradução completamente (spoiler: erraram rude!).\n",
    "\n",
    "**1966 - ELIZA, o Primeiro Chatbot**\n",
    "ELIZA foi um dos primeiros programas que \"conversava\" com humanos. Era como um psicólogo virtual muito simples, mas algumas pessoas realmente acreditavam estar falando com um humano!\n",
    "\n",
    "### Era Estatística (1980-1990)\n",
    "\n",
    "**Anos 1980-1990 - A Revolução dos Números**\n",
    "Os cientistas descobriram que podiam ensinar computadores a prever palavras contando quantas vezes elas apareciam juntas em textos grandes. Era como ensinar estatística para máquinas!\n",
    "\n",
    "**Exemplo simples**: Se em 1000 textos a palavra \"gato\" aparece depois de \"o\" 300 vezes, então há 30% de chance da próxima palavra após \"o\" ser \"gato\".\n",
    "\n",
    "### Era Neural Inicial (2000-2010)\n",
    "\n",
    "**2003 - Redes Neurais Superam Estatística**\n",
    "Pela primeira vez, programas inspirados no cérebro humano (redes neurais) conseguiram resultados melhores que os métodos estatísticos tradicionais.\n",
    "\n",
    "**2010 - Word2Vec: Palavras Viram Números**\n",
    "Tomáš Mikolov criou uma forma genial de transformar palavras em números matemáticos. O incrível é que palavras similares ficavam com números similares! Por exemplo: \"rei\" - \"homem\" + \"mulher\" = \"rainha\".\n",
    "\n",
    "### Era Moderna (2017-hoje)\n",
    "\n",
    "**2017 - A Revolução Transformer**\n",
    "Um grupo de pesquisadores do Google criou uma nova arquitetura chamada \"Transformer\". Era como dar superpoderes para os modelos de linguagem - eles conseguiam \"prestar atenção\" em todas as palavras de uma frase simultaneamente.\n",
    "\n",
    "**2018 - Nascimento dos Gigantes: BERT e GPT**\n",
    "- **BERT**: Criado pelo Google, era especialista em entender textos\n",
    "- **GPT**: Criado pela OpenAI, era especialista em gerar textos\n",
    "\n",
    "**2020 em diante - A Era dos Modelos Gigantes**\n",
    "- GPT-3: 175 bilhões de parâmetros (como 175 bilhões de \"neurônios artificiais\")\n",
    "- GPT-4: Ainda maior e mais capaz\n",
    "- Outros modelos gigantes surgiram, mudando o mundo da tecnologia\n",
    "\n",
    "### O que isso significa?\n",
    "\n",
    "Estamos vivendo um momento histórico! Os modelos de linguagem de hoje conseguem:\n",
    "- Escrever textos, poemas e até código\n",
    "- Responder perguntas complexas\n",
    "- Ter conversas que parecem naturais\n",
    "- Ajudar em tarefas criativas\n",
    "\n",
    "![Evolucao dos modelos](https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/evolucao_modelos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108b432",
   "metadata": {
    "id": "d108b432"
   },
   "source": [
    "## 0. Do **texto** aos **números**\n",
    "\n",
    "Antes de falar em Bag-of-Words, precisamos entender **como** um texto cru vira matéria-prima numérica para o computador.\n",
    "\n",
    "| Etapa | O que faz | Por quê? |\n",
    "|-------|-----------|----------|\n",
    "| **Tokenização** | Quebra a sequência em unidades menores, chamadas *tokens* (geralmente palavras). | O modelo trabalha palavra a palavra — precisa saber *onde* cada palavra começa e termina. |\n",
    "| **Limpeza** | Converte para minúsculas, remove pontuação, números, emojis, etc. | Uniformiza o texto e evita que “Gato” ≠ “gato”. |\n",
    "| **Remoção de stopwords** | Elimina palavras muito frequentes e pouco informativas (“o”, “de”, “e” …). | Reduz ruído e tamanho do vocabulário. |\n",
    "| *(Opcional)* **Stemming / Lemmatização** | Reduz palavras ao seu “tronco” (“correndo” → “corr”) ou à forma canônica (“cães” → “cão”). | Agrupa variações da mesma raiz, diminuindo a dispersão. |\n",
    "| **Vocabulário** `V` | Conjunto *único* de todos os tokens restantes. | Cada termo ganhará uma coluna (dimensão) no vetor. |\n",
    "| **Vetorização** | Mapeia cada documento a um vetor de tamanho \\(|V|\\). | Finalmente transforma texto em números! |\n",
    "\n",
    "### Exemplo rápido\n",
    "\n",
    "> Texto original  \n",
    "> *“Os gatos pretos dormem no sofá!”*\n",
    "\n",
    "1. **Tokenização:** `[Os, gatos, pretos, dormem, no, sofá]`  \n",
    "2. **Limpeza + stopwords:** `[gatos, pretos, dormem, sofá]`  \n",
    "3. **Vocabulário parcial:** `V = {gatos, pretos, dormem, sofá, …}`  \n",
    "4. **Vetorização BoW (contagens):** \\((gatos=1,\\; pretos=1,\\; dormem=1,\\; sofá=1,\\dots)\\)\n",
    "\n",
    "A partir daqui podemos aplicar **Bag-of-Words** ou outros métodos (TF-IDF, embeddings, Transformers) para que modelos matemáticos “leiam” o conteúdo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896ea55",
   "metadata": {
    "id": "1896ea55"
   },
   "source": [
    "## 1. Representação **Bag-of-Words** (Sacola de Palavras)\n",
    "\n",
    "> “Computadores não entendem letras, apenas números.”  \n",
    "> **Bag-of-Words (BoW)** é o primeiro passo para transformar texto em vetores numéricos.\n",
    "\n",
    "### 1.1 Definição formal\n",
    "Considere um vocabulário  \n",
    "$$\n",
    "V = \\{w_1, w_2, \\dots, w_{|V|}\\}\n",
    "$$\n",
    "\n",
    "obtido do corpus. Para cada documento \\(d\\) construímos o vetor de contagens\n",
    "\n",
    "$$\n",
    "\\mathbf v^{(d)} = \\left[\\, c(w_1,d),\\; c(w_2,d),\\; \\dots,\\; c(w_{|V|},d) \\right],\n",
    "$$\n",
    "\n",
    "onde  \n",
    "\n",
    "$$\n",
    "c(w_i,d) = \\text{número de vezes que a palavra } w_i \\text{ aparece em } d.\n",
    "$$\n",
    "\n",
    "A posição *i* do vetor corresponde **sempre** à palavra \\(w_i\\); a ordem do texto original é descartada.\n",
    "\n",
    "### 1.2 Exemplo rápido\n",
    "| Documento                 | Vetor BoW (parcial)                  |\n",
    "|---------------------------|--------------------------------------|\n",
    "| “O gato subiu no telhado” | (o=1, gato=1, subiu=1, telhado=1,...) |\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/exemplo_bag_of_words.png\" width=\"55%\">\n",
    "\n",
    "### 1.3 Vantagens × Desvantagens\n",
    "| ✅ Vantagens | ❌ Desvantagens |\n",
    "|-------------|----------------|\n",
    "| Simples e rápido | Perde **ordem** e contexto |\n",
    "| Boa base para classificação | Vetores esparsos e grandes |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pesos **TF-IDF**\n",
    "\n",
    "### 2.1 Fórmulas\n",
    "\n",
    "**Term Frequency (no documento \\(d\\))**\n",
    "\n",
    "$$\n",
    "\\text{TF}(t,d) = \\frac{c(t,d)}{\\sum_{t'} c(t',d)}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (no corpus \\(D\\) com \\(N\\) documentos)**\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t,D) = \\log\\left(\\frac{N + 1}{\\text{df}(t)+1}\\right) + 1,\n",
    "$$\n",
    "\n",
    "onde \\(\\text{df}(t)\\) é o número de documentos que contêm \\(t\\).\n",
    "\n",
    "**Peso final**\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\times \\text{IDF}(t,D)\n",
    "$$\n",
    "\n",
    "### 2.2 Intuição\n",
    "* Valor alto → termo frequente em \\(d\\) **e** raro no corpus → caracteriza o documento.  \n",
    "* Valor baixo → stopwords ou termos muito comuns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c7f70",
   "metadata": {
    "id": "a95c7f70"
   },
   "outputs": [],
   "source": [
    "import nltk, math, pandas as pd, numpy as np\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Corpus de teste\n",
    "# ---------------------------------------------------------------------\n",
    "corpus = [\n",
    "    \"O gato preto dorme\",                        # Doc 1\n",
    "    \"O cachorro marrom corre\",                   # Doc 2\n",
    "    \"O gato marrom dorme no sofá\",               # Doc 3\n",
    "    \"Os pássaros voam no céu\",                   # Doc 4\n",
    "    \"As crianças brincam no parque\",             # Doc 5\n",
    "    \"O gato e o cachorro correm juntos\"          # Doc 6\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Pré-processamento com NLTK\n",
    "#    – tokenização, lowercase, remoção de stopwords e não-alfabéticos\n",
    "# ---------------------------------------------------------------------\n",
    "stop_pt = set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [tok.lower() for tok in nltk.word_tokenize(text) if tok.isalpha()]\n",
    "    return [tok for tok in tokens if tok not in stop_pt]\n",
    "\n",
    "tokenized_docs = [tokenize(doc) for doc in corpus]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Vocabulário e matriz Bag-of-Words\n",
    "# ---------------------------------------------------------------------\n",
    "vocab = sorted({tok for doc in tokenized_docs for tok in doc})\n",
    "def bow_vector(tokens, vocab):\n",
    "    counts = Counter(tokens)\n",
    "    return [counts.get(term, 0) for term in vocab]\n",
    "\n",
    "bow_matrix = np.array([bow_vector(doc, vocab) for doc in tokenized_docs])\n",
    "\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=vocab,\n",
    "                      index=[f\"Doc {i+1}\" for i in range(len(corpus))])\n",
    "\n",
    "print(\"\\n=== BAG-OF-WORDS ===\")\n",
    "print(bow_df)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Cálculo manual de TF-IDF\n",
    "#    TF = frequência absoluta\n",
    "#    IDF = log((N + 1) / (df + 1)) + 1      (suavizado)\n",
    "# ---------------------------------------------------------------------\n",
    "N = len(corpus)\n",
    "df = np.count_nonzero(bow_matrix > 0, axis=0)          # nº docs que contêm a palavra\n",
    "idf = np.log((N + 1) / (df + 1)) + 1\n",
    "tf_idf_matrix = bow_matrix * idf\n",
    "\n",
    "tfidf_df = pd.DataFrame(np.round(tf_idf_matrix, 3), columns=vocab,\n",
    "                        index=[f\"Doc {i+1}\" for i in range(N)])\n",
    "\n",
    "print(\"\\n=== TF-IDF ===\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Interpretação rápida\n",
    "# ---------------------------------------------------------------------\n",
    "palavra = \"sofá\"\n",
    "doc_idx = 2  # Doc 3 (índice começa em zero)\n",
    "\n",
    "print(f\"\\nComparação da palavra '{palavra}':\")\n",
    "print(f\"  - Frequência em Doc 3 (BoW): {bow_df.loc[f'Doc {doc_idx+1}', palavra]}\")\n",
    "print(f\"  - Importância em Doc 3 (TF-IDF): {tfidf_df.loc[f'Doc {doc_idx+1}', palavra]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22c4a5",
   "metadata": {
    "id": "cb22c4a5"
   },
   "source": [
    "## Modelos N-gramas: Prevendo a Próxima Palavra\n",
    "\n",
    "Agora vamos dar um grande passo! Em vez de apenas contar palavras, vamos ensinar o computador a **prever** qual palavra vem em seguida.\n",
    "\n",
    "### O que são N-gramas?\n",
    "\n",
    "**Analogia simples:** Imagine que você está jogando um jogo onde precisa adivinhar a próxima palavra de uma frase. Para isso, você pode olhar:\n",
    "\n",
    "- **Unigrama (n=1)**: Apenas uma palavra → \"gato\" → qual a próxima?\n",
    "- **Bigrama (n=2)**: Duas palavras → \"o gato\" → qual a próxima?\n",
    "- **Trigrama (n=3)**: Três palavras → \"o gato preto\" → qual a próxima?\n",
    "\n",
    "Quanto mais contexto (palavras anteriores) você tem, melhor consegue adivinhar!\n",
    "\n",
    "### Como funciona um modelo de bigrama?\n",
    "\n",
    "O modelo bigrama olha **pares de palavras** que aparecem juntas e calcula probabilidades:\n",
    "\n",
    "**Exemplo prático:**\n",
    "Se no seu corpus você encontra:\n",
    "- \"o gato\" aparece 10 vezes\n",
    "- \"o cachorro\" aparece 5 vezes  \n",
    "- \"o\" aparece 20 vezes no total\n",
    "\n",
    "Então:\n",
    "- P(gato | o) = 10/20 = 50%\n",
    "- P(cachorro | o) = 5/20 = 25%\n",
    "\n",
    "**Interpretação:** Depois da palavra \"o\", há 50% de chance da próxima ser \"gato\".\n",
    "\n",
    "### Problema: E se nunca vimos uma combinação?\n",
    "\n",
    "**Cenário:** E se perguntarmos P(elefante | o) mas nunca vimos \"o elefante\" no corpus?\n",
    "\n",
    "**Solução - Suavização:** Adicionamos um \"pouquinho\" de probabilidade para todas as combinações possíveis. É como dar uma \"segunda chance\" para combinações não vistas.\n",
    "\n",
    "### Por que isso foi revolucionário?\n",
    "\n",
    "Por décadas (1990-2010), os N-gramas foram a base de:\n",
    "- Tradutores automáticos (Google Translate antigo)\n",
    "- Reconhecimento de fala (Siri primeira versão)\n",
    "- Corretores ortográficos\n",
    "\n",
    "Vamos implementar um modelo bigrama e ver ele funcionando!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe3937",
   "metadata": {
    "id": "72fe3937"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import math\n",
    "\n",
    "print(\"CONSTRUINDO UM MODELO BIGRAMA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Nosso corpus de treinamento (pequeno, mas funcional!)\n",
    "corpus = [\n",
    "    'o gato preto dorme',\n",
    "    'o cachorro marrom corre',\n",
    "    'o gato marrom dorme no sofá',\n",
    "    'o gato corre no quintal'\n",
    "]\n",
    "\n",
    "print(\"CORPUS DE TREINAMENTO:\")\n",
    "for i, frase in enumerate(corpus):\n",
    "    print(f\"{i+1}. {frase}\")\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Função para separar um texto em palavras individuais\"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "print(\"\\nPASSO 1: PREPARANDO OS DADOS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Vamos contar todos os bigramas (pares de palavras consecutivas)\n",
    "bigram_counts = defaultdict(Counter)  # Para contar bigramas\n",
    "unigram_counts = Counter()            # Para contar palavras individuais\n",
    "vocabulario = set()                   # Para guardar todas as palavras únicas\n",
    "\n",
    "for frase in corpus:\n",
    "    # Separar a frase em palavras\n",
    "    palavras = tokenize(frase)\n",
    "\n",
    "    # Adicionar marcadores de início e fim de frase\n",
    "    palavras = ['<INÍCIO>'] + palavras + ['<FIM>']\n",
    "\n",
    "    # Adicionar todas as palavras ao vocabulário\n",
    "    vocabulario.update(palavras)\n",
    "\n",
    "    # Contar bigramas (pares consecutivos)\n",
    "    for i in range(len(palavras) - 1):\n",
    "        palavra_atual = palavras[i]\n",
    "        próxima_palavra = palavras[i + 1]\n",
    "\n",
    "        # Contar este bigrama\n",
    "        bigram_counts[palavra_atual][próxima_palavra] += 1\n",
    "        # Contar a palavra atual\n",
    "        unigram_counts[palavra_atual] += 1\n",
    "\n",
    "    # Não esquecer de contar a última palavra\n",
    "    unigram_counts[palavras[-1]] += 1\n",
    "\n",
    "# Tamanho do vocabulário (para suavização)\n",
    "V = len(vocabulario)\n",
    "\n",
    "print(f\"Vocabulário total: {V} palavras únicas\")\n",
    "print(f\"Palavras do vocabulário: {sorted(vocabulario)}\")\n",
    "\n",
    "print(\"\\nPASSO 2: EXEMPLOS DE BIGRAMAS ENCONTRADOS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Alguns bigramas mais comuns:\")\n",
    "for palavra, próximas in list(bigram_counts.items())[:3]:\n",
    "    print(f\"Depois de '{palavra}':\")\n",
    "    for próxima, count in próximas.items():\n",
    "        print(f\"  - '{próxima}' aparece {count} vez(es)\")\n",
    "\n",
    "def probabilidade_bigrama(palavra_anterior, palavra_atual):\n",
    "    \"\"\"\n",
    "    Calcula P(palavra_atual | palavra_anterior) com suavização add-one\n",
    "    \"\"\"\n",
    "    # Suavização: adicionamos 1 a todas as contagens\n",
    "    numerador = bigram_counts[palavra_anterior][palavra_atual] + 1\n",
    "    denominador = unigram_counts[palavra_anterior] + V\n",
    "    return numerador / denominador\n",
    "\n",
    "print(\"\\nPASSO 3: TESTANDO PROBABILIDADES\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Testando algumas probabilidades\n",
    "exemplos_teste = [\n",
    "    ('o', 'gato'),\n",
    "    ('o', 'cachorro'),\n",
    "    ('gato', 'preto'),\n",
    "    ('o', 'elefante')  # Este nunca apareceu!\n",
    "]\n",
    "\n",
    "for anterior, atual in exemplos_teste:\n",
    "    prob = probabilidade_bigrama(anterior, atual)\n",
    "    print(f\"P('{atual}' | '{anterior}') = {prob:.4f} = {prob*100:.2f}%\")\n",
    "\n",
    "def probabilidade_frase(frase):\n",
    "    \"\"\"Calcula a probabilidade total de uma frase\"\"\"\n",
    "    palavras = ['<INÍCIO>'] + tokenize(frase) + ['<FIM>']\n",
    "    log_prob = 0.0\n",
    "\n",
    "    for i in range(len(palavras) - 1):\n",
    "        prob = probabilidade_bigrama(palavras[i], palavras[i + 1])\n",
    "        log_prob += math.log(prob)\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "print(\"\\nPASSO 4: TESTANDO FRASES COMPLETAS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "frases_teste = [\n",
    "    'o gato marrom corre',    # Frase similar ao treinamento\n",
    "    'o elefante verde voa'    # Frase totalmente nova\n",
    "]\n",
    "\n",
    "for frase in frases_teste:\n",
    "    prob_log = probabilidade_frase(frase)\n",
    "    print(f\"Frase: '{frase}'\")\n",
    "    print(f\"Probabilidade (log): {prob_log:.2f}\")\n",
    "    print(f\"Probabilidade real: {math.exp(prob_log):.2e}\")\n",
    "    print()\n",
    "\n",
    "def gerar_frase_aleatoria(max_palavras=8):\n",
    "    \"\"\"Gera uma frase aleatória usando o modelo bigrama\"\"\"\n",
    "    frase = []\n",
    "    palavra_atual = '<INÍCIO>'\n",
    "\n",
    "    for _ in range(max_palavras):\n",
    "        # Pegar todas as palavras possíveis após a palavra atual\n",
    "        palavras_possíveis = list(bigram_counts[palavra_atual].keys())\n",
    "\n",
    "        if not palavras_possíveis:  # Se não há continuação conhecida\n",
    "            break\n",
    "\n",
    "        # Calcular probabilidades para cada palavra possível\n",
    "        probabilidades = [probabilidade_bigrama(palavra_atual, p) for p in palavras_possíveis]\n",
    "\n",
    "        # Escolher aleatoriamente baseado nas probabilidades\n",
    "        palavra_atual = random.choices(palavras_possíveis, weights=probabilidades, k=1)[0]\n",
    "\n",
    "        if palavra_atual == '<FIM>':\n",
    "            break\n",
    "\n",
    "        frase.append(palavra_atual)\n",
    "\n",
    "    return ' '.join(frase)\n",
    "\n",
    "print(\"PASSO 5: GERANDO TEXTO AUTOMÁTICO!\")\n",
    "print(\"-\" * 35)\n",
    "print(\"Vamos ver o que nosso modelo aprendeu a escrever:\")\n",
    "\n",
    "for i in range(5):\n",
    "    frase_gerada = gerar_frase_aleatoria()\n",
    "    print(f\"{i+1}. {frase_gerada}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
