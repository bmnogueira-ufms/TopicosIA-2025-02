{"cells":[{"cell_type":"markdown","id":"9713c578","metadata":{"id":"9713c578"},"source":["# Instalação dos pacotes necessários / Configuração da máquina"]},{"cell_type":"code","execution_count":null,"id":"281ddc7b","metadata":{"id":"281ddc7b"},"outputs":[],"source":["# Instalação das dependências localmente\n","# Se estiver rodando localmente, descomente a linha abaixo para instalar as dependências\n","# ! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"id":"c264ce1a","metadata":{"id":"c264ce1a"},"outputs":[],"source":["# Se rodando no Google Colab, descomente a linha abaixo para montar o Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"id":"dUQdJBN-LKeQ","metadata":{"id":"dUQdJBN-LKeQ"},"outputs":[],"source":["# Instalação das dependências no Google Colab\n","# Mude CAMINHO_PARA_REPO para o caminho correto do seu repositório no seu Google Drive\n","# ! pip install -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt"]},{"cell_type":"markdown","id":"a7988806","metadata":{"id":"a7988806"},"source":["# Introdução aos modelos de linguagem\n","\n","Imagine um computador que consegue:\n","- Entender quando você escreve uma frase em português\n","- Prever qual palavra você vai digitar em seguida\n","- Traduzir um texto de inglês para português\n","- Escrever um texto que parece ter sido escrito por um humano\n","\n","Isso tudo é possível graças aos **modelos de linguagem**! Eles são programas especiais que \"aprendem\" como a linguagem humana funciona.\n","\n","### Por que isso é importante?\n","\n","Os modelos de linguagem estão por trás de muitas tecnologias que você usa no dia a dia:\n","- O corretor automático do seu celular\n","- O Google Tradutor\n","- Assistentes virtuais como Siri e Alexa\n","- O ChatGPT que tem feito tanto sucesso\n","\n","### O que faremos nesta aula?\n","\n","1. Vamos entender **o que são** modelos de linguagem de forma simples\n","2. Conhecer a **história** dessa tecnologia (é mais antiga do que você imagina!)\n","3. Implementar nossos **primeiros modelos** em Python"]},{"cell_type":"markdown","id":"f042e867","metadata":{"id":"f042e867"},"source":["## Sumário\n","1. [Modelos de linguagem: definição e aplicações](#Modelos-de-linguagem:-definição-e-aplicações)\n","2. [Linha do tempo e evolução histórica](#Linha-do-tempo-e-evolução-histórica)\n","3. [Representação Bag‑of‑Words](#Representação-Bag‑of‑Words)\n","4. [Modelos n‑gramas](#Modelos-n‑gramas)"]},{"cell_type":"markdown","id":"c4d651dc","metadata":{"id":"c4d651dc"},"source":["## Modelos de linguagem: definição e aplicações\n","\n","### O que é um modelo de linguagem?\n","\n","Vamos começar com uma **definição simples**:\n","\n","> Um modelo de linguagem é como um \"estudante\" muito dedicado que leu milhões de textos e aprendeu a prever quais palavras fazem sentido juntas.\n","\n","Imagine que você está escrevendo a frase \"O gato subiu no...\" - sua mente automaticamente pensa em palavras como \"telhado\", \"muro\" ou \"sofá\". Você sabe que \"O gato subiu no elefante\" seria estranho. Um modelo de linguagem faz exatamente isso, mas de forma matemática!\n","\n","### Definição técnica (mais precisa)\n","\n","Tecnicamente, um modelo de linguagem é uma função matemática que **calcula a probabilidade** de uma sequência de palavras aparecer em um idioma. Por exemplo:\n","\n","- \"Bom dia\" tem alta probabilidade (frase comum)\n","- \"Dia bom\" tem menor probabilidade (menos comum)\n","- \"Azul comer\" tem probabilidade quase zero (não faz sentido)\n","\n","### Onde encontramos modelos de linguagem?\n","\n","Você provavelmente já usou modelos de linguagem hoje, mesmo sem saber:\n","\n","**No seu celular:**\n","- Corretor automático que sugere palavras\n","- Teclado que completa suas frases\n","- Tradução instantânea de mensagens\n","\n","**Na internet:**\n","- Busca do Google que entende suas perguntas\n","- Legendas automáticas do YouTube\n","- Sugestões de email (\"Obrigado pela mensagem...\")\n","\n","**Assistentes virtuais:**\n","- Siri, Google Assistant, Alexa\n","- ChatGPT e outros chatbots\n","\n","### Por que são importantes?\n","\n","Os modelos de linguagem são fundamentais porque permitem que computadores:\n","1. **Compreendam** textos escritos por humanos\n","2. **Gerem** textos que parecem naturais\n","3. **Traduzam** entre diferentes idiomas\n","4. **Respondam** perguntas sobre textos\n","\n","Isso abriu as portas para uma revolução na forma como interagimos com computadores!"]},{"cell_type":"markdown","id":"63a43431","metadata":{"id":"63a43431"},"source":["## Linha do tempo e evolução histórica\n","\n","A história dos modelos de linguagem é fascinante! Vamos fazer uma viagem no tempo para entender como chegamos até aqui.\n","\n","### Era dos Pioneiros (1950-1960)\n","\n","**1950 - O Teste de Turing**\n","Alan Turing fez uma pergunta revolucionária: \"Máquinas podem pensar?\" Ele propôs um teste: se um computador conseguir conversar de forma indistinguível de um humano, poderíamos dizer que ele \"pensa\".\n","\n","**1954 - Primeira Tradução Automática**\n","Cientistas americanos conseguiram traduzir 60 frases do russo para o inglês usando um computador. Era muito simples, mas funcionava. Eles achavam que em poucos anos resolveriam a tradução completamente (spoiler: erraram rude!).\n","\n","**1966 - ELIZA, o Primeiro Chatbot**\n","ELIZA foi um dos primeiros programas que \"conversava\" com humanos. Era como um psicólogo virtual muito simples, mas algumas pessoas realmente acreditavam estar falando com um humano!\n","\n","### Era Estatística (1980-1990)\n","\n","**Anos 1980-1990 - A Revolução dos Números**\n","Os cientistas descobriram que podiam ensinar computadores a prever palavras contando quantas vezes elas apareciam juntas em textos grandes. Era como ensinar estatística para máquinas!\n","\n","**Exemplo simples**: Se em 1000 textos a palavra \"gato\" aparece depois de \"o\" 300 vezes, então há 30% de chance da próxima palavra após \"o\" ser \"gato\".\n","\n","### Era Neural Inicial (2000-2010)\n","\n","**2003 - Redes Neurais Superam Estatística**\n","Pela primeira vez, programas inspirados no cérebro humano (redes neurais) conseguiram resultados melhores que os métodos estatísticos tradicionais.\n","\n","**2010 - Word2Vec: Palavras Viram Números**\n","Tomáš Mikolov criou uma forma genial de transformar palavras em números matemáticos. O incrível é que palavras similares ficavam com números similares! Por exemplo: \"rei\" - \"homem\" + \"mulher\" = \"rainha\".\n","\n","### Era Moderna (2017-hoje)\n","\n","**2017 - A Revolução Transformer**\n","Um grupo de pesquisadores do Google criou uma nova arquitetura chamada \"Transformer\". Era como dar superpoderes para os modelos de linguagem - eles conseguiam \"prestar atenção\" em todas as palavras de uma frase simultaneamente.\n","\n","**2018 - Nascimento dos Gigantes: BERT e GPT**\n","- **BERT**: Criado pelo Google, era especialista em entender textos\n","- **GPT**: Criado pela OpenAI, era especialista em gerar textos\n","\n","**2020 em diante - A Era dos Modelos Gigantes**\n","- GPT-3: 175 bilhões de parâmetros (como 175 bilhões de \"neurônios artificiais\")\n","- GPT-4: Ainda maior e mais capaz\n","- Outros modelos gigantes surgiram, mudando o mundo da tecnologia\n","\n","### O que isso significa?\n","\n","Estamos vivendo um momento histórico! Os modelos de linguagem de hoje conseguem:\n","- Escrever textos, poemas e até código\n","- Responder perguntas complexas\n","- Ter conversas que parecem naturais\n","- Ajudar em tarefas criativas\n","\n","![Evolucao dos modelos](https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/evolucao_modelos.png)"]},{"cell_type":"markdown","id":"d108b432","metadata":{"id":"d108b432"},"source":["## 0. Do **texto** aos **números**\n","\n","Antes de falar em Bag-of-Words, precisamos entender **como** um texto cru vira matéria-prima numérica para o computador.\n","\n","| Etapa | O que faz | Por quê? |\n","|-------|-----------|----------|\n","| **Tokenização** | Quebra a sequência em unidades menores, chamadas *tokens* (geralmente palavras). | O modelo trabalha palavra a palavra — precisa saber *onde* cada palavra começa e termina. |\n","| **Limpeza** | Converte para minúsculas, remove pontuação, números, emojis, etc. | Uniformiza o texto e evita que “Gato” ≠ “gato”. |\n","| **Remoção de stopwords** | Elimina palavras muito frequentes e pouco informativas (“o”, “de”, “e” …). | Reduz ruído e tamanho do vocabulário. |\n","| *(Opcional)* **Stemming / Lemmatização** | Reduz palavras ao seu “tronco” (“correndo” → “corr”) ou à forma canônica (“cães” → “cão”). | Agrupa variações da mesma raiz, diminuindo a dispersão. |\n","| **Vocabulário** `V` | Conjunto *único* de todos os tokens restantes. | Cada termo ganhará uma coluna (dimensão) no vetor. |\n","| **Vetorização** | Mapeia cada documento a um vetor de tamanho \\(|V|\\). | Finalmente transforma texto em números! |\n","\n","### Exemplo rápido\n","\n","> Texto original  \n","> *“Os gatos pretos dormem no sofá!”*\n","\n","1. **Tokenização:** `[Os, gatos, pretos, dormem, no, sofá]`  \n","2. **Limpeza + stopwords:** `[gatos, pretos, dormem, sofá]`  \n","3. **Vocabulário parcial:** `V = {gatos, pretos, dormem, sofá, …}`  \n","4. **Vetorização BoW (contagens):** \\((gatos=1,\\; pretos=1,\\; dormem=1,\\; sofá=1,\\dots)\\)\n","\n","A partir daqui podemos aplicar **Bag-of-Words** ou outros métodos (TF-IDF, embeddings, Transformers) para que modelos matemáticos “leiam” o conteúdo.\n"]},{"cell_type":"markdown","id":"1896ea55","metadata":{"id":"1896ea55"},"source":["## 1. Representação **Bag-of-Words** (Sacola de Palavras)\n","\n","> “Computadores não entendem letras, apenas números.”  \n","> **Bag-of-Words (BoW)** é o primeiro passo para transformar texto em vetores numéricos.\n","\n","### 1.1 Definição formal\n","Considere um vocabulário  \n","$$\n","V = \\{w_1, w_2, \\dots, w_{|V|}\\}\n","$$\n","\n","obtido do corpus. Para cada documento \\(d\\) construímos o vetor de contagens\n","\n","$$\n","\\mathbf v^{(d)} = \\left[\\, c(w_1,d),\\; c(w_2,d),\\; \\dots,\\; c(w_{|V|},d) \\right],\n","$$\n","\n","onde  \n","\n","$$\n","c(w_i,d) = \\text{número de vezes que a palavra } w_i \\text{ aparece em } d.\n","$$\n","\n","A posição *i* do vetor corresponde **sempre** à palavra \\(w_i\\); a ordem do texto original é descartada.\n","\n","### 1.2 Exemplo rápido\n","| Documento                 | Vetor BoW (parcial)                  |\n","|---------------------------|--------------------------------------|\n","| “O gato subiu no telhado” | (o=1, gato=1, subiu=1, telhado=1,...) |\n","\n","<br/>\n","<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/exemplo_bag_of_words.png\" width=\"55%\">\n","\n","### 1.3 Vantagens × Desvantagens\n","| ✅ Vantagens | ❌ Desvantagens |\n","|-------------|----------------|\n","| Simples e rápido | Perde **ordem** e contexto |\n","| Boa base para classificação | Vetores esparsos e grandes |\n","\n","---\n","\n","## 2. Pesos **TF-IDF**\n","\n","### 2.1 Fórmulas\n","\n","**Term Frequency (no documento \\(d\\))**\n","\n","$$\n","\\text{TF}(t,d) = \\frac{c(t,d)}{\\sum_{t'} c(t',d)}\n","$$\n","\n","**Inverse Document Frequency (no corpus \\(D\\) com \\(N\\) documentos)**\n","\n","$$\n","\\text{IDF}(t,D) = \\log\\left(\\frac{N + 1}{\\text{df}(t)+1}\\right) + 1,\n","$$\n","\n","onde \\(\\text{df}(t)\\) é o número de documentos que contêm \\(t\\).\n","\n","**Peso final**\n","\n","$$\n","\\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\times \\text{IDF}(t,D)\n","$$\n","\n","### 2.2 Intuição\n","* Valor alto → termo frequente em \\(d\\) **e** raro no corpus → caracteriza o documento.  \n","* Valor baixo → stopwords ou termos muito comuns.\n","\n"]},{"cell_type":"code","execution_count":4,"id":"a95c7f70","metadata":{"id":"a95c7f70","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754617651756,"user_tz":240,"elapsed":1477,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"feeaf42b-baed-47c5-b483-9c7329cc6b3a"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import nltk, math, pandas as pd, numpy as np\n","from collections import Counter\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# 1. Corpus de teste\n","# ---------------------------------------------------------------------\n","corpus = [\n","    \"O gato preto dorme com o gato branco\",                        # Doc 1\n","    \"O cachorro marrom corre atrás do gato\",                   # Doc 2\n","    \"O gato marrom dorme no sofá\",               # Doc 3\n","    \"Os pássaros voam no céu e. os gatos olham\",                   # Doc 4\n","    \"As crianças brincam no parque\",             # Doc 5\n","    \"O gato e o cachorro correm juntos\"          # Doc 6\n","]"],"metadata":{"id":"KpLcsBCnjGwL","executionInfo":{"status":"ok","timestamp":1754617869547,"user_tz":240,"elapsed":4,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}}},"id":"KpLcsBCnjGwL","execution_count":9,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# 2. Pré-processamento com NLTK\n","#    – tokenização, lowercase, remoção de stopwords e não-alfabéticos\n","# ---------------------------------------------------------------------\n","stop_pt = set(nltk.corpus.stopwords.words(\"portuguese\"))\n","\n","def tokenize(text):\n","    tokens = [tok.lower() for tok in nltk.word_tokenize(text) if tok.isalpha()]\n","    return [tok for tok in tokens if tok not in stop_pt]\n","\n","tokenized_docs = [tokenize(doc) for doc in corpus]\n","tokenized_docs"],"metadata":{"id":"z-MAbL5djJEQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754617870767,"user_tz":240,"elapsed":10,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"4cf6f84e-3bac-4c32-b6ef-352f4b79debf"},"id":"z-MAbL5djJEQ","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['gato', 'preto', 'dorme', 'gato', 'branco'],\n"," ['cachorro', 'marrom', 'corre', 'atrás', 'gato'],\n"," ['gato', 'marrom', 'dorme', 'sofá'],\n"," ['pássaros', 'voam', 'céu', 'gatos', 'olham'],\n"," ['crianças', 'brincam', 'parque'],\n"," ['gato', 'cachorro', 'correm', 'juntos']]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# 3. Vocabulário e matriz Bag-of-Words\n","# ---------------------------------------------------------------------\n","vocab = sorted({tok for doc in tokenized_docs for tok in doc})\n","def bow_vector(tokens, vocab):\n","    counts = Counter(tokens)\n","    return [counts.get(term, 0) for term in vocab]\n","\n","bow_matrix = np.array([bow_vector(doc, vocab) for doc in tokenized_docs])\n","\n","bow_df = pd.DataFrame(bow_matrix, columns=vocab,\n","                      index=[f\"Doc {i+1}\" for i in range(len(corpus))])\n","\n","print(\"\\n=== BAG-OF-WORDS ===\")\n","print(bow_df)"],"metadata":{"id":"HgmiAiHajLLf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754617872496,"user_tz":240,"elapsed":3,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"4b80b2a6-bcee-4eff-f5f5-45f90510ba3c"},"id":"HgmiAiHajLLf","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== BAG-OF-WORDS ===\n","       atrás  branco  brincam  cachorro  corre  correm  crianças  céu  dorme  \\\n","Doc 1      0       1        0         0      0       0         0    0      1   \n","Doc 2      1       0        0         1      1       0         0    0      0   \n","Doc 3      0       0        0         0      0       0         0    0      1   \n","Doc 4      0       0        0         0      0       0         0    1      0   \n","Doc 5      0       0        1         0      0       0         1    0      0   \n","Doc 6      0       0        0         1      0       1         0    0      0   \n","\n","       gato  gatos  juntos  marrom  olham  parque  preto  pássaros  sofá  voam  \n","Doc 1     2      0       0       0      0       0      1         0     0     0  \n","Doc 2     1      0       0       1      0       0      0         0     0     0  \n","Doc 3     1      0       0       1      0       0      0         0     1     0  \n","Doc 4     0      1       0       0      1       0      0         1     0     1  \n","Doc 5     0      0       0       0      0       1      0         0     0     0  \n","Doc 6     1      0       1       0      0       0      0         0     0     0  \n"]}]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# 4. Cálculo manual de TF-IDF\n","#    TF = frequência absoluta\n","#    IDF = log((N + 1) / (df + 1)) + 1      (suavizado)\n","# ---------------------------------------------------------------------\n","N = len(corpus)\n","df = np.count_nonzero(bow_matrix > 0, axis=0)          # nº docs que contêm a palavra\n","idf = np.log((N + 1) / (df + 1)) + 1\n","tf_idf_matrix = bow_matrix * idf\n","\n","tfidf_df = pd.DataFrame(np.round(tf_idf_matrix, 3), columns=vocab,\n","                        index=[f\"Doc {i+1}\" for i in range(N)])\n","\n","print(\"\\n=== TF-IDF ===\")\n","print(tfidf_df)"],"metadata":{"id":"xnsyuMcBjN_4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754617881662,"user_tz":240,"elapsed":21,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"93c5be4a-6e04-4e64-ffe1-f4b5c991a43e"},"id":"xnsyuMcBjN_4","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== TF-IDF ===\n","       atrás  branco  brincam  cachorro  corre  correm  crianças    céu  \\\n","Doc 1  0.000   2.253    0.000     0.000  0.000   0.000     0.000  0.000   \n","Doc 2  2.253   0.000    0.000     1.847  2.253   0.000     0.000  0.000   \n","Doc 3  0.000   0.000    0.000     0.000  0.000   0.000     0.000  0.000   \n","Doc 4  0.000   0.000    0.000     0.000  0.000   0.000     0.000  2.253   \n","Doc 5  0.000   0.000    2.253     0.000  0.000   0.000     2.253  0.000   \n","Doc 6  0.000   0.000    0.000     1.847  0.000   2.253     0.000  0.000   \n","\n","       dorme   gato  gatos  juntos  marrom  olham  parque  preto  pássaros  \\\n","Doc 1  1.847  2.673  0.000   0.000   0.000  0.000   0.000  2.253     0.000   \n","Doc 2  0.000  1.336  0.000   0.000   1.847  0.000   0.000  0.000     0.000   \n","Doc 3  1.847  1.336  0.000   0.000   1.847  0.000   0.000  0.000     0.000   \n","Doc 4  0.000  0.000  2.253   0.000   0.000  2.253   0.000  0.000     2.253   \n","Doc 5  0.000  0.000  0.000   0.000   0.000  0.000   2.253  0.000     0.000   \n","Doc 6  0.000  1.336  0.000   2.253   0.000  0.000   0.000  0.000     0.000   \n","\n","        sofá   voam  \n","Doc 1  0.000  0.000  \n","Doc 2  0.000  0.000  \n","Doc 3  2.253  0.000  \n","Doc 4  0.000  2.253  \n","Doc 5  0.000  0.000  \n","Doc 6  0.000  0.000  \n"]}]},{"cell_type":"code","source":["# ---------------------------------------------------------------------\n","# 5. Interpretação rápida\n","# ---------------------------------------------------------------------\n","palavra = \"sofá\"\n","doc_idx = 2  # Doc 3 (índice começa em zero)\n","\n","print(f\"\\nComparação da palavra '{palavra}':\")\n","print(f\"  - Frequência em Doc 3 (BoW): {bow_df.loc[f'Doc {doc_idx+1}', palavra]}\")\n","print(f\"  - Importância em Doc 3 (TF-IDF): {tfidf_df.loc[f'Doc {doc_idx+1}', palavra]:.3f}\")"],"metadata":{"id":"6wcdP4fUjVDS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754617946082,"user_tz":240,"elapsed":8,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"fa482ab5-69ab-4736-a186-3bafa5b92693"},"id":"6wcdP4fUjVDS","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Comparação da palavra 'sofá':\n","  - Frequência em Doc 3 (BoW): 1\n","  - Importância em Doc 3 (TF-IDF): 2.253\n"]}]},{"cell_type":"markdown","id":"cb22c4a5","metadata":{"id":"cb22c4a5"},"source":["## Modelos N-gramas: Prevendo a Próxima Palavra\n","\n","Agora vamos dar um grande passo! Em vez de apenas contar palavras, vamos ensinar o computador a **prever** qual palavra vem em seguida.\n","\n","### O que são N-gramas?\n","\n","**Analogia simples:** Imagine que você está jogando um jogo onde precisa adivinhar a próxima palavra de uma frase. Para isso, você pode olhar:\n","\n","- **Unigrama (n=1)**: Apenas uma palavra → \"gato\" → qual a próxima?\n","- **Bigrama (n=2)**: Duas palavras → \"o gato\" → qual a próxima?\n","- **Trigrama (n=3)**: Três palavras → \"o gato preto\" → qual a próxima?\n","\n","Quanto mais contexto (palavras anteriores) você tem, melhor consegue adivinhar!\n","\n","### Como funciona um modelo de bigrama?\n","\n","O modelo bigrama olha **pares de palavras** que aparecem juntas e calcula probabilidades:\n","\n","**Exemplo prático:**\n","Se no seu corpus você encontra:\n","- \"o gato\" aparece 10 vezes\n","- \"o cachorro\" aparece 5 vezes  \n","- \"o\" aparece 20 vezes no total\n","\n","Então:\n","- P(gato | o) = 10/20 = 50%\n","- P(cachorro | o) = 5/20 = 25%\n","\n","**Interpretação:** Depois da palavra \"o\", há 50% de chance da próxima ser \"gato\".\n","\n","### Problema: E se nunca vimos uma combinação?\n","\n","**Cenário:** E se perguntarmos P(elefante | o) mas nunca vimos \"o elefante\" no corpus?\n","\n","**Solução - Suavização:** Adicionamos um \"pouquinho\" de probabilidade para todas as combinações possíveis. É como dar uma \"segunda chance\" para combinações não vistas.\n","\n","### Por que isso foi revolucionário?\n","\n","Por décadas (1990-2010), os N-gramas foram a base de:\n","- Tradutores automáticos (Google Translate antigo)\n","- Reconhecimento de fala (Siri primeira versão)\n","- Corretores ortográficos"]},{"cell_type":"code","execution_count":null,"id":"72fe3937","metadata":{"id":"72fe3937"},"outputs":[],"source":["import re\n","from collections import defaultdict, Counter\n","import random\n","import math\n","\n","print(\"CONSTRUINDO UM MODELO BIGRAMA\")\n","print(\"=\"*50)\n","\n","# Nosso corpus de treinamento (pequeno, mas funcional!)\n","corpus = [\n","    'o gato preto dorme',\n","    'o cachorro marrom corre',\n","    'o gato marrom dorme no sofá',\n","    'o gato corre no quintal'\n","]\n","\n","print(\"CORPUS DE TREINAMENTO:\")\n","for i, frase in enumerate(corpus):\n","    print(f\"{i+1}. {frase}\")\n","\n","def tokenize(text):\n","    \"\"\"Função para separar um texto em palavras individuais\"\"\"\n","    return re.findall(r'\\b\\w+\\b', text.lower())\n","\n","print(\"\\nPASSO 1: PREPARANDO OS DADOS\")\n","print(\"-\" * 30)\n","\n","# Vamos contar todos os bigramas (pares de palavras consecutivas)\n","bigram_counts = defaultdict(Counter)  # Para contar bigramas\n","unigram_counts = Counter()            # Para contar palavras individuais\n","vocabulario = set()                   # Para guardar todas as palavras únicas\n","\n","for frase in corpus:\n","    # Separar a frase em palavras\n","    palavras = tokenize(frase)\n","\n","    # Adicionar marcadores de início e fim de frase\n","    palavras = ['<INÍCIO>'] + palavras + ['<FIM>']\n","\n","    # Adicionar todas as palavras ao vocabulário\n","    vocabulario.update(palavras)\n","\n","    # Contar bigramas (pares consecutivos)\n","    for i in range(len(palavras) - 1):\n","        palavra_atual = palavras[i]\n","        próxima_palavra = palavras[i + 1]\n","\n","        # Contar este bigrama\n","        bigram_counts[palavra_atual][próxima_palavra] += 1\n","        # Contar a palavra atual\n","        unigram_counts[palavra_atual] += 1\n","\n","    # Não esquecer de contar a última palavra\n","    unigram_counts[palavras[-1]] += 1\n","\n","# Tamanho do vocabulário (para suavização)\n","V = len(vocabulario)\n","\n","print(f\"Vocabulário total: {V} palavras únicas\")\n","print(f\"Palavras do vocabulário: {sorted(vocabulario)}\")\n","\n","print(\"\\nPASSO 2: EXEMPLOS DE BIGRAMAS ENCONTRADOS\")\n","print(\"-\" * 40)\n","print(\"Alguns bigramas mais comuns:\")\n","for palavra, próximas in list(bigram_counts.items())[:3]:\n","    print(f\"Depois de '{palavra}':\")\n","    for próxima, count in próximas.items():\n","        print(f\"  - '{próxima}' aparece {count} vez(es)\")\n","\n","def probabilidade_bigrama(palavra_anterior, palavra_atual):\n","    \"\"\"\n","    Calcula P(palavra_atual | palavra_anterior) com suavização add-one\n","    \"\"\"\n","    # Suavização: adicionamos 1 a todas as contagens\n","    numerador = bigram_counts[palavra_anterior][palavra_atual] + 1\n","    denominador = unigram_counts[palavra_anterior] + V\n","    return numerador / denominador\n","\n","print(\"\\nPASSO 3: TESTANDO PROBABILIDADES\")\n","print(\"-\" * 35)\n","\n","# Testando algumas probabilidades\n","exemplos_teste = [\n","    ('o', 'gato'),\n","    ('o', 'cachorro'),\n","    ('gato', 'preto'),\n","    ('o', 'elefante')  # Este nunca apareceu!\n","]\n","\n","for anterior, atual in exemplos_teste:\n","    prob = probabilidade_bigrama(anterior, atual)\n","    print(f\"P('{atual}' | '{anterior}') = {prob:.4f} = {prob*100:.2f}%\")\n","\n","def probabilidade_frase(frase):\n","    \"\"\"Calcula a probabilidade total de uma frase\"\"\"\n","    palavras = ['<INÍCIO>'] + tokenize(frase) + ['<FIM>']\n","    log_prob = 0.0\n","\n","    for i in range(len(palavras) - 1):\n","        prob = probabilidade_bigrama(palavras[i], palavras[i + 1])\n","        log_prob += math.log(prob)\n","\n","    return log_prob\n","\n","print(\"\\nPASSO 4: TESTANDO FRASES COMPLETAS\")\n","print(\"-\" * 35)\n","\n","frases_teste = [\n","    'o gato marrom corre',    # Frase similar ao treinamento\n","    'o elefante verde voa'    # Frase totalmente nova\n","]\n","\n","for frase in frases_teste:\n","    prob_log = probabilidade_frase(frase)\n","    print(f\"Frase: '{frase}'\")\n","    print(f\"Probabilidade (log): {prob_log:.2f}\")\n","    print(f\"Probabilidade real: {math.exp(prob_log):.2e}\")\n","    print()\n","\n","def gerar_frase_aleatoria(max_palavras=8):\n","    \"\"\"Gera uma frase aleatória usando o modelo bigrama\"\"\"\n","    frase = []\n","    palavra_atual = '<INÍCIO>'\n","\n","    for _ in range(max_palavras):\n","        # Pegar todas as palavras possíveis após a palavra atual\n","        palavras_possíveis = list(bigram_counts[palavra_atual].keys())\n","\n","        if not palavras_possíveis:  # Se não há continuação conhecida\n","            break\n","\n","        # Calcular probabilidades para cada palavra possível\n","        probabilidades = [probabilidade_bigrama(palavra_atual, p) for p in palavras_possíveis]\n","\n","        # Escolher aleatoriamente baseado nas probabilidades\n","        palavra_atual = random.choices(palavras_possíveis, weights=probabilidades, k=1)[0]\n","\n","        if palavra_atual == '<FIM>':\n","            break\n","\n","        frase.append(palavra_atual)\n","\n","    return ' '.join(frase)\n","\n","print(\"PASSO 5: GERANDO TEXTO AUTOMÁTICO!\")\n","print(\"-\" * 35)\n","print(\"Vamos ver o que nosso modelo aprendeu a escrever:\")\n","\n","for i in range(5):\n","    frase_gerada = gerar_frase_aleatoria()\n","    print(f\"{i+1}. {frase_gerada}\")"]},{"cell_type":"markdown","source":["# N-gramas para palavras compostas no Bag-of-Words\n","\n","No Bag-of-Words (BoW) “padrão”, representamos cada documento por **unigramas** (palavras individuais). Isso pode perder informação importante quando o significado depende de termos **multi-palavra**, como *“inteligência artificial”*, *“banco de dados”* ou *“São Paulo”*.  \n","Para capturar essas **palavras compostas**, usamos **n-gramas**, isto é, sequências contíguas de *n* tokens:\n","- **Unigrama (n=1):** `[\"banco\", \"dados\", ...]`\n","- **Bigrama (n=2):** `[\"banco de\", \"de dados\", \"banco de dados\", ...]`\n","- **Trigrama (n=3):** `[\"aprendizado de máquina\", ...]`\n","\n","**Por que isso ajuda?**  \n","- Desambiguação: o unigrama “banco” é ambíguo; já **“banco de dados”** e **“banco da (praça)”** distinguem sentidos.  \n","- Expressões fixas: “inteligência artificial” costuma funcionar como uma unidade semântica.\n","\n","**Trade-offs e dicas práticas**  \n","- **Dimensão e esparsidade aumentam** rapidamente ao incluir bigramas/trigramas.  \n","- Comece com `ngram_range=(1,2)` (unigramas + bigramas) e controle o vocabulário com `min_df`, `max_df` ou `max_features`.  \n","- Se quiser ser mais criterioso, selecione apenas n-gramas frequentes ou com alta associação (ex.: PMI), mas isso já foge do BoW “puro”.\n","\n","A seguir, um exemplo gerando um BoW com unigramas e bigramas e mostrando como bigramas capturam palavras compostas."],"metadata":{"id":"Fg90kpPwmzdt"},"id":"Fg90kpPwmzdt"},{"cell_type":"code","source":["# BoW com unigramas e bigramas para capturar palavras compostas\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","corpus = [\n","    \"Modelos de linguagem são um subcampo de inteligência artificial.\",\n","    \"Aprendizado de máquina é aplicado em processamento de linguagem natural.\",\n","    \"A cidade de São Paulo abriga empresas de tecnologia.\",\n","    \"O banco de dados foi atualizado.\",\n","    \"O banco da praça foi pintado.\"\n","]\n","\n","# Apenas unigramas (baseline)\n","vec_uni = CountVectorizer(ngram_range=(1,1), lowercase=True)\n","X_uni = vec_uni.fit_transform(corpus)\n","\n","# Unigramas + bigramas (capturam palavras compostas)\n","vec_bi = CountVectorizer(ngram_range=(1,2), lowercase=True)\n","X_bi = vec_bi.fit_transform(corpus)\n","\n","print(\"Tamanho do vocabulário (unigramas):\", len(vec_uni.get_feature_names_out()))\n","print(\"Tamanho do vocabulário (até bigramas):\", len(vec_bi.get_feature_names_out()))\n"],"metadata":{"id":"Mj4HZ3Jxm2ZH"},"id":"Mj4HZ3Jxm2ZH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Vamos inspecionar alguns bigramas típicos de “palavras compostas”\n","bigrams_interesse = [\n","    \"inteligência artificial\",\n","    \"aprendizado de\",\n","    \"aprendizado de máquina\",\n","    \"são paulo\",\n","    \"banco de\",\n","    \"banco de dados\",\n","    \"banco da\"\n","]\n","\n","def counts_for(terms, vectorizer, X, corpus_size):\n","    rows = []\n","    for t in terms:\n","        if t in vectorizer.vocabulary_:\n","            idx = vectorizer.vocabulary_[t]\n","            counts = X[:, idx].toarray().ravel()\n","        else:\n","            counts = [0]*corpus_size\n","        rows.append(counts)\n","    df = pd.DataFrame(rows, index=terms, columns=[f\"doc{i+1}\" for i in range(corpus_size)])\n","    return df\n","\n","df_bi = counts_for(bigrams_interesse, vec_bi, X_bi, len(corpus))\n","\n","# Mostra apenas os termos que realmente apareceram como bigramas\n","df_bi_filtrado = df_bi[df_bi.sum(axis=1) > 0]\n","display(df_bi_filtrado)\n","\n"],"metadata":{"id":"P1uA20Cmm_Pn"},"id":"P1uA20Cmm_Pn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ex.: \"inteligência\" e \"artificial\" existem separadamente como unigramas,\n","# mas apenas o modelo com bigramas tem \"inteligência artificial\" como um único recurso.\n","terms_unigramas = [\"inteligência\", \"artificial\", \"banco\", \"dados\", \"praça\"]\n","def counts_uni(terms, vectorizer, X, corpus_size):\n","    rows = []\n","    for t in terms:\n","        if t in vectorizer.vocabulary_:\n","            idx = vectorizer.vocabulary_[t]\n","            counts = X[:, idx].toarray().ravel()\n","        else:\n","            counts = [0]*corpus_size\n","        rows.append(counts)\n","    return pd.DataFrame(rows, index=terms, columns=[f\"doc{i+1}\" for i in range(corpus_size)])\n","\n","df_uni = counts_uni(terms_unigramas, vec_uni, X_uni, len(corpus))\n","display(df_uni)\n","\n","# vec_bi = CountVectorizer(ngram_range=(1,2), min_df=2, max_df=0.8, max_features=30000)"],"metadata":{"id":"SUJIHt64nDh3"},"id":"SUJIHt64nDh3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelo Vetorial (Vector Space Model)\n","\n","A ideia central do **modelo vetorial** é representar cada documento (e também a consulta) como um **vetor de pesos** em um espaço de termos. Em vez de olhar só para presença/ausência, usamos pesos como **TF-IDF** para dar mais importância aos termos característicos.\n","\n","- **Vocabulário**: $ V = \\{t_1, \\dots, t_{|V|}\\} $  \n","- **Representação** do documento $d$: $ \\mathbf{d} = [w_1, \\dots, w_{|V|}] $, com $ w_i = \\mathrm{tf}(t_i, d)\\cdot \\mathrm{idf}(t_i) $.\n","\n","Uma forma comum (usada pelo `scikit-learn`) é:\n","$$\n","\\mathrm{idf}(t)=\\log\\!\\left(\\frac{N+1}{\\mathrm{df}(t)+1}\\right)+1\n","$$\n","onde $N$ é o número de documentos e $\\mathrm{df}(t)$ quantos documentos contêm $t$.\n","\n","Para medir relevância entre **consulta** $\\mathbf{q}$ e **documento** $\\mathbf{d}$, usamos a **similaridade do cosseno**:\n","$$\n","\\cos(\\mathbf{q}, \\mathbf{d})=\\frac{\\mathbf{q}\\cdot \\mathbf{d}}{\\lVert \\mathbf{q}\\rVert\\,\\lVert \\mathbf{d}\\rVert}\n","$$\n","\n","**Por que funciona bem?**\n","- **TF-IDF** destaca termos que são frequentes no documento, mas raros na coleção.\n","- **Normalização** (L2) reduz o viés por documentos longos.\n","- **N-gramas** (ex.: bigramas) permitem tratar **palavras compostas** como “inteligência artificial” e “banco de dados”.\n","\n","**Limitações e dicas**\n","- Ainda é **BoW** (perde ordem longa e sintaxe); sinônimos não são unidos.\n","- Controle o tamanho do vocabulário com `min_df`, `max_df` ou `max_features`.\n","- Para ir além: **LSA/SVD** (redução de dimensionalidade) ou **embeddings** neuronais."],"metadata":{"id":"m73w4dmCoCu8"},"id":"m73w4dmCoCu8"},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import pandas as pd\n","\n","corpus = [\n","    \"Modelos de linguagem são um subcampo de inteligência artificial.\",\n","    \"Aprendizado de máquina é aplicado em processamento de linguagem natural.\",\n","    \"A cidade de São Paulo abriga empresas de tecnologia.\",\n","    \"O banco de dados foi atualizado.\",\n","    \"O banco da praça foi pintado.\"\n","]\n","\n","# TF-IDF com unigramas + bigramas para capturar compostos\n","vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True)\n","X = vectorizer.fit_transform(corpus)\n","\n","def search(query, topk=5):\n","    q = vectorizer.transform([query])\n","    sims = cosine_similarity(q, X).ravel()\n","    idx = np.argsort(-sims)[:topk]\n","    return pd.DataFrame({\n","        \"rank\": np.arange(1, len(idx)+1),\n","        \"score\": np.round(sims[idx], 3),\n","        \"doc_id\": idx + 1,\n","        \"texto\": [corpus[i] for i in idx]\n","    })\n","\n","print(\"Dimensão do espaço vetorial (nº de features TF-IDF):\", X.shape[1])\n","\n","# Exemplos de consulta mostrando desambiguação via n-gramas\n","display(search(\"banco de dados\", topk=5))\n","display(search(\"banco da praça\", topk=5))\n","display(search(\"inteligência artificial\", topk=5))"],"metadata":{"id":"ueBc6tPc9rcC"},"id":"ueBc6tPc9rcC","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## O que é normalização L2?\n","\n","**Normalização L2** (ou normalização pela **norma Euclidiana**) é o processo de **escalar um vetor para ter norma 2 igual a 1**, preservando sua direção.  \n","Dado um vetor de características $\\mathbf{x} = (x_1,\\dots,x_n)$, sua versão normalizada é:\n","\n","$$\n","\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n","\\qquad\\text{e}\\qquad\n","\\hat{\\mathbf{x}} \\;=\\;\n","\\begin{cases}\n","\\displaystyle \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}, & \\|\\mathbf{x}\\|_2 \\neq 0 \\\\\n","\\mathbf{0}, & \\text{caso contrário}\n","\\end{cases}\n","$$\n","\n","### Intuição rápida\n","- Mantém a **direção** do vetor e ajusta apenas o **tamanho** para 1.\n","- Em textos (BoW/TF-IDF), reduz o viés de documentos longos: compara-se **perfil de termos**, não comprimento.\n","- Após L2, a **similaridade do cosseno** entre vetores vira o **produto interno** direto:\n","  $$\n","  \\cos(\\hat{\\mathbf{q}}, \\hat{\\mathbf{d}}) \\;=\\; \\hat{\\mathbf{q}}\\cdot \\hat{\\mathbf{d}}\n","  $$\n","\n","### Exemplo\n","Vetor $\\mathbf{x}=(3,4)$ tem $\\|\\mathbf{x}\\|_2=5$.  \n","Normalizado: $\\hat{\\mathbf{x}}=(0{,}6,\\,0{,}8)$.\n","\n","### L2 x outras normalizações\n","- **L1**: força $\\sum_i |x_i| = 1$ (soma dos valores absolutos).\n","- **Padronização (z-score)**: transforma cada **feature** para média 0 e desvio 1 — é outra coisa, não uma normalização de vetor.\n","- No `scikit-learn`, o `TfidfVectorizer` usa **`norm='l2'`** por padrão."],"metadata":{"id":"rveuW-TZo1Js"},"id":"rveuW-TZo1Js"},{"cell_type":"code","source":[],"metadata":{"id":"8PlK2ISGo0qj"},"id":"8PlK2ISGo0qj","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hGoqg_dxoUR3"},"id":"hGoqg_dxoUR3","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env-misc","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":5}