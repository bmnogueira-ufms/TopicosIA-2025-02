{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91876adb",
   "metadata": {},
   "source": [
    "# Aula 1.2 — Introdução a Embeddings - Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce10522",
   "metadata": {},
   "source": [
    "# Instalação dos pacotes necessários / Configuração da máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das dependências localmente\n",
    "# Se estiver rodando localmente, descomente a linha abaixo para instalar as dependências\n",
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc19317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se rodando no Google Colab, descomente a linha abaixo para montar o Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d486cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das dependências no Google Colab\n",
    "# Mude CAMINHO_PARA_REPO para o caminho correto do seu repositório no seu Google Drive\n",
    "# ! pip install -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491500a3",
   "metadata": {},
   "source": [
    "## Onde estávamos: representações vetoriais tradicionais\n",
    "- **One-hot (uma-quente):** vetor esparso com 1 na posição da palavra e 0 no resto.  \n",
    "- **BoW (bag-of-words):** conta frequências de palavras, **ignora ordem**.  \n",
    "- **n-gramas:** leva em conta sequências de tamanho *n* (ex.: bigramas), porém o espaço cresce muito.\n",
    "\n",
    "**Consequência:** essas representações são úteis, mas **não codificam semelhança semântica** (ex.: *bom* vs *ótimo* são ortogonais em one-hot) e podem **falhar com negação** e **ordem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def cosine(u, v, eps=1e-9):\n",
    "    u = np.array(u, dtype=float); v = np.array(v, dtype=float)\n",
    "    return float(u.dot(v) / (np.linalg.norm(u)*np.linalg.norm(v) + eps))\n",
    "\n",
    "# Vocabulário pequeno para ilustração\n",
    "vocab = [\"o\",\"filme\",\"é\",\"não\",\"bom\",\"ótimo\",\"atendeu\",\"paciente\",\"hospital\"]\n",
    "tok2idx = {t:i for i,t in enumerate(vocab)}\n",
    "\n",
    "def bow_vector(tokens):\n",
    "    x = np.zeros(len(vocab), dtype=float)\n",
    "    for t in tokens:\n",
    "        if t in tok2idx: x[tok2idx[t]] += 1\n",
    "    return x\n",
    "\n",
    "s1 = \"o filme é bom\".split()\n",
    "s2 = \"o filme é ótimo\".split()\n",
    "s3 = \"o filme não é bom\".split()\n",
    "\n",
    "v1, v2, v3 = map(bow_vector, [s1,s2,s3])\n",
    "\n",
    "print(\"cos( 'o filme é bom', 'o filme é ótimo'):\", round(cosine(v1,v2),3))\n",
    "print(\"cos( 'o filme é bom', 'o filme não é bom'):\", round(cosine(v1,v3),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d2167",
   "metadata": {},
   "source": [
    "### Ordem importa, e BoW não vê\n",
    "Com BoW, **“o paciente atendeu o hospital”** e **“o hospital atendeu o paciente”** são idênticas.  \n",
    "Bigramas ajudam um pouco, mas o **número de features explode**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bee9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n=2):\n",
    "    return [\"_\".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def vec_from_counts(counts, vocab):\n",
    "    x = np.zeros(len(vocab), dtype=float)\n",
    "    for i,t in enumerate(vocab):\n",
    "        x[i] = counts.get(t, 0.0)\n",
    "    return x\n",
    "\n",
    "a = \"o hospital atendeu o paciente\".split()\n",
    "b = \"o paciente atendeu o hospital\".split()\n",
    "\n",
    "# BoW (unigramas)\n",
    "u_vocab = vocab  # já definido\n",
    "ua, ub = bow_vector(a), bow_vector(b)\n",
    "\n",
    "# Bigramas\n",
    "bigrams_a = Counter(ngrams(a,2))\n",
    "bigrams_b = Counter(ngrams(b,2))\n",
    "b_vocab = sorted(list(set(list(bigrams_a)+list(bigrams_b))))\n",
    "xba = vec_from_counts(bigrams_a, b_vocab)\n",
    "xbb = vec_from_counts(bigrams_b, b_vocab)\n",
    "\n",
    "print(\"BoW (ignora ordem) — cos:\", round(cosine(ua,ub),3), \"(idênticas)\")\n",
    "print(\"Bigramas (vê ordem)  — cos:\", round(cosine(xba,xbb),3), \"(diferentes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ba111",
   "metadata": {},
   "source": [
    "### Por que n-gramas não escalam?\n",
    "- Com vocabulário \\(V\\), o número potencial de **bigramas** é \\(O(V^2)\\); de **trigramas**, \\(O(V^3)\\).  \n",
    "- Mesmo usando estruturas **esparsas**, o custo de memória/treino cresce rápido e muitos n-gramas raramente aparecem.\n",
    "\n",
    "**Moral da história:** precisamos de **vetores densos** que **generalizem** além do que foi visto *literalmente*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_ngrams(V, n):  # estimativa grosseira O(V^n)\n",
    "    return V**n\n",
    "\n",
    "for V in [5_000, 50_000]:\n",
    "    print(f\"\\nVocabulário V={V:,}\")\n",
    "    for n in [1,2,3]:\n",
    "        print(f\"  n={n}: ~{possible_ngrams(V,n):,} features potenciais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfdbb2",
   "metadata": {},
   "source": [
    "## O que são **embeddings**?\n",
    "- Uma **função aprendida** \\( f: \\mathcal{X} \\to \\mathbb{R}^d \\) que mapeia palavras, sub-palavras ou sentenças para **vetores densos** de baixa dimensão (\\(d \\ll |\\mathcal{X}|\\)).  \n",
    "- **Princípio distribucional:** palavras com **contextos semelhantes** tendem a ter **vetores próximos** (alto cosseno).  \n",
    "- **Geometria útil:** *direções* e *distâncias* capturam relações (semelhança, analogias).\n",
    "\n",
    "Imagine num **mapa semântico**: *praia* fica perto de *mar*, longe de *hospital*; *médico* se aproxima de *saúde*, etc.\n",
    "\n",
    "### Vantagens sobre BoW / n-gramas\n",
    "- **Compartilham informação** entre palavras parecidas (ex.: *bom* e *ótimo*).  \n",
    "- **Generalizam** para contextos não vistos literalmente.  \n",
    "- Dimensão **controlada** (50–768+), em vez de crescer com \\(V, V^2, V^3\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5156075",
   "metadata": {},
   "source": [
    "## Como aprendemos embeddings? (alto nível)\n",
    "- **Word2Vec (Skip-gram/CBOW):** aprende vetores prevendo **contexto** da palavra (ou vice-versa).  \n",
    "- **GloVe:** fatoração explícita de coocorrências globais.  \n",
    "- **fastText:** soma vetores de **sub-palavras** (n-gramas de caracteres) → lida melhor com OOV e morfologia.  \n",
    "- **Embeddings contextuais (ELMo/BERT):** um vetor **por ocorrência**, dependente do **contexto** na sentença."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da8c9f",
   "metadata": {},
   "source": [
    "## Experimento inicial\n",
    "Vamos “sentir” na prática as limitações que motivam embeddings.\n",
    "1. **One-hot**: qualquer par de palavras diferentes tem cosseno \\(0\\).  \n",
    "2. **BoW**: frases com **negação** ou **ordem invertida** podem parecer erroneamente parecidas.  \n",
    "3. **Bigramas**: corrigem parcialmente, mas **não escalam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) One-hot não codifica semelhança: 'bom' ~ 'ótimo'?\n",
    "def one_hot(word, vocab):\n",
    "    x = np.zeros(len(vocab), dtype=float)\n",
    "    if word in tok2idx: x[tok2idx[word]] = 1.0\n",
    "    return x\n",
    "\n",
    "oh_bom = one_hot(\"bom\", vocab)\n",
    "oh_otimo = one_hot(\"ótimo\", vocab)\n",
    "\n",
    "print(\"cos(one-hot('bom'), one-hot('ótimo')) =\", cosine(oh_bom, oh_otimo), \"(zero)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) BoW erra na negação; (3) Bigramas ajudam um pouco\n",
    "neg = \"o filme não é bom\".split()\n",
    "pos = \"o filme é bom\".split()\n",
    "\n",
    "bow_neg, bow_pos = bow_vector(neg), bow_vector(pos)\n",
    "bi_neg = Counter(ngrams(neg,2)); bi_pos = Counter(ngrams(pos,2))\n",
    "b_vocab = sorted(list(set(list(bi_neg)+list(bi_pos))))\n",
    "xneg, xpos = vec_from_counts(bi_neg, b_vocab), vec_from_counts(bi_pos, b_vocab)\n",
    "\n",
    "print(\"BoW  — cos(neg, pos) =\", round(cosine(bow_neg, bow_pos),3))\n",
    "print(\"Bi-gramas — cos(neg, pos) =\", round(cosine(xneg, xpos),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951a85c",
   "metadata": {},
   "source": [
    "## Breve história (do macro ao micro)\n",
    "- **Anos 1990 — LSA/LSI:** fatoração (SVD) de matrizes termo-documento gera vetores densos.  \n",
    "- **2003 — Neural Probabilistic LM (Bengio et al.):** primeiras redes neurais aprendendo embeddings junto com o LM.  \n",
    "- **2013 — Word2Vec:** *skip-gram* / *CBOW* com *negative sampling* (rápido, popularizou embeddings).  \n",
    "- **2014 — GloVe:** vetores a partir de coocorrências globais.  \n",
    "- **2016/17 — fastText:** incorpora **sub-palavras** (morfologia e OOV).  \n",
    "- **2018 — ELMo/BERT:** **embeddings contextuais** (um vetor por ocorrência).  \n",
    "- **2019+ — Sentence-BERT e afins:** bons **embeddings de sentenças** para busca e recuperação.  \n",
    "- **2020s — LLMs (GPT, etc.):** camadas internas geram representações ricas; APIs fornecem **embeddings universais**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf9751",
   "metadata": {},
   "source": [
    "## Intuição\n",
    "- **One-hot**: cada palavra → vetor esparso com 1 numa posição. Sem noção de semelhança.\n",
    "- **Embeddings**: mapeiam cada palavra para um vetor **denso** de baixa dimensão (ex.: 50–300), aprendidos a partir de contexto.\n",
    "- Ideia central (*Distribucional*): “Vocês conhecerão uma palavra pelo **contexto** que ela mantém”.\n",
    "  \n",
    "### Do ponto de vista computacional\n",
    "- Vocabulário com \\(V\\) palavras. Dimensão do embedding \\(d \\ll V\\).  \n",
    "- Tabela \\(E \\in \\mathbb{R}^{V \\times d}\\). A palavra \\(w_i\\) tem vetor \\( \\mathbf{e}_i = E[i] \\).\n",
    "- Similaridade por **cosseno**:\n",
    "\\[\n",
    "\\text{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{u}\\|\\;\\|\\mathbf{v}\\|}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a6cb6",
   "metadata": {},
   "source": [
    "## De one-hot a vetores densos\n",
    "Vamos construir vetores one-hot e comparar com embeddings aleatórios (apenas para enxergar a diferença de forma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulário de exemplo \n",
    "vocab = [\"brasil\", \"rio\", \"sao_paulo\", \"praia\", \"futebol\", \"selva\", \"amazonia\", \"cafe\", \"universidade\", \"hospital\"]\n",
    "idx = {w:i for i,w in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "d = 5  # dimensão dos embeddings \"densos\" para visualização\n",
    "\n",
    "# One-hot\n",
    "I = np.eye(V)\n",
    "\n",
    "# Embeddings densos aleatórios (só para comparação visual)\n",
    "np.random.seed(42)\n",
    "E_rand = np.random.normal(scale=0.5, size=(V,d))\n",
    "\n",
    "print(\"One-hot shape:\", I.shape)\n",
    "print(\"Embeddings densos shape:\", E_rand.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização simples (PCA) dos embeddings aleatórios vs one-hot\n",
    "def plot_points(X, labels, title):\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    X2 = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X2[:,0], X2[:,1], c=\"tab:blue\")\n",
    "    for i, txt in enumerate(labels):\n",
    "        plt.annotate(txt, (X2[i,0]+0.01, X2[i,1]+0.01), fontsize=9)\n",
    "    plt.title(title)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_points(I, vocab, \"One-hot (projeção PCA)\")\n",
    "plot_points(E_rand, vocab, \"Vetores densos aleatórios (projeção PCA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4a60f",
   "metadata": {},
   "source": [
    "**Observação:** One-hot não captura relação entre palavras. Já vetores densos *podem* capturar (se aprendidos de dados). Vamos treinar rapidamente um **Word2Vec** em um corpus pequeno de frases em PT-BR para demonstrar a ideia (não esperem qualidade de produção).  \n",
    "*Siglas usadas*: OOV (*Out-Of-Vocabulary*), PCA (Análise de Componentes Principais), t-SNE (*t-distributed Stochastic Neighbor Embedding*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Treino rápido de Word2Vec com Gensim (toy)\n",
    "corpus = [\n",
    "    \"o brasil gosta de futebol\",\n",
    "    \"o rio tem praia bonita\",\n",
    "    \"sao paulo tem universidade e hospital\",\n",
    "    \"a amazonia tem selva e rios\",\n",
    "    \"o brasil produz cafe\",\n",
    "    \"o rio de janeiro tem futebol e praia\",\n",
    "    \"sao paulo tem hospital famoso\",\n",
    "    \"a universidade pesquisa inteligencia artificial\",\n",
    "    \"cafe do brasil é famoso\",\n",
    "    \"a selva da amazonia é densa\",\n",
    "    \"o hospital do rio é universitario\",\n",
    "]\n",
    "\n",
    "# Tokenização bem simples (minúsculas e split)\n",
    "sentences = [s.lower().split() for s in corpus]\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=50,   # dimensão do embedding\n",
    "    window=3,         # tamanho do contexto\n",
    "    min_count=1,      # mantém todas as palavras do nosso mini-corpus\n",
    "    sg=1,             # 1 = skip-gram, 0 = CBOW\n",
    "    epochs=200,       # mais épocas pra compensar corpus pequeno\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Palavras no vocabulário:\", list(w2v.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a648e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 4) Funções auxiliares: similaridade por cosseno e vizinhos mais próximos\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(u, v, eps=1e-9):\n",
    "    u = np.array(u); v = np.array(v)\n",
    "    return float(u.dot(v) / (norm(u)*norm(v) + eps))\n",
    "\n",
    "def nearest_neighbors(model, query, topk=5):\n",
    "    if query not in model.wv:\n",
    "        return []\n",
    "    qv = model.wv[query]\n",
    "    words = []\n",
    "    sims = []\n",
    "    for w in model.wv.index_to_key:\n",
    "        if w == query: \n",
    "            continue\n",
    "        s = cosine(qv, model.wv[w])\n",
    "        words.append(w); sims.append(s)\n",
    "    order = np.argsort(sims)[::-1][:topk]\n",
    "    return [(words[i], sims[i]) for i in order]\n",
    "\n",
    "for q in [\"brasil\", \"rio\", \"universidade\", \"praia\", \"hospital\", \"amazonia\"]:\n",
    "    print(f\"\\nMais próximos de '{q}':\")\n",
    "    for w,s in nearest_neighbors(w2v, q, topk=5):\n",
    "        print(f\"  {w:15s}  cos={s: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfead148",
   "metadata": {},
   "source": [
    "## Analogias \n",
    "Analogias famosas do tipo *rei − homem + mulher ≈ rainha* tendem a emergir em embeddings maiores e com muito dado.  \n",
    "Com nosso corpus pequeno, veremos apenas um **exemplo didático**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0374adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model, a, b, c, topk=5):\n",
    "    # a : b :: c : ?\n",
    "    if not all(w in model.wv for w in [a,b,c]):\n",
    "        return []\n",
    "    vec = model.wv[b] - model.wv[a] + model.wv[c]\n",
    "    words = []\n",
    "    sims = []\n",
    "    for w in model.wv.index_to_key:\n",
    "        if w in [a,b,c]: \n",
    "            continue\n",
    "        s = cosine(vec, model.wv[w])\n",
    "        words.append(w); sims.append(s)\n",
    "    order = np.argsort(sims)[::-1][:topk]\n",
    "    return [(words[i], sims[i]) for i in order]\n",
    "\n",
    "print(\"Exemplo de analogia (qual palavra se aproxima de 'rio' - 'praia' + 'sao'):\")\n",
    "print(analogy(w2v, \"praia\", \"rio\", \"sao\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26065f1",
   "metadata": {},
   "source": [
    "> **Nota:** Em corpus real (bilhões de tokens) as relações ficam bem mais nítidas. Aqui o objetivo é ver **a mecânica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd23dd5",
   "metadata": {},
   "source": [
    "## Embedding como camada de rede (PyTorch)\n",
    "A camada `nn.Embedding(V, d)` implementa uma **tabela** onde cada índice retorna um vetor d-dimensional.  \n",
    "Vamos treinar um mini-modelo para prever uma palavra do contexto (*CBOW* simplificado) apenas para “sentir” o gradiente atualizando a tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados (pares contexto->alvo) a partir do nosso corpus\n",
    "word2idx = {w:i for i,w in enumerate(w2v.wv.index_to_key)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "V = len(word2idx)\n",
    "d = 32\n",
    "window = 2\n",
    "\n",
    "token_ids = [[word2idx[w] for w in s if w in word2idx] for s in sentences]\n",
    "\n",
    "X, Y = [], []\n",
    "for sent in token_ids:\n",
    "    for i in range(len(sent)):\n",
    "        left = max(0, i-window)\n",
    "        right = min(len(sent), i+window+1)\n",
    "        ctx = [sent[j] for j in range(left, right) if j != i]\n",
    "        if not ctx:\n",
    "            continue\n",
    "        X.append(ctx)\n",
    "        Y.append(sent[i])\n",
    "\n",
    "# Pad de comprimento variável num jeito simples: média dos embeddings de contexto\n",
    "# (para simplificar, dentro do forward vamos computar a média de embeddings)\n",
    "X_lens = np.array([len(x) for x in X])\n",
    "\n",
    "# Modelo CBOW simplificado\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, d):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d)\n",
    "        self.lin = nn.Linear(d, vocab_size)\n",
    "    def forward(self, ctx_batch):\n",
    "        # ctx_batch: list of lists (índices)\n",
    "        max_len = max(len(c) for c in ctx_batch)\n",
    "        padded = []\n",
    "        for c in ctx_batch:\n",
    "            if len(c) < max_len:\n",
    "                c = c + [0]*(max_len - len(c))\n",
    "            padded.append(c)\n",
    "        x = torch.tensor(padded, dtype=torch.long)\n",
    "        E = self.emb(x)                   # [B, L, d]\n",
    "        m = E.mean(dim=1)                 # [B, d]\n",
    "        logits = self.lin(m)              # [B, V]\n",
    "        return logits\n",
    "\n",
    "model = CBOW(V, d)\n",
    "opt = optim.Adam(model.parameters(), lr=0.05)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Treino rápido\n",
    "def batchify(X, Y, bs=16):\n",
    "    for i in range(0, len(X), bs):\n",
    "        yield X[i:i+bs], Y[i:i+bs]\n",
    "\n",
    "for epoch in range(150):\n",
    "    total = 0.0\n",
    "    for xb, yb in batchify(X, Y, bs=8):\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        y = torch.tensor(yb, dtype=torch.long)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += float(loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"época {epoch+1:3d} | loss {total/len(X):.4f}\")\n",
    "\n",
    "# Extra: extrair embeddings aprendidos\n",
    "E_cbow = model.emb.weight.detach().numpy()\n",
    "print(\"Tabela de embeddings aprendida:\", E_cbow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecionar vizinhos no embedding do CBOW treinado\n",
    "def nearest_from_matrix(E, word, topk=5):\n",
    "    if word not in word2idx: \n",
    "        return []\n",
    "    i = word2idx[word]\n",
    "    q = E[i]\n",
    "    sims = []\n",
    "    words = []\n",
    "    for j in range(V):\n",
    "        if j == i: \n",
    "            continue\n",
    "        s = cosine(q, E[j])\n",
    "        sims.append(s); words.append(idx2word[j])\n",
    "    order = np.argsort(sims)[::-1][:topk]\n",
    "    return [(words[k], sims[k]) for k in order]\n",
    "\n",
    "for q in [\"brasil\", \"rio\", \"universidade\", \"praia\"]:\n",
    "    print(f\"\\nVizinhos (CBOW) de '{q}':\")\n",
    "    for w,s in nearest_from_matrix(E_cbow, q, 5):\n",
    "        print(f\"  {w:15s}  cos={s: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42c74f",
   "metadata": {},
   "source": [
    "## Sub-palavras e OOV\n",
    "- *OOV* (*Out-Of-Vocabulary*): palavras não vistas no treino.  \n",
    "- **fastText** usa **n-gramas de caracteres**: permite vetorizar “brasilão” mesmo sem tê-la visto, somando vetores de sub-palavras.  \n",
    "Abaixo, um treino rápido **opcional** (pode demorar ~alguns segundos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2276380",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from gensim.models.fasttext import FastText\n",
    "    ft = FastText(sentences, vector_size=50, window=3, min_count=1, sg=1, epochs=100)\n",
    "    for q in [\"brasil\", \"brasilao\", \"universidade\", \"universitario\"]:\n",
    "        print(f\"\\nVizinhos (fastText) de '{q}':\")\n",
    "        if q in ft.wv:\n",
    "            nnft = ft.wv.most_similar(q, topn=5)\n",
    "            for w,s in nnft:\n",
    "                print(f\"  {w:15s}  cos={s: .3f}\")\n",
    "        else:\n",
    "            print(\"  (palavra não está no vocabulário, mas fastText ainda gera vetor por sub-palavras)\")\n",
    "            vec = ft.wv.get_vector(q)  # ainda retorna vetor!\n",
    "            # vamos medir distância desse vetor para algumas palavras conhecidas\n",
    "            sims = [(w, cosine(vec, ft.wv[w])) for w in vocab if w in ft.wv]\n",
    "            sims = sorted(sims, key=lambda x: x[1], reverse=True)[:5]\n",
    "            for w,s in sims:\n",
    "                print(f\"  {w:15s}  cos={s: .3f}\")\n",
    "except Exception as e:\n",
    "    print(\"fastText não disponível neste ambiente:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742b596",
   "metadata": {},
   "source": [
    "## Embeddings de sentenças\n",
    "Duas formas simples:\n",
    "1. **Média** dos embeddings de palavras (rápido, baseline forte).  \n",
    "2. **Modelo pré-treinado** (ex.: *Sentence-Transformers* multilíngue) — **opcional** (baixa o modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Média de palavras (usando o Word2Vec treinado)\n",
    "def sent_embed_mean(sent, model):\n",
    "    toks = [w for w in sent.lower().split() if w in model.wv]\n",
    "    if not toks:\n",
    "        return np.zeros(model.vector_size)\n",
    "    mat = np.stack([model.wv[w] for w in toks])\n",
    "    return mat.mean(axis=0)\n",
    "\n",
    "docs = [\n",
    "    \"praia lotada no rio de janeiro\",\n",
    "    \"hospital universitario em sao paulo\",\n",
    "    \"pesquisa em inteligencia artificial na universidade\",\n",
    "    \"cafe do brasil é exportado\",\n",
    "    \"passeio na amazonia e na selva\"\n",
    "]\n",
    "D = np.stack([sent_embed_mean(s, w2v) for s in docs])\n",
    "\n",
    "def search(query, k=3):\n",
    "    qv = sent_embed_mean(query, w2v)\n",
    "    scores = [cosine(qv, D[i]) for i in range(len(docs))]\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [(docs[i], scores[i]) for i in order]\n",
    "\n",
    "for q in [\"praia no rio\", \"universidade e hospital\", \"selva amazonia\", \"cafe brasileiro\"]:\n",
    "    print(f\"\\nConsulta: {q}\")\n",
    "    for d,s in search(q, 3):\n",
    "        print(f\"  → {d:70s}  cos={s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ]embeddings de sentenças com modelo pré-treinado (multilíngue)\n",
    "# AVISO: baixa ~100MB na primeira vez.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "try:\n",
    "    model_st = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    E_docs = model_st.encode(docs, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    def search_st(query, k=3):\n",
    "        q = model_st.encode([query], convert_to_tensor=True, normalize_embeddings=True)\n",
    "        sims = util.cos_sim(q, E_docs).cpu().numpy().ravel()\n",
    "        order = np.argsort(sims)[::-1][:k]\n",
    "        return [(docs[i], float(sims[i])) for i in order]\n",
    "    for q in [\"praia no rio\", \"universidade e hospital\", \"selva amazonia\", \"cafe brasileiro\"]:\n",
    "        print(f\"\\n(Pré-treinado) Consulta: {q}\")\n",
    "        for d,s in search_st(q, 3):\n",
    "            print(f\"  → {d:70s}  cos={s:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"Sentence-Transformers indisponível/opcional falhou:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cbd0f",
   "metadata": {},
   "source": [
    "## Visualização (PCA e t-SNE)\n",
    "Visualizações ajudam a intuir agrupamentos semânticos. Vamos projetar algumas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2717793",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_plot = [\"brasil\",\"rio\",\"sao\",\"paulo\",\"sao_paulo\",\"praia\",\"futebol\",\"universidade\",\"hospital\",\"amazonia\",\"selva\",\"cafe\"]\n",
    "# normalizar nomes para o vocabulário do modelo w2v\n",
    "words_to_plot = [w for w in words_to_plot if w in w2v.wv]\n",
    "\n",
    "X = np.stack([w2v.wv[w] for w in words_to_plot])\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "X2 = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X2[:,0], X2[:,1], c=\"tab:green\")\n",
    "for i,w in enumerate(words_to_plot):\n",
    "    plt.annotate(w, (X2[i,0]+0.01, X2[i,1]+0.01), fontsize=9)\n",
    "plt.title(\"Projeção PCA dos embeddings (Word2Vec)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# t-SNE (pode demorar um pouco com muitos pontos; aqui é pequeno)\n",
    "tsne = TSNE(n_components=2, perplexity=5, learning_rate='auto', init='pca', random_state=0)\n",
    "X3 = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X3[:,0], X3[:,1], c=\"tab:purple\")\n",
    "for i,w in enumerate(words_to_plot):\n",
    "    plt.annotate(w, (X3[i,0]+0.8, X3[i,1]+0.8), fontsize=9)\n",
    "plt.title(\"Projeção t-SNE dos embeddings (Word2Vec)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7499b",
   "metadata": {},
   "source": [
    "## Miniaplicação: busca semântica interativa (baseline)\n",
    "Dado um conjunto de frases/documentos, calculem embeddings (média de palavras) e retornem os mais parecidos para uma consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocês podem alterar a lista 'docs2' e testar consultas.\n",
    "docs2 = [\n",
    "    \"ingressos para jogo de futebol no maracana\",\n",
    "    \"trilha na floresta da amazonia\",\n",
    "    \"praias de copacabana e ipanema\",\n",
    "    \"hospital publico em sao paulo\",\n",
    "    \"pesquisa de ia na universidade federal\",\n",
    "    \"cafeteria tradicional no centro\"\n",
    "]\n",
    "E2 = np.stack([sent_embed_mean(s, w2v) for s in docs2])\n",
    "\n",
    "def search_docs(query, docs_list, E, k=3):\n",
    "    qv = sent_embed_mean(query, w2v)\n",
    "    scores = [cosine(qv, E[i]) for i in range(len(docs_list))]\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [(docs_list[i], scores[i]) for i in order]\n",
    "\n",
    "# Exemplo\n",
    "for q in [\"futebol no rio\", \"praia\", \"selva amazonia\", \"universidade ia\", \"cafe\"]:\n",
    "    print(f\"\\nConsulta: {q}\")\n",
    "    for d,s in search_docs(q, docs2, E2, 3):\n",
    "        print(f\"  → {d:60s}  cos={s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df8d37",
   "metadata": {},
   "source": [
    "## Notas críticas (viés e limitações)\n",
    "- **Viés**: embeddings refletem padrões do corpus. Se os dados têm estereótipos, o vetor também terá.  \n",
    "- **Contexto**: embeddings “estáticos” (Word2Vec/GloVe) **não** diferenciam sentidos: *banco* (sentar) vs *banco* (financeiro).  \n",
    "- **Solução moderna**: embeddings **contextuais** (BERT/Transformers) geram vetores **por ocorrência**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445739d7",
   "metadata": {},
   "source": [
    "## Resumo rápido\n",
    "- Embeddings: mapeiam palavras/sentenças para vetores **densos** que capturam **similaridade semântica**.  \n",
    "- É possível **treinar** (Word2Vec/CBOW/Skip-gram) ou **usar pré-treinados** (fastText, Sentence-Transformers).  \n",
    "- Ferramentas: similaridade por cosseno, vizinhos, analogias, PCA/t-SNE.  \n",
    "- Cuidados: viés, OOV, sentidos múltiplos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c8e88",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Modifiquem o corpus adicionando 5–10 frases novas com tema de sua escolha (ex.: esportes, saúde, educação).\n",
    "# - Re-treiem o Word2Vec (mudem window/vector_size/epochs) e observem:\n",
    "#   a) Vizinhos mais próximos de 3 palavras.\n",
    "#   b) Mudanças na visualização PCA.\n",
    "# Dica: copiem e ajustem as células do treino e da visualização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementem uma função 'most_similar_to_set(positivos, negativos, topk=5)' no embedding do w2v,\n",
    "# que calcula: v = sum(positivos) - sum(negativos), e retorna vizinhos de v.\n",
    "# Testem casos como: positivos=['rio','praia'], negativos=['selva']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338cc24",
   "metadata": {},
   "source": [
    "## Leituras e recursos adicionais\n",
    "**Artigos / docs (abertos):**\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation of Word Representations in Vector Space* (Word2Vec).  \n",
    "- Pennington, J., Socher, R., & Manning, C. D. (2014). *GloVe: Global Vectors for Word Representation*.  \n",
    "- Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). *Enriching Word Vectors with Subword Information* (fastText).  \n",
    "- Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*.  \n",
    "- Goldberg, Y. (2016). *A Primer on Neural Network Models for Natural Language Processing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b17e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
