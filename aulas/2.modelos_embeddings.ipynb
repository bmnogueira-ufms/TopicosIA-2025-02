{"cells":[{"cell_type":"markdown","id":"95701fe4","metadata":{"id":"95701fe4"},"source":["# Tópicos em IA — Modelos de Linguagem\n","## Aula: Word Embeddings Clássicos — **Word2Vec, GloVe e FastText**\n","\n","**Pré-requisitos:** Python básico, cálculo, estatística introdutória.\n","\n","### Objetivos\n","- Entender, em detalhe, os modelos **Word2Vec (CBOW e Skip-gram)**, **GloVe** e **FastText**.\n","- Derivar as **funções objetivo** e os **gradientes** centrais.\n","- Implementar mini-versões dos algoritmos para compreender a mecânica de treinamento.\n","- Comparar qualitativamente os modelos (semântica, morfologia, OOV).\n","- Exercitar avaliação intrínseca (similaridade/analogia) e discutir vieses.\n","\n","> Referências base: notas do **CS224n** (Manning) sobre Word Vectors e GloVe; ver bibliografia ao final."]},{"cell_type":"markdown","id":"6c2f9c32","metadata":{"id":"6c2f9c32"},"source":["## Sumário\n","1) Motivação: do *one-hot* a vetores densos  \n","2) **Word2Vec**: softmax, *negative sampling*, CBOW vs. Skip-gram; **derivações**  \n","3) **GloVe**: matriz de coocorrência, objetivo ponderado, **derivações**  \n","4) **FastText**: n-gramas de caracteres, OOV, morfologia  \n","5) Avaliação (intrínseca/extrínseca), analogias, TSNE  \n","6) Atividades guiadas, ética e vieses, ganchos para a próxima aula"]},{"cell_type":"code","execution_count":null,"id":"10411860","metadata":{"id":"10411860"},"outputs":[],"source":["# Instalação das dependências localmente\n","# Se estiver rodando localmente, descomente a linha abaixo para instalar as dependências\n","# ! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":5,"id":"841ef678","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"841ef678","executionInfo":{"status":"ok","timestamp":1756428219752,"user_tz":240,"elapsed":4290,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"6695d29e-0845-4e1b-f4bc-3423bcda08c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# Se rodando no Google Colab, descomente a linha abaixo para montar o Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":6,"id":"98794ba1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"98794ba1","executionInfo":{"status":"ok","timestamp":1756428445169,"user_tz":240,"elapsed":223905,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"8c068fd5-d01a-4959-8086-bbc0f138290a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pandas==2.2.3 (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1))\n","  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 2)) (3.9.1)\n","Collecting matplotlib==3.10.1 (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4))\n","  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 5)) (1.6.1)\n","Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 6)) (0.13.2)\n","Collecting torch==2.2.2 (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting gensim==4.3.3 (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 8))\n","  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: sentence-transformers==5.1.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (5.1.0)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1)) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 2)) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 2)) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 2)) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 2)) (4.67.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 4)) (3.2.3)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 5)) (1.16.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 5)) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (4.15.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting numpy>=1.26.0 (from pandas==2.2.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1))\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn==1.6.1->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 5))\n","  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim==4.3.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 8)) (7.3.0.post1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (4.55.4)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (0.34.4)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (12.6.85)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (1.1.8)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 1)) (1.17.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim==4.3.3->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 8)) (1.17.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (0.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 7)) (1.3.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==5.1.0->-r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt (line 9)) (2025.8.3)\n","Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, scipy, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, matplotlib, gensim\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.1\n","    Uninstalling scipy-1.16.1:\n","      Successfully uninstalled scipy-1.16.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.8.0+cu126\n","    Uninstalling torch-2.8.0+cu126:\n","      Successfully uninstalled torch-2.8.0+cu126\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 matplotlib-3.10.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pandas-2.2.3 scipy-1.13.1 torch-2.2.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]},"id":"77ef15a3b05342d187e9a1cb7a7174bd"}},"metadata":{}}],"source":["# Instalação das dependências no Google Colab\n","# Mude CAMINHO_PARA_REPO para o caminho correto do seu repositório no seu Google Drive\n","! pip install -r /content/drive/MyDrive/UFMS/Aulas/2025-2/TOPIA/repo/TopicosIA-2025-02/requirements.txt"]},{"cell_type":"markdown","id":"3cc739c6","metadata":{"id":"3cc739c6"},"source":["## 1) Motivação: do one-hot para embeddings\n","\n","**Problema do one-hot:** vetores são ortogonais; não há noção de similaridade.\n","\n","Sejam duas palavras `hotel` e `motel`:\n","$$\n","\\mathbf{e}_{\\text{hotel}},\\ \\mathbf{e}_{\\text{motel}} \\in \\mathbb{R}^{|V|},\\quad\n","\\mathbf{e}_{\\text{hotel}}^\\top \\mathbf{e}_{\\text{motel}} = 0\n","$$\n","\n","**Ideia:** aprender **vetores densos** $\\mathbf{w}\\in\\mathbb{R}^d$ (com $d\\ll |V|$) onde a proximidade (p.ex. cosseno) reflete semântica:\n","$$\n","\\cos(\\mathbf{w}_a,\\mathbf{w}_b)=\\frac{\\mathbf{w}_a^\\top \\mathbf{w}_b}{\\|\\mathbf{w}_a\\|\\,\\|\\mathbf{w}_b\\|}\n","$$\n","\n","**Hipótese distribucional:** “Conhecerás a palavra pela companhia que ela mantém.” (Firth, 1957)."]},{"cell_type":"code","execution_count":1,"id":"49e4cbcc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49e4cbcc","executionInfo":{"status":"ok","timestamp":1756428905553,"user_tz":240,"elapsed":7,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"908bd83c-b95e-48ca-e49c-41199bca4ec7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosseno(one-hot hotel, motel) = 0.0\n","Cosseno(embeddings densos) ≈ 0.9939728426989571\n"]}],"source":["import numpy as np\n","\n","V = 6\n","idx = {\"hotel\": 1, \"motel\": 4}\n","e_hotel = np.eye(V)[idx[\"hotel\"]]\n","e_motel = np.eye(V)[idx[\"motel\"]]\n","\n","cos = e_hotel @ e_motel / (np.linalg.norm(e_hotel)*np.linalg.norm(e_motel) + 1e-12)\n","print(\"Cosseno(one-hot hotel, motel) =\", cos)  # 0.0\n","\n","rng = np.random.default_rng(0)\n","w_hotel = rng.normal(size=50)\n","w_motel = w_hotel + 0.1*rng.normal(size=50)\n","cos_dense = (w_hotel @ w_motel)/(np.linalg.norm(w_hotel)*np.linalg.norm(w_motel))\n","print(\"Cosseno(embeddings densos) ≈\", float(cos_dense))"]},{"cell_type":"markdown","id":"14bce4f2","metadata":{"id":"14bce4f2"},"source":["## 2) Word2Vec (Mikolov et al., 2013)\n","\n","Dadas sequências $(w_1,\\dots,w_T)$, com janela de tamanho $m$:\n","\n","- **Skip-gram:** prediz contexto $w_{t+j}$ a partir da palavra central $w_t$.\n","- **CBOW:** prediz a central $w_t$ a partir do **bag** dos contextos $\\{w_{t\\pm j}\\}$.\n","\n","<br/>\n","<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/skip-gram-cbow.png\" width=\"55%\">\n","\n","### Probabilidade via softmax (modelo \"naïve softmax\")\n","Para central $c$ e contexto $o$, com **dois** vetores por palavra (centro $\\mathbf{v}_w$, contexto $\\mathbf{u}_w$):\n","$$\n","P(o\\mid c)=\\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{w\\in V}\\exp(\\mathbf{u}_w^\\top \\mathbf{v}_c)}\n","$$\n","\n","\n","**Passo a passo da intuição:**\n","\n","1. **Produto interno ($\\mathbf{u}_o^\\top \\mathbf{v}_c$):**  \n","   Mede a *similaridade* entre os vetores da palavra central $c$ e da candidata $o$.  \n","   - Alto $\\rightarrow$ palavras aparecem juntas com frequência.  \n","   - Baixo $\\rightarrow$ palavras raramente aparecem juntas.\n","\n","2. **Exponenciação ($\\exp(\\cdot)$):**  \n","   - Garante valores positivos.  \n","   - Amplifica diferenças: pequenas vantagens viram grandes diferenças de peso.\n","\n","3. **Normalização (softmax):**  \n","   - Divide pelo somatório de todos os candidatos do vocabulário.  \n","   - Cria uma **distribuição de probabilidade** sobre todas as palavras.  \n","\n","---\n","\n","**Intuição global:**  \n","O modelo responde à pergunta:  \n","> *“Dada a palavra central $c$, qual a probabilidade de $o$ estar em seu contexto?”*  \n","\n","Se a compatibilidade (produto interno) entre $c$ e $o$ é alta, a probabilidade cresce.  \n","Se é baixa, a probabilidade cai.  \n","\n","---\n","\n","**Objetivo (Skip-gram):**\n","$$\n","J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^T \\sum_{-m\\le j\\le m,\\ j\\ne 0}\\ \\log P(w_{t+j}\\mid w_t)\n","$$\n","\n","O softmax requer normalização sobre $V$ → caro para vocabulários grandes.  \n","Duas soluções: **Negative Sampling** e **Hierarchical Softmax**."]},{"cell_type":"markdown","id":"398b6bc5","metadata":{"id":"398b6bc5"},"source":["### Derivações (naïve softmax, Skip-gram)\n","\n","Para um par $(c,o)$ e predição $\\hat{\\mathbf{y}}=\\text{softmax}(U\\mathbf{v}_c)$, com rótulo one-hot $\\mathbf{y}$ s.t. $y_o=1$:\n","\n","- $\\mathbf{v}_c \\in \\mathbb{R}^d$ → vetor da palavra **central** $c$ (embedding de dimensão $d$).  \n","- $U \\in \\mathbb{R}^{|V|\\times d}$ → matriz que contém os **vetores de contexto** (um para cada palavra do vocabulário).  \n","  - Cada linha $\\mathbf{u}_w^\\top$ de $U$ é o vetor associado à palavra $w$ quando ela atua como **contexto**.\n","\n","  - **Produto interno com todos os contextos**  \n","    - Para cada palavra $w$ do vocabulário, calcula $\\mathbf{u}_w^\\top \\mathbf{v}_c$.  \n","    - Esse valor é um **escore de compatibilidade** entre $c$ (central) e $w$ (possível contexto).  \n","\n","  - **Resultado vetorial**  \n","    - O produto $U \\mathbf{v}_c$ produz um vetor de dimensão $|V|$.  \n","    - Cada posição corresponde ao “quanto o modelo acredita” que aquela palavra pode ser contexto de $c$.  \n","\n","  - **Softmax transforma em probabilidade**  \n","    - Em seguida, aplicamos:\n","     $$\n","     \\hat{\\mathbf{y}} = \\text{softmax}(U \\mathbf{v}_c)\n","     $$\n","    - Assim, cada escore vira uma **probabilidade normalizada** de a palavra ser o contexto verdadeiro.\n","\n","**Perda por par:**\n","$$\n","J_{\\text{pair}} = -\\log P(o\\mid c)\n","$$\n","\n","- $c$ = palavra **central**  \n","- $o$ = palavra **de contexto**  \n","- $P(o \\mid c)$ = probabilidade atribuída pelo modelo de observar $o$ no contexto de $c$  \n","\n","---\n","\n","#### Como interpretar\n","\n","1. **Maximizar probabilidade correta**  \n","   - Se o modelo acha que $o$ é muito provável dado $c$, então $P(o\\mid c) \\approx 1$ → perda próxima de 0.  \n","   - Se o modelo acha que $o$ é improvável, $P(o\\mid c) \\ll 1$ → perda grande.\n","\n","2. **Por que o log?**  \n","   - Evita lidar com produtos de probabilidades muito pequenas.  \n","   - Cada par $(c,o)$ contribui de forma aditiva ao custo total.  \n","\n","3. **Por que o sinal de menos?**  \n","   - Como $\\log P \\leq 0$, o “menos” transforma em valor **positivo**.  \n","   - Isso equivale a **minimizar a perda** em vez de maximizar a probabilidade.\n","\n","---\n","\n","#### Intuição global\n","A função está dizendo:  \n","\n","*“Quero que a probabilidade atribuída ao par correto $(c,o)$ seja a maior possível.”*\n","\n","- Se $P(o\\mid c)$ é alto → perda pequena.  \n","- Se $P(o\\mid c)$ é baixo → perda grande.  \n","\n","---\n","\n","**Gradientes:**\n","$$\n","\\frac{\\partial J_{\\text{pair}}}{\\partial \\mathbf{v}_c} = U^\\top(\\hat{\\mathbf{y}}-\\mathbf{y}),\n","\\qquad\n","\\frac{\\partial J_{\\text{pair}}}{\\partial \\mathbf{u}_w} = (\\hat{y}_w - y_w)\\,\\mathbf{v}_c,\\ \\forall w\\in V\n","$$\n","\n","Intuição: $(\\hat{\\mathbf{y}}-\\mathbf{y})$ é o **erro**; atualizamos $\\mathbf{v}_c$ e as $\\mathbf{u}_w$ proporcionais a esse erro.  \n","Complexidade ainda é $O(|V|)$ por atualização → motivação para **Negative Sampling**."]},{"cell_type":"markdown","id":"3436bdb7","metadata":{"id":"3436bdb7"},"source":["### Negative Sampling (Skip-gram)\n","\n","**Ideia:** treinar classificadores binários que **distingam** um par real $(c,o)$ de $k$ pares negativos $(c,w_i)$ amostrados de uma distribuição de ruído $P_n$.\n","\n","**Objetivo por par $(c,o)$ e amostras $\\{w_i\\}_{i=1}^k$:**\n","$$\n","J_{\\text{NS}} = -\\log \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)\\;-\\; \\sum_{i=1}^k \\log \\sigma\\!\\big(-\\mathbf{u}_{w_i}^\\top \\mathbf{v}_c\\big),\n","$$\n","onde $\\sigma(x)=\\tfrac{1}{1+e^{-x}}$.\n","\n","**Gradientes:**\n","$$\n","\\frac{\\partial J_{\\text{NS}}}{\\partial \\mathbf{v}_c} = \\big(\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)-1\\big)\\mathbf{u}_o\n","\\;+\\; \\sum_{i=1}^k \\sigma(\\mathbf{u}_{w_i}^\\top \\mathbf{v}_c)\\,\\mathbf{u}_{w_i}\n","$$\n","\n","$$\n","\\frac{\\partial J_{\\text{NS}}}{\\partial \\mathbf{u}_o} = \\big(\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)-1\\big)\\mathbf{v}_c,\n","\\qquad\n","\\frac{\\partial J_{\\text{NS}}}{\\partial \\mathbf{u}_{w_i}} = \\sigma(\\mathbf{u}_{w_i}^\\top \\mathbf{v}_c)\\,\\mathbf{v}_c\n","$$\n","\n","**Prática:** amostrar negativos com $P_n(w)\\propto U(w)^{3/4}$."]},{"cell_type":"code","execution_count":2,"id":"fe1da87e","metadata":{"id":"fe1da87e","executionInfo":{"status":"ok","timestamp":1756430357311,"user_tz":240,"elapsed":28,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}}},"outputs":[],"source":["corpus = [\n","    \"o brasil gosta de futebol\",\n","    \"o rio tem praia bonita\",\n","    \"sao paulo tem universidade e hospital\",\n","    \"a amazonia tem selva e rios\",\n","    \"o brasil produz cafe\",\n","    \"o rio de janeiro tem futebol e praia\",\n","    \"sao paulo tem hospital famoso\",\n","    \"a universidade pesquisa inteligencia artificial\",\n","    \"cafe do brasil é famoso\",\n","    \"a selva da amazonia é densa\",\n","    \"o hospital do rio é universitario\",\n","\n","    \"o cruzeiro é o melhor time do brasil\",\n","    \"o time de futebol joga no estadio\",\n","    \"a torcida canta no estadio\",\n","    \"o jogador marcou um gol\",\n","    \"o tecnico escala o time titular\",\n","    \"o campeonato brasileiro tem rodada no domingo\",\n","    \"o arbitro apita o jogo de futebol\",\n","    \"o atacante fez dois gols no classico\",\n","    \"o goleiro defendeu o penalti\",\n","    \"o time treinou no campo de grama\",\n","    \"o maracana fica no rio de janeiro\",\n","    \"o morumbi fica em sao paulo\",\n","    \"o estadio recebe finais importantes\",\n","    \"o time carioca venceu o jogo\",\n","    \"o time paulista empatou fora de casa\",\n","    \"a base revela novos jogadores\",\n","    \"o lateral cruza a bola para a area\",\n","    \"o meio campista organiza o ataque\",\n","    \"o zagueiro corta o cruzamento\",\n","    \"o treinador analisa estatisticas do jogo\",\n","\n","    \"a praia de copacabana tem areia branca\",\n","    \"ipanema tem mar e sol\",\n","    \"o turista caminha no calcadao de copacabana\",\n","    \"o surfista pega onda na praia\",\n","    \"banhistas lotam a praia no verao\",\n","    \"o quiosque vende agua de coco\",\n","    \"o rio tem orla movimentada\",\n","    \"a brisa do mar refresca a tarde\",\n","    \"a praia tem guarda sol e cadeira\",\n","    \"o por do sol em ipanema é bonito\",\n","    \"o passeio de bicicleta na orla é popular\",\n","\n","    \"a universidade tem campus grande\",\n","    \"o campus tem biblioteca e laboratorio\",\n","    \"o professor orienta estudante de mestrado\",\n","    \"a pesquisa em dados utiliza python\",\n","    \"o laboratorio de ia treina redes neurais\",\n","    \"a pos graduacao publica artigos cientificos\",\n","    \"o grupo de pesquisa organiza seminario semanal\",\n","    \"a universidade federal tem curso de computacao\",\n","    \"o aluno estuda no laboratorio de informatica\",\n","    \"a disciplina ensina machine learning\",\n","    \"o dataset tem noticias do brasil\",\n","    \"a avaliacao usa acuracia e f1\",\n","    \"o modelo aprende representacoes distribuídas\",\n","    \"o repositorio guarda codigo e dados\",\n","    \"o professor apresenta resultados no congresso\",\n","    \"o curso tem prova e projeto final\",\n","    \"a biblioteca empresta livros e revistas\",\n","    \"a universidade tem hospital universitario\",\n","    \"a pesquisa em linguagem natural usa word2vec\",\n","    \"o laboratorio de visao computacional anota imagens\",\n","    \"a turma pratica classificacao de textos\",\n","    \"o servidor treina o modelo com gpu\",\n","    \"a extensao conecta universidade e comunidade\",\n","    \"o estagio aproxima alunos do mercado\",\n","\n","    \"o hospital universitario atende pacientes\",\n","    \"o medico realiza cirurgia no centro cirurgico\",\n","    \"a enfermeira aplica vacina na sala de atendimento\",\n","    \"o paciente marcou consulta com o especialista\",\n","    \"a emergencia recebe ambulancia a noite\",\n","    \"o laboratorio realiza exame de sangue\",\n","    \"o hospital tem leitos e uti\",\n","    \"a saude publica contrata medicos\",\n","    \"o medico receita medicamento para o paciente\",\n","    \"o hospital de sao paulo e referencia\",\n","    \"o pronto socorro fica lotado no feriado\",\n","    \"a vacina previne doenca infecciosa\",\n","    \"a farmacia do hospital entrega remedio\",\n","    \"a triagem organiza a fila de atendimento\",\n","    \"o prontuario registra historico do paciente\",\n","\n","    \"a amazonia abriga grande biodiversidade\",\n","    \"o rio amazonas corta a floresta\",\n","    \"o rio negro encontra o rio solimoes\",\n","    \"a floresta tem arvores altas e lianas\",\n","    \"a chuva e intensa na amazonia\",\n","    \"comunidades indigenas vivem na regiao\",\n","    \"o pesquisador estuda fauna e flora\",\n","    \"o boto cor de rosa nada no rio\",\n","    \"o peixe e alimento importante na regiao\",\n","    \"o parque nacional protege a mata\",\n","    \"o desmatamento ameaca a floresta\",\n","    \"o ibama fiscaliza a regiao amazonica\",\n","    \"o clima e umido e quente na floresta\",\n","    \"barcos transportam pessoas pelos rios\",\n","    \"manaus tem porto no rio negro\",\n","    \"belem fica proxima da foz do amazonas\",\n","\n","    \"o cafe de minas gerais tem qualidade\",\n","    \"a fazenda colhe graos maduros de cafe\",\n","    \"a torra define o sabor do cafe\",\n","    \"o barista prepara espresso na cafeteria\",\n","    \"a xicara de cafe acompanha o pao de queijo\",\n","    \"o brasil exporta cafe para a europa\",\n","    \"o cafe especial recebe pontuacao alta\",\n","    \"sao paulo tem muitas cafeterias\",\n","    \"o produtor investe em irrigacao por gotejamento\",\n","    \"a cooperativa vende graos torrados\",\n","    \"o cafe arabica cresce em altitude\",\n","    \"o cafe robusta cresce no espirito santo\",\n","\n","    \"brasilia e a capital do brasil\",\n","    \"belo horizonte tem feira de artesanato\",\n","    \"curitiba tem parque bonito e organizado\",\n","    \"salvador tem carnaval famoso\",\n","    \"fortaleza tem praia do futuro movimentada\",\n","    \"recife tem o porto digital de tecnologia\",\n","    \"porto alegre tem inverno frio\",\n","    \"florianopolis tem muitas praias e trilhas\",\n","    \"natal tem duna e passeio de buggy\",\n","    \"gramado recebe muitos turistas no inverno\",\n","\n","    \"o estadio maracana recebe finais e classicos de futebol\",\n","    \"o morumbi recebe shows e jogos de futebol\",\n","    \"o campus da universidade tem hospital universitario\",\n","    \"o hospital universitario participa de pesquisa clinica\",\n","    \"a universidade organiza torneio de futebol entre cursos\",\n","    \"a praia de copacabana recebe turistas e atletas\",\n","    \"o laboratorio de dados analisa estatisticas do campeonato\",\n","    \"o time treina na academia do clube\",\n","    \"a cafeteria do campus serve cafe especial\",\n","    \"o congresso de ia ocorre em sao paulo\",\n","    \"o museu do futebol fica no estadio do pacaembu\",\n","    \"o centro de inovacao conecta startups e universidade\",\n","]"]},{"cell_type":"code","execution_count":4,"id":"d7f43155","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7f43155","executionInfo":{"status":"ok","timestamp":1756430528622,"user_tz":240,"elapsed":5646,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"bf760cb5-6054-4c6f-da5a-99353d722b90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulário: 373 Pares: 4404\n","Época 1: loss=3.9222\n","Época 2: loss=3.3198\n","Época 3: loss=2.9047\n","Época 4: loss=2.6319\n","Época 5: loss=2.4482\n","Época 6: loss=2.3066\n","Época 7: loss=2.1929\n","Época 8: loss=2.0837\n","Mais similares a 'brasil': [('dataset', 0.7110552168929765), ('noticias', 0.7014946646290638), ('exporta', 0.6615901013568335), ('produz', 0.6314033832038936), ('europa', 0.6210742745737582)]\n","Mais similares a 'casa': [('fora', 0.8386197413733373), ('paulista', 0.8097028074874444), ('empatou', 0.7889224316204481), ('time', 0.6162074163412092), ('joga', 0.39942721620393296)]\n","Mais similares a 'pesquisa': [('python', 0.6655366165945182), ('natural', 0.6609540536280938), ('clinica', 0.657251436114786), ('utiliza', 0.6550364899392077), ('artificial', 0.6422188582347633)]\n"]}],"source":["# Mini Skip-gram com Negative Sampling (NumPy)\n","import numpy as np\n","from collections import Counter, defaultdict\n","rng = np.random.default_rng(42)\n","\n","# --- 1) Corpus e preparação ---\n","sentences = [s.split() for s in corpus]\n","window = 5\n","tokens = [w for s in sentences for w in s]\n","vocab = sorted(set(tokens))\n","word2id = {w:i for i,w in enumerate(vocab)}\n","id2word = {i:w for w,i in word2id.items()}\n","ids = [word2id[w] for w in tokens]\n","\n","# Pares (central, contexto)\n","pairs = []\n","for s in sentences:\n","    ids_s = [word2id[w] for w in s]\n","    for t, c in enumerate(ids_s):\n","        L = max(0, t-window); R = min(len(ids_s), t+window+1)\n","        for j in range(L, R):\n","            if j==t: continue\n","            pairs.append((c, ids_s[j]))\n","\n","V = len(vocab)\n","print(\"Vocabulário:\", V, \"Pares:\", len(pairs))\n","\n","# Frequências para negativos (unigrama^0.75)\n","freq = Counter(ids)\n","unigram = np.array([freq[i] for i in range(V)], dtype=np.float64)\n","noise_dist = unigram**0.75\n","noise_dist /= noise_dist.sum()\n","\n","def sample_negatives(k):\n","    return rng.choice(V, size=k, replace=True, p=noise_dist)\n","\n","# --- 2) Parâmetros ---\n","d = 100\n","lr = 0.05\n","k = 5  # negativos\n","# Vetores: centro (V) e contexto (U)\n","V_c = (rng.standard_normal((V, d)) / np.sqrt(d))\n","U_o = (rng.standard_normal((V, d)) / np.sqrt(d))\n","\n","def sigmoid(x): return 1/(1+np.exp(-x))\n","\n","# --- 3) Treino ---\n","def train(epochs=5):\n","    global V_c, U_o\n","    for ep in range(epochs):\n","        rng.shuffle(pairs)\n","        loss = 0.0\n","        for c,o in pairs:\n","            vc = V_c[c]          # (d,)\n","            uo = U_o[o]          # (d,)\n","            negs = sample_negatives(k)\n","            # evitar amostrar o positivo como negativo (opcional)\n","            negs = np.array([w for w in negs if w!=o])\n","            if len(negs)<k:\n","                extra = sample_negatives(k-len(negs))\n","                negs = np.concatenate([negs, extra])\n","\n","            # positivo\n","            score_pos = uo @ vc\n","            sig_pos = sigmoid(score_pos)\n","            loss += -np.log(sig_pos + 1e-10)\n","\n","            # negativos\n","            u_negs = U_o[negs]            # (k,d)\n","            scores_neg = u_negs @ vc      # (k,)\n","            sig_neg = sigmoid(-scores_neg)\n","            loss += -np.sum(np.log(sig_neg + 1e-10))\n","\n","            # grad vc\n","            grad_v = (sig_pos - 1.0)*uo + np.sum(sigmoid(scores_neg)[:,None]*u_negs, axis=0)\n","            # grad uo e u_negs\n","            grad_uo = (sig_pos - 1.0)*vc\n","            grad_unegs = (sigmoid(scores_neg)[:,None])*vc[None,:]\n","\n","            # atualizações\n","            V_c[c] -= lr * grad_v\n","            U_o[o] -= lr * grad_uo\n","            U_o[negs] -= lr * grad_unegs\n","\n","        print(f\"Época {ep+1}: loss={loss/len(pairs):.4f}\")\n","\n","train(epochs=8)\n","\n","# --- 4) Similaridades\n","def most_similar(word, topn=5):\n","    wid = word2id[word]\n","    # vetor final: média de V e U (prática comum)\n","    W = (V_c + U_o)/2\n","    w = W[wid]\n","    sims = W @ w / (np.linalg.norm(W, axis=1)*np.linalg.norm(w)+1e-9)\n","    best = np.argsort(-sims)\n","    return [(id2word[i], float(sims[i])) for i in best if i!=wid][:topn]\n","\n","print(\"Mais similares a 'brasil':\", most_similar(\"brasil\"))\n","print(\"Mais similares a 'casa':\", most_similar(\"casa\"))\n","print(\"Mais similares a 'pesquisa':\", most_similar(\"pesquisa\"))"]},{"cell_type":"markdown","id":"ec0c6ec3","metadata":{"id":"ec0c6ec3"},"source":["### CBOW\n","\n","No **CBOW**, agregamos os vetores de contexto (p.ex., soma ou média) para prever a central:\n","$$\n","\\mathbf{h} = \\sum_{j\\in \\mathcal{C}(t)} \\mathbf{u}_{w_{t+j}},\n","\\qquad\n","P(w_t\\mid \\mathcal{C}(t)) = \\text{softmax}(V\\,\\mathbf{h})\n","$$\n","Os gradientes seguem de forma análoga ao caso Skip-gram, com $\\mathbf{h}$ no lugar de $\\mathbf{v}_c$.\n","\n","**Observações práticas**:\n","- **Skip-gram** tende a funcionar melhor para palavras raras (mais pares por central).\n","- **CBOW** é mais rápido e liso, pois agrega contexto (menos ruído).\n","- Janela pequena → mais **sintaxe**; janela grande → mais **semântica**."]},{"cell_type":"markdown","id":"1eb1302d","metadata":{"id":"1eb1302d"},"source":["## 3) GloVe — Global Vectors (Pennington, Socher, Manning, 2014)\n","\n","Constrói matriz de **coocorrência** $X \\in \\mathbb{R}^{|V|\\times |V|}$, onde $X_{ij}$ é a contagem (ponderada por distância) de $j$ no contexto de $i$.\n","\n","**Objetivo:**\n","$$\n","J = \\sum_{i,j} f(X_{ij})\\left( \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n","$$\n","com\n","$$\n","f(x)=\\begin{cases}\n","(x/x_{\\max})^\\alpha, & \\text{se } x < x_{\\max}\\\\\n","1, & \\text{caso contrário}\n","\\end{cases}\n","\\quad\\text{(tipicamente } x_{\\max}=100,\\ \\alpha=\\tfrac{3}{4}\\text{)}\n","$$\n","\n","**Gradientes:**\n","Defina o erro\n","$\n","E_{ij} = \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij}.\n","$\n","Então\n","$$\n","\\frac{\\partial J}{\\partial \\mathbf{w}_i} = \\sum_j f(X_{ij})\\,E_{ij}\\,\\tilde{\\mathbf{w}}_j,\\quad\n","\\frac{\\partial J}{\\partial \\tilde{\\mathbf{w}}_j} = \\sum_i f(X_{ij})\\,E_{ij}\\,\\mathbf{w}_i\n","$$\n","$$\n","\\frac{\\partial J}{\\partial b_i} = \\sum_j f(X_{ij})\\,E_{ij},\\quad\n","\\frac{\\partial J}{\\partial \\tilde{b}_j} = \\sum_i f(X_{ij})\\,E_{ij}\n","$$"]},{"cell_type":"markdown","id":"41799259","metadata":{"id":"41799259"},"source":["### Intuição da função objetivo do GloVe\n","\n","**Objetivo:**\n","$$\n","J = \\sum_{i,j} f(X_{ij})\\left( \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n","$$\n","\n","- $X_{ij}$ = número de vezes que a palavra $j$ aparece no contexto de $i$ (coocorrência).  \n","- $\\mathbf{w}_i, \\tilde{\\mathbf{w}}_j$ = vetores da palavra $i$ e do contexto $j$.  \n","- $b_i, \\tilde{b}_j$ = vieses (ajustes de escala).  \n","- $f(X_{ij})$ = função de ponderação que controla o peso de cada par $(i,j)$.  \n","\n","---\n","\n","#### O que essa equação está dizendo?\n","\n","1. **Queremos aproximar:**\n","$$\n","\\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j \\;\\;\\approx\\;\\; \\log X_{ij}\n","$$\n","\n","Ou seja:  \n","o **produto interno dos vetores** (mais biases) deve se alinhar com o **log da coocorrência**.  \n","\n","- Se duas palavras aparecem muito juntas → $\\log X_{ij}$ alto → vetor $\\mathbf{w}_i$ precisa ficar próximo de $\\tilde{\\mathbf{w}}_j$.  \n","- Se raramente aparecem juntas → $\\log X_{ij}$ baixo → o produto interno deve ser pequeno ou negativo.\n","\n","---\n","\n","2. **Por que usar $\\log X_{ij}$ em vez de $X_{ij}$?**  \n","- As contagens brutas crescem muito rápido.  \n","- O log “comprime” grandes diferenças, permitindo comparar palavras comuns e raras de forma mais equilibrada.\n","\n","---\n","\n","3. **Por que a função de ponderação $f(x)$?**  \n","$$\n","f(x)=\\begin{cases}\n","(x/x_{\\max})^\\alpha & \\text{se } x < x_{\\max} \\\\\n","1 & \\text{caso contrário}\n","\\end{cases}\n","$$\n","\n","- Pares muito frequentes (ex.: “de”, “o”) **não devem dominar** o treinamento → $f(x)$ os limita.  \n","- Pares muito raros (coocorrência próxima de 0) **não são confiáveis** → $f(x)$ os enfraquece.  \n","- Tipicamente: $x_{\\max}=100$, $\\alpha=3/4$.\n","\n","---\n","\n","#### E os gradientes?\n","\n","Definindo o erro:\n","$$\n","E_{ij} = \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij}\n","$$\n","\n","- **Para $\\mathbf{w}_i$:**  \n","  move o vetor da palavra $i$ para mais perto ou mais longe de $\\tilde{\\mathbf{w}}_j$, dependendo do erro.  \n","- **Para $\\tilde{\\mathbf{w}}_j$:**  \n","  ajuste simétrico para o vetor de contexto.  \n","- **Para os bias $b_i, \\tilde{b}_j$:**  \n","  pequenos deslocamentos que ajudam a calibrar os valores sem precisar alterar todo o vetor.\n","\n","---\n","\n","### Intuição global do GloVe\n","\n","- Cada par de palavras é uma **equação de regressão**:  \n","  “o quanto $i$ e $j$ coocorrem deve ser igual ao produto dos seus vetores”.  \n","- O treinamento ajusta todos os vetores para que essas equações sejam satisfeitas “o melhor possível”.  \n","- O resultado são embeddings que preservam **relações semânticas lineares** (ex.: rei − homem + mulher ≈ rainha).\n","\n","---"]},{"cell_type":"code","execution_count":5,"id":"8ccc6143","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ccc6143","executionInfo":{"status":"ok","timestamp":1756431384392,"user_tz":240,"elapsed":8116,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"caf6b07f-ccc4-48de-8e42-0167f6530bcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["[GloVe] it 01 | J/|X| = 0.0076\n","[GloVe] it 02 | J/|X| = 0.0022\n","[GloVe] it 03 | J/|X| = 0.0007\n","[GloVe] it 04 | J/|X| = 0.0003\n","[GloVe] it 05 | J/|X| = 0.0002\n","[GloVe] it 10 | J/|X| = 0.0000\n","[GloVe] it 20 | J/|X| = 0.0000\n","[GloVe] it 30 | J/|X| = 0.0000\n","[GloVe] it 40 | J/|X| = 0.0000\n","[GloVe] it 50 | J/|X| = 0.0000\n","[GloVe] it 60 | J/|X| = 0.0000\n","\n","[GloVe] vizinhos de 'rio': [('o', 0.5710707482288602), ('no', 0.5353837859189711), ('brasil', 0.35991923669257053), ('classico', 0.33672464589557843), ('jogo', 0.3336451727563669), ('de', 0.29578503725832733), ('estadio', 0.27284795525861805)]\n","[GloVe] vizinhos de 'futebol': [('de', 0.47555533787574417), ('laboratorio', 0.4036974739370981), ('entre', 0.3443427537920709), ('sala', 0.3329014533438338), ('doenca', 0.33026864791887695), ('atendimento', 0.3192575251714123), ('com', 0.3176498737600798)]\n","[GloVe] vizinhos de 'universidade': [('a', 0.6196808997322713), ('e', 0.4728297843586194), ('tem', 0.45525250338372514), ('praia', 0.4310534321557507), ('finais', 0.39890977582162335), ('queijo', 0.39675900973428674), ('floresta', 0.37066421029534485)]\n"]}],"source":["# ===== Mini-GloVe\n","import numpy as np\n","from collections import defaultdict\n","rng = np.random.default_rng(0)\n","\n","# 1) Matriz de coocorrência esparsa X com janela simétrica ponderada (1/dist)\n","window = 5  # janela um pouco maior costuma ajudar semântica\n","X = defaultdict(float)\n","for s in sentences:\n","    ids_s = [word2id[w] for w in s if w in word2id]\n","    for t, wi in enumerate(ids_s):\n","        L = max(0, t-window); R = min(len(ids_s), t+window+1)\n","        for j in range(L, R):\n","            if j == t:\n","                continue\n","            wj = ids_s[j]\n","            dist = abs(j - t)\n","            X[(wi, wj)] += 1.0 / dist\n","\n","V = len(word2id)\n","d = 100                 # um pouco maior que no toy anterior\n","W  = rng.normal(scale=0.1, size=(V, d))\n","Wt = rng.normal(scale=0.1, size=(V, d))\n","b  = np.zeros(V)\n","bt = np.zeros(V)\n","\n","xmax, alpha = 100.0, 0.75\n","def f(x):\n","    return (x/xmax)**alpha if x < xmax else 1.0\n","\n","# AdaGrad\n","eps = 1e-8\n","gW  = np.zeros_like(W)\n","gWt = np.zeros_like(Wt)\n","gb  = np.zeros_like(b)\n","gbt = np.zeros_like(bt)\n","\n","keys = list(X.keys())\n","\n","def glove_train(iters=70, lr=0.05):\n","    for it in range(iters):\n","        rng.shuffle(keys)\n","        J = 0.0\n","        for (i, j) in keys:\n","            xij = X[(i, j)]\n","            wij = W[i]; wjt = Wt[j]\n","            err = wij @ wjt + b[i] + bt[j] - np.log(xij)\n","            fij = f(xij)\n","            J += 0.5 * fij * (err**2)\n","\n","            # gradientes\n","            grad_wi = fij * err * wjt\n","            grad_wj = fij * err * wij\n","            grad_bi = fij * err\n","            grad_bj = fij * err\n","\n","            # AdaGrad updates\n","            gW[i]  += grad_wi**2\n","            gWt[j] += grad_wj**2\n","            gb[i]  += grad_bi**2\n","            gbt[j] += grad_bj**2\n","\n","            W[i]  -= lr * grad_wi / (np.sqrt(gW[i])  + eps)\n","            Wt[j] -= lr * grad_wj / (np.sqrt(gWt[j]) + eps)\n","            b[i]  -= lr * grad_bi / (np.sqrt(gb[i])  + eps)\n","            bt[j] -= lr * grad_bj / (np.sqrt(gbt[j]) + eps)\n","\n","        if (it+1) % 10 == 0 or it < 5:\n","            print(f\"[GloVe] it {it+1:02d} | J/|X| = {J/len(keys):.4f}\")\n","\n","glove_train(iters=60, lr=0.05)\n","\n","# Vetores finais práticos: soma\n","E_glove = W + Wt\n","\n","def most_similar_glove(word, topn=10):\n","    if word not in word2id:\n","        raise KeyError(f\"'{word}' não está no vocabulário do GloVe.\")\n","    wid = word2id[word]\n","    w = E_glove[wid]\n","    norms = np.linalg.norm(E_glove, axis=1) * (np.linalg.norm(w) + 1e-9)\n","    sims = (E_glove @ w) / (norms + 1e-9)\n","    order = np.argsort(-sims)\n","    return [(id2word[i], float(sims[i])) for i in order if i != wid][:topn]\n","\n","print(\"\\n[GloVe] vizinhos de 'rio':\", most_similar_glove(\"rio\")[:7])\n","print(\"[GloVe] vizinhos de 'futebol':\", most_similar_glove(\"futebol\")[:7])\n","print(\"[GloVe] vizinhos de 'universidade':\", most_similar_glove(\"universidade\")[:7])"]},{"cell_type":"markdown","id":"9b552bb8","metadata":{"id":"9b552bb8"},"source":["## 4) FastText (Bojanowski et al., 2017)\n","\n","O **FastText** foi proposto pelo Facebook em 2017 como uma extensão natural do Word2Vec.  \n","A grande diferença é que **cada palavra não é tratada como indivisível**, mas como uma soma de vetores de **sub-palavras** (n-gramas de caracteres).\n","\n","---\n","\n","### Ideia central\n","- Para cada palavra $w$, consideramos todos os seus **n-gramas de caracteres** (tipicamente $n \\in [3,6]$).  \n","- Adicionam-se símbolos de início/fim (`<`, `>`) para capturar prefixos e sufixos.  \n","\n","**Exemplo (n=3):**\n","- Palavra: `\"casa\"`  \n","- n-gramas: `<ca`, `cas`, `asa`, `sa>`  \n","\n","Cada n-grama tem um vetor $\\mathbf{z}_g$.  \n","O vetor final da palavra é a soma (ou média) desses vetores:\n","\n","$$\n","\\mathbf{z}(w)=\\sum_{g\\in G_w} \\mathbf{z}_g\n","$$\n","\n","---\n","\n","### Como o treinamento funciona?\n","O modelo FastText treina quase da mesma forma que o Skip-gram com *negative sampling*:  \n","\n","- Antes: no Skip-gram padrão, usamos o vetor da palavra central $\\mathbf{v}_c$.  \n","- Agora: substituímos $\\mathbf{v}_c$ por $\\mathbf{z}(w_c)$, que é construído a partir de n-gramas.  \n","\n","Assim, **as mesmas equações de probabilidade e gradientes** do Word2Vec continuam válidas, mas agora a informação é **compartilhada entre palavras com pedaços semelhantes**.\n","\n","---\n","\n","### Por que isso é importante?\n","\n","1. **Palavras raras**  \n","   - Word2Vec e GloVe precisam ver muitas ocorrências para aprender um bom vetor.  \n","   - FastText generaliza porque compartilha n-gramas entre palavras.  \n","   - Ex.: \"corrida\", \"correr\", \"corredor\" → compartilham `corr`.\n","\n","2. **Palavras fora do vocabulário (OOV)**  \n","   - Em Word2Vec/GloVe, se a palavra não está no vocabulário → não existe vetor.  \n","   - Em FastText, ainda podemos gerar um embedding **somando os n-gramas** (mesmo que a palavra nunca tenha aparecido).  \n","   - Ex.: \"cachorrinho\" → pode ser decomposta em sub-palavras conhecidas de \"cachorro\".\n","\n","3. **Morfologia rica (como no português)**  \n","   - Idiomas flexivos têm muitas variações de uma mesma raiz (plural, feminino, conjugação verbal).  \n","   - FastText é capaz de capturar essas regularidades sem precisar de milhões de ocorrências por forma.\n","\n","---\n","\n","### Comparação rápida\n","- **Word2Vec/GloVe:** vetor único por palavra (não entende morfologia).  \n","- **FastText:** vetor = soma de sub-palavras → mais robusto para línguas com flexão.  \n","- **Limitação:** continua sendo um modelo **estático** (não resolve polissemia como BERT/ELMo).\n","\n","---\n","\n","### Vantagens práticas\n","- Funciona muito bem em **português** e outras línguas ricas em morfologia.  \n","- Permite embeddings para **neologismos** e **palavras OOV**.  \n","- Modelo rápido, aproveitando a mesma mecânica do Skip-gram com Negative Sampling.  "]},{"cell_type":"code","execution_count":8,"id":"9dada38e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dada38e","executionInfo":{"status":"ok","timestamp":1756432151028,"user_tz":240,"elapsed":2782,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"20020e5f-a14a-447a-8d81-43159a4a8099"},"outputs":[{"output_type":"stream","name":"stdout","text":["[FastText] vizinhos de 'praia': [('praias', 0.6014888286590576), ('cafeteria', 0.4616388976573944), ('corta', 0.44736960530281067), ('cafeterias', 0.4299085736274719), ('cor', 0.41287893056869507), ('apita', 0.4104938507080078), ('amazonia', 0.41014111042022705)]\n","[FastText] vizinhos de 'hospital': [('grama', 0.4797201454639435), ('espresso', 0.4091796278953552), ('capital', 0.3997696042060852), ('apita', 0.39324894547462463), ('gramado', 0.3681802749633789), ('de', 0.36249253153800964), ('treinador', 0.3564921021461487)]\n","[FastText] vizinhos de 'futebol' pelo FastText: [('aplica', 0.36737164855003357), ('seminario', 0.3531761169433594), ('de', 0.350708931684494), ('dataset', 0.3385619521141052), ('pacientes', 0.33558928966522217), ('comunidade', 0.328666627407074), ('e', 0.32252174615859985)]\n"]}],"source":["from gensim.models import FastText\n","\n","ft_model = FastText(\n","    sentences=sentences,\n","    vector_size=100,\n","    window=3,\n","    min_count=1,\n","    workers=2,\n","    sg=1\n",")\n","\n","def most_similar_fasttext(word, topn=10):\n","    return ft_model.wv.most_similar(word, topn=topn)\n","\n","print(\"[FastText] vizinhos de 'praia':\", most_similar_fasttext(\"praia\")[:7])\n","print(\"[FastText] vizinhos de 'hospital':\", most_similar_fasttext(\"hospital\")[:7])\n","\n","oov = \"futebolzinho\"\n","vec_oov = ft_model.wv[oov]\n","print(\"[FastText] vizinhos de 'futebol' pelo FastText:\", most_similar_fasttext(\"futebol\")[:7])"]},{"cell_type":"code","execution_count":7,"id":"614ba086","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"614ba086","executionInfo":{"status":"ok","timestamp":1756432109605,"user_tz":240,"elapsed":18,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"0882ef91-531f-4e1c-e743-91007a175f1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Comparação] W2V vizinhos de 'rio': [('negro', 0.8243900187907073), ('solimoes', 0.8036408041739286), ('encontra', 0.7706052551916701), ('manaus', 0.6894251696101605), ('janeiro', 0.6652490168761672), ('bonita', 0.6312710137635308), ('porto', 0.6017809013231292)]\n","[Comparação] GloVe vizinhos de 'rio': [('o', 0.5710707482288602), ('no', 0.5353837859189711), ('brasil', 0.35991923669257053), ('classico', 0.33672464589557843), ('jogo', 0.3336451727563669), ('de', 0.29578503725832733), ('estadio', 0.27284795525861805)]\n","[Comparação] FastText vizinhos de 'rio': [('laboratorio', 0.5692732930183411), ('cafeterias', 0.5472129583358765), ('prontuario', 0.531193196773529), ('frio', 0.5243390798568726), ('dados', 0.497134268283844), ('rios', 0.4895647168159485), ('estadio', 0.4870374798774719)]\n"]}],"source":["W_w2v = (V_c + U_o) / 2\n","\n","def vec_w2v(token):\n","    if token not in word2id:\n","        raise KeyError(f\"'{token}' não está no vocabulário do modelo W2V caseiro.\")\n","    return W_w2v[word2id[token]]\n","\n","def vec_glove(token):\n","    if token not in word2id:\n","        raise KeyError(f\"'{token}' não está no vocabulário do GloVe.\")\n","    return E_glove[word2id[token]]\n","\n","def vec_fasttext(token):\n","    return ft_model.wv[token]  # lida com OOV via subword\n","\n","def most_similar_generic(model_name, token, topn=10):\n","    if model_name == \"w2v\":\n","        w = vec_w2v(token)\n","        E = W_w2v\n","    elif model_name == \"glove\":\n","        w = vec_glove(token)\n","        E = E_glove\n","    elif model_name == \"fasttext\":\n","        return ft_model.wv.most_similar(token, topn=topn)\n","    else:\n","        raise ValueError(\"model_name deve ser 'w2v', 'glove' ou 'fasttext'.\")\n","    sims = E @ w / (np.linalg.norm(E, axis=1) * (np.linalg.norm(w) + 1e-9) + 1e-9)\n","    order = np.argsort(-sims)\n","    wid = word2id.get(token, None)\n","    result = []\n","    for i in order:\n","        if wid is not None and i == wid:\n","            continue\n","        result.append((id2word[i], float(sims[i])))\n","        if len(result) >= topn:\n","            break\n","    return result\n","\n","print(\"[Comparação] W2V vizinhos de 'rio':\", most_similar_generic(\"w2v\",\"rio\")[:7])\n","print(\"[Comparação] GloVe vizinhos de 'rio':\", most_similar_generic(\"glove\",\"rio\")[:7])\n","print(\"[Comparação] FastText vizinhos de 'rio':\", most_similar_generic(\"fasttext\",\"rio\")[:7])"]},{"cell_type":"code","execution_count":9,"id":"bac8c52a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bac8c52a","executionInfo":{"status":"ok","timestamp":1756432205939,"user_tz":240,"elapsed":5234,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"4e0911fd-0aa0-412a-e0ec-969c546e76f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Exemplo com bigramas: ['o', 'brasil', 'gosta', 'de_futebol']\n","\n","=== Vizinhos mais similares (CBOW) ===\n","            rio -> [('solimoes', 0.9957651495933533), ('encontra', 0.9957128763198853), ('negro', 0.9931566715240479), ('futuro', 0.9918559789657593), ('bonita', 0.9918133616447449)]\n"," rio_de_janeiro -> [('<<OOV ou ausente no vocabulário>>', 0.0)]\n","      sao_paulo -> [('referencia', 0.9949139952659607), ('cafeterias', 0.9941678047180176), ('hospital', 0.993751049041748), ('leitos', 0.9937194585800171), ('trilhas', 0.993485689163208)]\n","        futebol -> [('pacaembu', 0.9938187003135681), ('museu', 0.9928635358810425), ('no_estadio', 0.9914768934249878), ('fica', 0.9913777112960815), ('janeiro', 0.989892303943634)]\n","   universidade -> [('extensao', 0.9937509894371033), ('comunidade', 0.99355149269104), ('inovacao', 0.9928590059280396), ('conecta', 0.9927719235420227), ('startups', 0.9926608800888062)]\n","       hospital -> [('uti', 0.9938451647758484), ('sao_paulo', 0.9937511086463928), ('carnaval', 0.9936617016792297), ('referencia', 0.9935713410377502), ('leitos', 0.9932467937469482)]\n","       amazonia -> [('densa', 0.9957613348960876), ('chuva', 0.994529664516449), ('biodiversidade', 0.9939336180686951), ('selva', 0.9931902289390564), ('abriga', 0.9931661486625671)]\n","           cafe -> [('produz', 0.9940185546875), ('gerais', 0.9935325384140015), ('exporta', 0.9933426380157471), ('europa', 0.9932353496551514), ('alta', 0.9932056069374084)]\n","       maracana -> [('classicos', 0.9948269724845886), ('estadio', 0.9934882521629333), ('importantes', 0.9925872683525085), ('finais', 0.9903852343559265), ('recebe', 0.9866536259651184)]\n","\n","=== Vizinhos mais similares (Skip-gram) ===\n","            rio -> [('encontra', 0.9386967420578003), ('solimoes', 0.932676374912262), ('negro', 0.8941337466239929), ('bonita', 0.8901340961456299), ('praia', 0.8855618834495544)]\n"," rio_de_janeiro -> [('<<OOV ou ausente no vocabulário>>', 0.0)]\n","      sao_paulo -> [('cafeterias', 0.890882134437561), ('referencia', 0.8768629431724548), ('hospital', 0.8734146356582642), ('leitos', 0.8557650446891785), ('uti', 0.8472994565963745)]\n","        futebol -> [('pacaembu', 0.9560561180114746), ('museu', 0.9520156383514404), ('no_estadio', 0.9180850982666016), ('janeiro', 0.9180223941802979), ('fica', 0.8982257843017578)]\n","   universidade -> [('comunidade', 0.9677749872207642), ('extensao', 0.9588946104049683), ('conecta', 0.9476365447044373), ('startups', 0.9395183324813843), ('inovacao', 0.9170370697975159)]\n","       hospital -> [('leitos', 0.9349290728569031), ('uti', 0.9227590560913086), ('famoso', 0.9053441882133484), ('referencia', 0.9043861627578735), ('universitario', 0.9040223956108093)]\n","       amazonia -> [('selva', 0.9713646769523621), ('densa', 0.9560202360153198), ('abriga', 0.9347118139266968), ('biodiversidade', 0.9235531091690063), ('grande', 0.9196434617042542)]\n","           cafe -> [('produz', 0.8526323437690735), ('especial', 0.8418427109718323), ('brasil', 0.8173578381538391), ('serve', 0.8159837126731873), ('arabica', 0.815819263458252)]\n","       maracana -> [('classicos', 0.9467964172363281), ('estadio', 0.9463880062103271), ('finais', 0.9337008595466614), ('importantes', 0.931943416595459), ('morumbi', 0.8841118216514587)]\n","\n","=== Vizinhos mais similares (FastText) ===\n","            rio -> [('frio', 0.926480770111084), ('negro', 0.9235453605651855), ('no_rio', 0.9089738726615906), ('janeiro', 0.8793288469314575), ('solimoes', 0.8744482398033142)]\n"," rio_de_janeiro -> [('janeiro', 0.9619547724723816), ('goleiro', 0.95719975233078), ('rodada', 0.954512894153595), ('zagueiro', 0.95448237657547), ('manaus', 0.9445387125015259)]\n","      sao_paulo -> [('uti', 0.9588592648506165), ('carnaval', 0.9505115747451782), ('famoso', 0.9480129480361938), ('noticias', 0.9472743272781372), ('capital', 0.9344640374183655)]\n","        futebol -> [('de_futebol', 0.9706323146820068), ('no_estadio', 0.9633113741874695), ('museu', 0.9420206546783447), ('pacaembu', 0.928810179233551), ('janeiro', 0.9074875712394714)]\n","   universidade -> [('a_universidade', 0.998326301574707), ('biodiversidade', 0.9916224479675293), ('comunidade', 0.9489728808403015), ('universitario', 0.9464476704597473), ('qualidade', 0.9399711489677429)]\n","       hospital -> [('hospital_universitario', 0.9811592102050781), ('universitario', 0.9722393751144409), ('capital', 0.9665060639381409), ('natal', 0.9436718225479126), ('famoso', 0.9268055558204651)]\n","       amazonia -> [('selva', 0.9631646275520325), ('amazonica', 0.9622425436973572), ('densa', 0.9604120850563049), ('amazonas', 0.9504308104515076), ('intensa', 0.9187278747558594)]\n","           cafe -> [('serve', 0.8878685235977173), ('cafeteria', 0.8878611922264099), ('prepara', 0.8801103830337524), ('especial', 0.8554383516311646), ('espirito', 0.8483270406723022)]\n","       maracana -> [('finais', 0.9658620357513428), ('de_futebol', 0.9586420655250549), ('no_estadio', 0.9529639482498169), ('estadio', 0.9496219158172607), ('recebe', 0.920198380947113)]\n","\n","=== Analogia: rio_de_janeiro : maracana :: sao_paulo : ? ===\n","CBOW     -> [('classicos', 0.9880025386810303), ('importantes', 0.9869130253791809), ('estadio', 0.9854817390441895), ('finais', 0.9831718802452087), ('recebe', 0.9804791212081909)]\n","Skip-gram-> [('morumbi', 0.9150128364562988), ('classicos', 0.8763086795806885), ('shows', 0.868721067905426), ('finais', 0.8684640526771545), ('estadio', 0.8683205842971802)]\n","FastText -> [('shows', 0.9650833010673523), ('morumbi', 0.9618176221847534), ('final', 0.9297532439231873), ('finais', 0.9181427359580994), ('gerais', 0.9101065993309021)]\n","\n","OOV 'futebolzinho': norma do vetor FastText = 1.209199070930481\n","Vizinhos de 'futebol' no FastText -> [('de_futebol', 0.9706323146820068), ('no_estadio', 0.9633113741874695), ('museu', 0.9420206546783447), ('pacaembu', 0.928810179233551), ('janeiro', 0.9074875712394714), ('estadio', 0.9063723683357239), ('maracana', 0.8966490626335144)]\n"]}],"source":["import numpy as np\n","from gensim.models import Word2Vec, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","\n","# 1) Tokenização e bigramas\n","sentences = [s.split() for s in corpus]\n","\n","# aprende bigramas no seu corpus\n","phrases = Phrases(sentences, min_count=2, threshold=5, delimiter='_')\n","bigram = Phraser(phrases)\n","sentences_bg = [bigram[s] for s in sentences]\n","\n","print(\"Exemplo com bigramas:\", sentences_bg[0][:15])\n","\n","vector_size = 100\n","window = 5\n","min_count = 1\n","workers = 2\n","\n","# Word2Vec CBOW\n","w2v_cbow = Word2Vec(\n","    sentences=sentences_bg,\n","    vector_size=vector_size,\n","    window=window,\n","    min_count=min_count,\n","    workers=workers,\n","    sg=0,   # CBOW\n","    negative=10,\n","    epochs=150\n",")\n","\n","# Word2Vec Skip-gram\n","w2v_sg = Word2Vec(\n","    sentences=sentences_bg,\n","    vector_size=vector_size,\n","    window=window,\n","    min_count=min_count,\n","    workers=workers,\n","    sg=1,   # Skip-gram\n","    negative=10,\n","    epochs=150\n",")\n","\n","# FastText Skip-gram (subwords)\n","ft_sg = FastText(\n","    sentences=sentences_bg,\n","    vector_size=vector_size,\n","    window=window,\n","    min_count=min_count,\n","    workers=workers,\n","    sg=1,   # Skip-gram\n","    negative=10,\n","    epochs=150\n",")\n","\n","def most_similar(model, token, topn=7):\n","    try:\n","        return model.wv.most_similar(token, topn=topn)\n","    except KeyError as e:\n","        return [(\"<<OOV ou ausente no vocabulário>>\", 0.0)]\n","\n","def phrase_mean_vec(model, phrase_tokens):\n","    vecs = []\n","    for t in phrase_tokens:\n","        if t in model.wv:\n","            vecs.append(model.wv[t])\n","    if not vecs:\n","        raise KeyError(f\"Nenhum token com vetor em {phrase_tokens}\")\n","    return np.mean(vecs, axis=0)\n","\n","def analogy(model, a, b, c, topn=7):\n","    # a : b :: c : ?\n","    def get_vec(term):\n","        if term in model.wv:\n","            return model.wv[term]\n","        else:\n","            toks = term.split('_')\n","            return phrase_mean_vec(model, toks)\n","\n","    va, vb, vc = get_vec(a), get_vec(b), get_vec(c)\n","    target = vb - va + vc\n","    sims = model.wv.cosine_similarities(target, model.wv.vectors)\n","    order = np.argsort(-sims)\n","    idx2tok = model.wv.index_to_key\n","    # excluir termos da consulta\n","    excl = {a, b, c}\n","    ans = []\n","    for idx in order:\n","        tok = idx2tok[idx]\n","        if tok in excl:\n","            continue\n","        ans.append((tok, float(sims[idx])))\n","        if len(ans) >= topn:\n","            break\n","    return ans\n","\n","probes = [\"rio\", \"rio_de_janeiro\", \"sao_paulo\", \"futebol\", \"universidade\", \"hospital\", \"amazonia\", \"cafe\", \"maracana\"]\n","\n","print(\"\\n=== Vizinhos mais similares (CBOW) ===\")\n","for p in probes:\n","    print(f\"{p:>15} -> {most_similar(w2v_cbow, p)[:5]}\")\n","\n","print(\"\\n=== Vizinhos mais similares (Skip-gram) ===\")\n","for p in probes:\n","    print(f\"{p:>15} -> {most_similar(w2v_sg, p)[:5]}\")\n","\n","print(\"\\n=== Vizinhos mais similares (FastText) ===\")\n","for p in probes:\n","    print(f\"{p:>15} -> {most_similar(ft_sg, p)[:5]}\")\n","\n","print(\"\\n=== Analogia: rio_de_janeiro : maracana :: sao_paulo : ? ===\")\n","print(\"CBOW     ->\", analogy(w2v_cbow, \"rio_de_janeiro\", \"maracana\", \"sao_paulo\", topn=5))\n","print(\"Skip-gram->\", analogy(w2v_sg,   \"rio_de_janeiro\", \"maracana\", \"sao_paulo\", topn=5))\n","print(\"FastText ->\", analogy(ft_sg,    \"rio_de_janeiro\", \"maracana\", \"sao_paulo\", topn=5))\n","\n","oov = \"futebolzinho\"\n","vec_oov = ft_sg.wv[oov]\n","print(f\"\\nOOV '{oov}': norma do vetor FastText =\", float(np.linalg.norm(vec_oov)))\n","print(\"Vizinhos de 'futebol' no FastText ->\", most_similar(ft_sg, \"futebol\")[:7])"]},{"cell_type":"markdown","id":"e2e69cba","metadata":{"id":"e2e69cba"},"source":["## 5) Avaliação de embeddings\n","\n","**Intrínseca**\n","- **Similaridade**: correlação com anotações humanas (WordSim, RG65 etc.).\n","- **Analogia**: avaliar se $\\mathbf{w}_{\\text{king}} - \\mathbf{w}_{\\text{man}} + \\mathbf{w}_{\\text{woman}} \\approx \\mathbf{w}_{\\text{queen}}$.\n","- **Visualização**: TSNE/UMAP de vizinhanças semânticas.\n","\n","**Extrínseca**\n","- Plug-and-play em tarefas (NER, POS, classificação) e medir F1/acc.\n","\n","> Em PT-BR, tentem analogias como “Brasil : Brasília :: França : ?”."]},{"cell_type":"code","execution_count":10,"id":"e0260ef3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0260ef3","executionInfo":{"status":"ok","timestamp":1756432424809,"user_tz":240,"elapsed":47,"user":{"displayName":"Bruno Magalhaes Nogueira","userId":"18320277366917905276"}},"outputId":"f3593a3d-1847-49a7-a049-815c6ea7bc93"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[Analogy W2V] 'rio' : 'maracana' :: 'sao paulo' : ? [('morumbi', 0.6085105929441199), ('referencia', 0.6060654522152604), ('ocorre', 0.5560846161596892), ('ia', 0.5062746899176973), ('em', 0.48854337195450953)]\n","[Analogy GloVe] 'rio' : 'maracana' :: 'sao paulo' : ? [('manaus', 0.5674081380262639), ('cor', 0.5178318935988464), ('ipanema', 0.3984236274013174), ('bonita', 0.38364511741956864), ('startups', 0.3687289198018162)]\n","[Analogy FT] 'rio de janeiro' : 'maracana' :: 'sao paulo' : ? [('pao', 0.3076087236404419), ('ameaca', 0.27927762269973755), ('learning', 0.2667211592197418), ('parque', 0.2543705403804779), ('atendimento', 0.2465866506099701)]\n"]}],"source":["import re\n","def tokenize_phrase(text):\n","    # separa por espaço, mantém tokens tal como no corpus (sem acento/caixa já condizentes)\n","    return [t for t in re.split(r\"\\s+\", text.strip()) if t]\n","\n","def phrase_vec(model, phrase):\n","    toks = tokenize_phrase(phrase)\n","    vecs = []\n","    for t in toks:\n","        try:\n","            if model == \"w2v\":\n","                vecs.append(vec_w2v(t))\n","            elif model == \"glove\":\n","                vecs.append(vec_glove(t))\n","            elif model == \"fasttext\":\n","                vecs.append(vec_fasttext(t))\n","            else:\n","                raise ValueError\n","        except KeyError:\n","            # ignora tokens OOV no W2V/GloVe; FastText deve cobrir quase tudo\n","            pass\n","    if not vecs:\n","        raise KeyError(f\"Nenhum token com vetor em '{phrase}'.\")\n","    return np.mean(vecs, axis=0)\n","\n","def analogy(a, b, c, model=\"w2v\", topn=10):\n","    # Resolve analogia: a : b :: c : ?\n","    va = phrase_vec(model, a)\n","    vb = phrase_vec(model, b)\n","    vc = phrase_vec(model, c)\n","    target = vb - va + vc\n","\n","    # matriz de referência\n","    if model == \"w2v\":\n","        E = W_w2v\n","        idx2tok = id2word\n","    elif model == \"glove\":\n","        E = E_glove\n","        idx2tok = id2word\n","    elif model == \"fasttext\":\n","        # Para FastText, criamos uma matriz E para termos do vocabulário conhecido\n","        vocab_ft = list(ft_model.wv.key_to_index.keys())\n","        E = np.vstack([ft_model.wv[w] for w in vocab_ft])\n","        idx2tok = {i:w for i,w in enumerate(vocab_ft)}\n","    else:\n","        raise ValueError\n","\n","    sims = E @ target / (np.linalg.norm(E, axis=1) * (np.linalg.norm(target) + 1e-9) + 1e-9)\n","    order = np.argsort(-sims)\n","\n","    # Remove termos presentes na entrada (quando possível mapear para índice)\n","    exclude = set()\n","    for phr in [a,b,c]:\n","        for t in tokenize_phrase(phr):\n","            # mapear para índice correspondente\n","            if model in (\"w2v\",\"glove\"):\n","                if t in word2id:\n","                    exclude.add(word2id[t])\n","            else:\n","                # FastText: mapeamento por string\n","                pass\n","\n","    ans = []\n","    for i in order:\n","        tok = idx2tok[i]\n","        # excluir entradas\n","        if model in (\"w2v\",\"glove\"):\n","            if i in exclude:\n","                continue\n","        if tok in set(tokenize_phrase(a) + tokenize_phrase(b) + tokenize_phrase(c)):\n","            continue\n","        ans.append((tok, float(sims[i])))\n","        if len(ans) >= topn:\n","            break\n","    return ans\n","\n","# Exemplos: cidades e estádios (tokens disponíveis no corpus)\n","print(\"\\n[Analogy W2V] 'rio' : 'maracana' :: 'sao paulo' : ?\", analogy(\"rio\",\"maracana\",\"sao paulo\",\"w2v\",topn=5))\n","print(\"[Analogy GloVe] 'rio' : 'maracana' :: 'sao paulo' : ?\", analogy(\"rio\",\"maracana\",\"sao paulo\",\"glove\",topn=5))\n","print(\"[Analogy FT] 'rio de janeiro' : 'maracana' :: 'sao paulo' : ?\", analogy(\"rio de janeiro\",\"maracana\",\"sao paulo\",\"fasttext\",topn=5))"]},{"cell_type":"markdown","id":"fd3f12cc","metadata":{"id":"fd3f12cc"},"source":["## Referências\n","\n","- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781.\n","- Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors for Word Representation. In *EMNLP 2014*, 1532–1543.\n","- Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information. *TACL*, 5, 135–146.\n","- Firth, J. R. (1957). *A Synopsis of Linguistic Theory 1930–1955*. Oxford.\n","- Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. *TACL*, 3, 211–225."]},{"cell_type":"markdown","id":"0923b495","metadata":{"id":"0923b495"},"source":[]}],"metadata":{"kernelspec":{"display_name":"env-misc","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}