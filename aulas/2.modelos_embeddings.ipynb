{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95701fe4",
   "metadata": {},
   "source": [
    "# Tópicos em IA — Modelos de Linguagem\n",
    "## Aula: Word Embeddings Clássicos — **Word2Vec, GloVe e FastText**\n",
    "\n",
    "**Pré-requisitos:** Python básico, cálculo, estatística introdutória.\n",
    "\n",
    "### Objetivos\n",
    "- Entender, em detalhe, os modelos **Word2Vec (CBOW e Skip-gram)**, **GloVe** e **FastText**.\n",
    "- Derivar as **funções objetivo** e os **gradientes** centrais.\n",
    "- Implementar mini-versões dos algoritmos para compreender a mecânica de treinamento.\n",
    "- Comparar qualitativamente os modelos (semântica, morfologia, OOV).\n",
    "- Exercitar avaliação intrínseca (similaridade/analogia) e discutir vieses.\n",
    "\n",
    "> Referências base: notas do **CS224n** (Manning) sobre Word Vectors e GloVe; ver bibliografia ao final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f9c32",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "1) Motivação: do *one-hot* a vetores densos  \n",
    "2) **Word2Vec**: softmax, *negative sampling*, CBOW vs. Skip-gram; **derivações**  \n",
    "3) **GloVe**: matriz de coocorrência, objetivo ponderado, **derivações**  \n",
    "4) **FastText**: n-gramas de caracteres, OOV, morfologia  \n",
    "5) Avaliação (intrínseca/extrínseca), analogias, TSNE  \n",
    "6) Atividades guiadas, ética e vieses, ganchos para a próxima aula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc739c6",
   "metadata": {},
   "source": [
    "## 1) Motivação: do one-hot para embeddings\n",
    "\n",
    "**Problema do one-hot:** vetores são ortogonais; não há noção de similaridade.\n",
    "\n",
    "Sejam duas palavras `hotel` e `motel`:\n",
    "$$\n",
    "\\mathbf{e}_{\\text{hotel}},\\ \\mathbf{e}_{\\text{motel}} \\in \\mathbb{R}^{|V|},\\quad\n",
    "\\mathbf{e}_{\\text{hotel}}^\\top \\mathbf{e}_{\\text{motel}} = 0\n",
    "$$\n",
    "\n",
    "**Ideia:** aprender **vetores densos** $\\mathbf{w}\\in\\mathbb{R}^d$ (com $d\\ll |V|$) onde a proximidade (p.ex. cosseno) reflete semântica:\n",
    "$$\n",
    "\\cos(\\mathbf{w}_a,\\mathbf{w}_b)=\\frac{\\mathbf{w}_a^\\top \\mathbf{w}_b}{\\|\\mathbf{w}_a\\|\\,\\|\\mathbf{w}_b\\|}\n",
    "$$\n",
    "\n",
    "**Hipótese distribucional:** “Conhecerás a palavra pela companhia que ela mantém.” (Firth, 1957)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e4cbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosseno(one-hot hotel, motel) = 0.0\n",
      "Cosseno(embeddings densos) ≈ 0.9939728426989571\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Exemplo toy: \"hotel\" e \"motel\" em one-hot\n",
    "V = 6\n",
    "idx = {\"hotel\": 1, \"motel\": 4}\n",
    "e_hotel = np.eye(V)[idx[\"hotel\"]]\n",
    "e_motel = np.eye(V)[idx[\"motel\"]]\n",
    "\n",
    "cos = e_hotel @ e_motel / (np.linalg.norm(e_hotel)*np.linalg.norm(e_motel) + 1e-12)\n",
    "print(\"Cosseno(one-hot hotel, motel) =\", cos)  # 0.0\n",
    "\n",
    "# Exemplo de embeddings densos quaisquer (ilustrativo)\n",
    "rng = np.random.default_rng(0)\n",
    "w_hotel = rng.normal(size=50)\n",
    "w_motel = w_hotel + 0.1*rng.normal(size=50)  # ligeiramente parecido\n",
    "cos_dense = (w_hotel @ w_motel)/(np.linalg.norm(w_hotel)*np.linalg.norm(w_motel))\n",
    "print(\"Cosseno(embeddings densos) ≈\", float(cos_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bce4f2",
   "metadata": {},
   "source": [
    "## 2) Word2Vec (Mikolov et al., 2013)\n",
    "\n",
    "Dadas sequências $(w_1,\\dots,w_T)$, com janela de tamanho $m$:\n",
    "\n",
    "- **Skip-gram:** prediz contexto $w_{t+j}$ a partir da palavra central $w_t$.\n",
    "- **CBOW:** prediz a central $w_t$ a partir do **bag** dos contextos $\\{w_{t\\pm j}\\}$.\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/skip-gram-cbow.png\" width=\"55%\">\n",
    "\n",
    "### Probabilidade via softmax (modelo \"naïve softmax\")\n",
    "Para central $c$ e contexto $o$, com **dois** vetores por palavra (centro $\\mathbf{v}_w$, contexto $\\mathbf{u}_w$):\n",
    "$$\n",
    "P(o\\mid c)=\\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{w\\in V}\\exp(\\mathbf{u}_w^\\top \\mathbf{v}_c)}\n",
    "$$\n",
    "\n",
    "Para central $c$ e contexto $o$, com dois vetores por palavra (centro $\\mathbf{v}_w$, contexto $\\mathbf{u}_w$):\n",
    "$$\n",
    "P(o\\mid c)=\\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{w\\in V}\\exp(\\mathbf{u}_w^\\top \\mathbf{v}_c)}\n",
    "$$\n",
    "\n",
    "**Passo a passo da intuição:**\n",
    "\n",
    "1. **Produto interno ($\\mathbf{u}_o^\\top \\mathbf{v}_c$):**  \n",
    "   Mede a *similaridade* entre os vetores da palavra central $c$ e da candidata $o$.  \n",
    "   - Alto $\\rightarrow$ palavras aparecem juntas com frequência.  \n",
    "   - Baixo $\\rightarrow$ palavras raramente aparecem juntas.\n",
    "\n",
    "2. **Exponenciação ($\\exp(\\cdot)$):**  \n",
    "   - Garante valores positivos.  \n",
    "   - Amplifica diferenças: pequenas vantagens viram grandes diferenças de peso.\n",
    "\n",
    "3. **Normalização (softmax):**  \n",
    "   - Divide pelo somatório de todos os candidatos do vocabulário.  \n",
    "   - Cria uma **distribuição de probabilidade** sobre todas as palavras.  \n",
    "\n",
    "---\n",
    "\n",
    "**Intuição global:**  \n",
    "O modelo responde à pergunta:  \n",
    "> *“Dada a palavra central $c$, qual a probabilidade de $o$ estar em seu contexto?”*  \n",
    "\n",
    "Se a compatibilidade (produto interno) entre $c$ e $o$ é alta, a probabilidade cresce.  \n",
    "Se é baixa, a probabilidade cai.  \n",
    "\n",
    "---\n",
    "\n",
    "**Analogia:**  \n",
    "Cada palavra tem **dois papéis**: como **ator central** ($\\mathbf{v}_w$) e como **ator de contexto** ($\\mathbf{u}_w$).  \n",
    "Se a química entre os dois papéis é alta, o modelo acredita ser **muito provável** vê-los juntos na mesma cena (mesmo contexto).\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo (Skip-gram):**\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^T \\sum_{-m\\le j\\le m,\\ j\\ne 0}\\ \\log P(w_{t+j}\\mid w_t)\n",
    "$$\n",
    "\n",
    "O softmax requer normalização sobre $V$ → caro para vocabulários grandes.  \n",
    "Duas soluções: **Negative Sampling** e **Hierarchical Softmax**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b6bc5",
   "metadata": {},
   "source": [
    "### Derivações (naïve softmax, Skip-gram)\n",
    "\n",
    "Para um par $(c,o)$ e predição $\\hat{\\mathbf{y}}=\\text{softmax}(U\\mathbf{v}_c)$, com rótulo one-hot $\\mathbf{y}$ s.t. $y_o=1$:\n",
    "\n",
    "**Perda por par:**\n",
    "$$\n",
    "J_{\\text{pair}} = -\\log P(o\\mid c)\n",
    "$$\n",
    "\n",
    "**Gradientes:**\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{pair}}}{\\partial \\mathbf{v}_c} = U^\\top(\\hat{\\mathbf{y}}-\\mathbf{y}),\n",
    "\\qquad\n",
    "\\frac{\\partial J_{\\text{pair}}}{\\partial \\mathbf{u}_w} = (\\hat{y}_w - y_w)\\,\\mathbf{v}_c,\\ \\forall w\\in V\n",
    "$$\n",
    "\n",
    "Intuição: $(\\hat{\\mathbf{y}}-\\mathbf{y})$ é o **erro**; atualizamos $\\mathbf{v}_c$ e as $\\mathbf{u}_w$ proporcionais a esse erro.  \n",
    "Complexidade ainda é $O(|V|)$ por atualização → motivação para **Negative Sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436bdb7",
   "metadata": {},
   "source": [
    "### Negative Sampling (Skip-gram)\n",
    "\n",
    "**Ideia:** treinar classificadores binários que **distingam** um par real $(c,o)$ de $k$ pares negativos $(c,w_i)$ amostrados de uma distribuição de ruído $P_n$.\n",
    "\n",
    "**Objetivo por par $(c,o)$ e amostras $\\{w_i\\}_{i=1}^k$:**\n",
    "$$\n",
    "J_{\\text{NS}} = -\\log \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)\\;-\\; \\sum_{i=1}^k \\log \\sigma\\!\\big(-\\mathbf{u}_{w_i}^\\top \\mathbf{v}_c\\big),\n",
    "$$\n",
    "onde $\\sigma(x)=\\tfrac{1}{1+e^{-x}}$.\n",
    "\n",
    "**Gradientes:**\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{NS}}}{\\partial \\mathbf{v}_c} = \\big(\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)-1\\big)\\mathbf{u}_o\n",
    "\\;+\\; \\sum_{i=1}^k \\sigma(\\mathbf{u}_{w_i}^\\top \\mathbf{v}_c)\\,\\mathbf{u}_{w_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{NS}}}{\\partial \\mathbf{u}_o} = \\big(\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)-1\\big)\\mathbf{v}_c,\n",
    "\\qquad\n",
    "\\frac{\\partial J_{\\text{NS}}}{\\partial \\mathbf{u}_{w_i}} = \\sigma(\\mathbf{u}_{w_i}^\\top \\mathbf{v}_c)\\,\\mathbf{v}_c\n",
    "$$\n",
    "\n",
    "**Prática:** amostrar negativos com $P_n(w)\\propto U(w)^{3/4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe1da87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"o brasil gosta de futebol\",\n",
    "    \"o rio tem praia bonita\",\n",
    "    \"sao paulo tem universidade e hospital\",\n",
    "    \"a amazonia tem selva e rios\",\n",
    "    \"o brasil produz cafe\",\n",
    "    \"o rio de janeiro tem futebol e praia\",\n",
    "    \"sao paulo tem hospital famoso\",\n",
    "    \"a universidade pesquisa inteligencia artificial\",\n",
    "    \"cafe do brasil é famoso\",\n",
    "    \"a selva da amazonia é densa\",\n",
    "    \"o hospital do rio é universitario\",\n",
    "\n",
    "    \"o cruzeiro é o melhor time do brasil\",\n",
    "    \"o time de futebol joga no estadio\",\n",
    "    \"a torcida canta no estadio\",\n",
    "    \"o jogador marcou um gol\",\n",
    "    \"o tecnico escala o time titular\",\n",
    "    \"o campeonato brasileiro tem rodada no domingo\",\n",
    "    \"o arbitro apita o jogo de futebol\",\n",
    "    \"o atacante fez dois gols no classico\",\n",
    "    \"o goleiro defendeu o penalti\",\n",
    "    \"o time treinou no campo de grama\",\n",
    "    \"o maracana fica no rio de janeiro\",\n",
    "    \"o morumbi fica em sao paulo\",\n",
    "    \"o estadio recebe finais importantes\",\n",
    "    \"o time carioca venceu o jogo\",\n",
    "    \"o time paulista empatou fora de casa\",\n",
    "    \"a base revela novos jogadores\",\n",
    "    \"o lateral cruza a bola para a area\",\n",
    "    \"o meio campista organiza o ataque\",\n",
    "    \"o zagueiro corta o cruzamento\",\n",
    "    \"o treinador analisa estatisticas do jogo\",\n",
    "\n",
    "    \"a praia de copacabana tem areia branca\",\n",
    "    \"ipanema tem mar e sol\",\n",
    "    \"o turista caminha no calcadao de copacabana\",\n",
    "    \"o surfista pega onda na praia\",\n",
    "    \"banhistas lotam a praia no verao\",\n",
    "    \"o quiosque vende agua de coco\",\n",
    "    \"o rio tem orla movimentada\",\n",
    "    \"a brisa do mar refresca a tarde\",\n",
    "    \"a praia tem guarda sol e cadeira\",\n",
    "    \"o por do sol em ipanema é bonito\",\n",
    "    \"o passeio de bicicleta na orla é popular\",\n",
    "\n",
    "    \"a universidade tem campus grande\",\n",
    "    \"o campus tem biblioteca e laboratorio\",\n",
    "    \"o professor orienta estudante de mestrado\",\n",
    "    \"a pesquisa em dados utiliza python\",\n",
    "    \"o laboratorio de ia treina redes neurais\",\n",
    "    \"a pos graduacao publica artigos cientificos\",\n",
    "    \"o grupo de pesquisa organiza seminario semanal\",\n",
    "    \"a universidade federal tem curso de computacao\",\n",
    "    \"o aluno estuda no laboratorio de informatica\",\n",
    "    \"a disciplina ensina machine learning\",\n",
    "    \"o dataset tem noticias do brasil\",\n",
    "    \"a avaliacao usa acuracia e f1\",\n",
    "    \"o modelo aprende representacoes distribuídas\",\n",
    "    \"o repositorio guarda codigo e dados\",\n",
    "    \"o professor apresenta resultados no congresso\",\n",
    "    \"o curso tem prova e projeto final\",\n",
    "    \"a biblioteca empresta livros e revistas\",\n",
    "    \"a universidade tem hospital universitario\",\n",
    "    \"a pesquisa em linguagem natural usa word2vec\",\n",
    "    \"o laboratorio de visao computacional anota imagens\",\n",
    "    \"a turma pratica classificacao de textos\",\n",
    "    \"o servidor treina o modelo com gpu\",\n",
    "    \"a extensao conecta universidade e comunidade\",\n",
    "    \"o estagio aproxima alunos do mercado\",\n",
    "\n",
    "    \"o hospital universitario atende pacientes\",\n",
    "    \"o medico realiza cirurgia no centro cirurgico\",\n",
    "    \"a enfermeira aplica vacina na sala de atendimento\",\n",
    "    \"o paciente marcou consulta com o especialista\",\n",
    "    \"a emergencia recebe ambulancia a noite\",\n",
    "    \"o laboratorio realiza exame de sangue\",\n",
    "    \"o hospital tem leitos e uti\",\n",
    "    \"a saude publica contrata medicos\",\n",
    "    \"o medico receita medicamento para o paciente\",\n",
    "    \"o hospital de sao paulo e referencia\",\n",
    "    \"o pronto socorro fica lotado no feriado\",\n",
    "    \"a vacina previne doenca infecciosa\",\n",
    "    \"a farmacia do hospital entrega remedio\",\n",
    "    \"a triagem organiza a fila de atendimento\",\n",
    "    \"o prontuario registra historico do paciente\",\n",
    "\n",
    "    \"a amazonia abriga grande biodiversidade\",\n",
    "    \"o rio amazonas corta a floresta\",\n",
    "    \"o rio negro encontra o rio solimoes\",\n",
    "    \"a floresta tem arvores altas e lianas\",\n",
    "    \"a chuva e intensa na amazonia\",\n",
    "    \"comunidades indigenas vivem na regiao\",\n",
    "    \"o pesquisador estuda fauna e flora\",\n",
    "    \"o boto cor de rosa nada no rio\",\n",
    "    \"o peixe e alimento importante na regiao\",\n",
    "    \"o parque nacional protege a mata\",\n",
    "    \"o desmatamento ameaca a floresta\",\n",
    "    \"o ibama fiscaliza a regiao amazonica\",\n",
    "    \"o clima e umido e quente na floresta\",\n",
    "    \"barcos transportam pessoas pelos rios\",\n",
    "    \"manaus tem porto no rio negro\",\n",
    "    \"belem fica proxima da foz do amazonas\",\n",
    "\n",
    "    \"o cafe de minas gerais tem qualidade\",\n",
    "    \"a fazenda colhe graos maduros de cafe\",\n",
    "    \"a torra define o sabor do cafe\",\n",
    "    \"o barista prepara espresso na cafeteria\",\n",
    "    \"a xicara de cafe acompanha o pao de queijo\",\n",
    "    \"o brasil exporta cafe para a europa\",\n",
    "    \"o cafe especial recebe pontuacao alta\",\n",
    "    \"sao paulo tem muitas cafeterias\",\n",
    "    \"o produtor investe em irrigacao por gotejamento\",\n",
    "    \"a cooperativa vende graos torrados\",\n",
    "    \"o cafe arabica cresce em altitude\",\n",
    "    \"o cafe robusta cresce no espirito santo\",\n",
    "\n",
    "    \"brasilia e a capital do brasil\",\n",
    "    \"belo horizonte tem feira de artesanato\",\n",
    "    \"curitiba tem parque bonito e organizado\",\n",
    "    \"salvador tem carnaval famoso\",\n",
    "    \"fortaleza tem praia do futuro movimentada\",\n",
    "    \"recife tem o porto digital de tecnologia\",\n",
    "    \"porto alegre tem inverno frio\",\n",
    "    \"florianopolis tem muitas praias e trilhas\",\n",
    "    \"natal tem duna e passeio de buggy\",\n",
    "    \"gramado recebe muitos turistas no inverno\",\n",
    "\n",
    "    \"o estadio maracana recebe finais e classicos de futebol\",\n",
    "    \"o morumbi recebe shows e jogos de futebol\",\n",
    "    \"o campus da universidade tem hospital universitario\",\n",
    "    \"o hospital universitario participa de pesquisa clinica\",\n",
    "    \"a universidade organiza torneio de futebol entre cursos\",\n",
    "    \"a praia de copacabana recebe turistas e atletas\",\n",
    "    \"o laboratorio de dados analisa estatisticas do campeonato\",\n",
    "    \"o time treina na academia do clube\",\n",
    "    \"a cafeteria do campus serve cafe especial\",\n",
    "    \"o congresso de ia ocorre em sao paulo\",\n",
    "    \"o museu do futebol fica no estadio do pacaembu\",\n",
    "    \"o centro de inovacao conecta startups e universidade\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f43155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: 373 Pares: 2546\n",
      "Época 1: loss=4.0524\n",
      "Época 2: loss=3.6783\n",
      "Época 3: loss=3.2893\n",
      "Época 4: loss=2.9787\n",
      "Época 5: loss=2.7250\n",
      "Época 6: loss=2.4952\n",
      "Época 7: loss=2.3210\n",
      "Época 8: loss=2.1815\n",
      "Mais similares a 'brasil': [('cafe', 0.7007219158266396), ('produz', 0.6932317867634673), ('do', 0.6689019618394818), ('exporta', 0.6194163760682415), ('noticias', 0.578653985473427)]\n",
      "Mais similares a 'casa': [('fora', 0.607225408197346), ('cursos', 0.5778117056747542), ('entre', 0.5147343566593109), ('de', 0.4388081772317695), ('clinica', 0.4321712309960902)]\n"
     ]
    }
   ],
   "source": [
    "# Mini Skip-gram com Negative Sampling (NumPy) — treino toy para entender o mecanismo\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# --- 1) Corpus e preparação ---\n",
    "sentences = [s.split() for s in corpus]\n",
    "window = 2\n",
    "tokens = [w for s in sentences for w in s]\n",
    "vocab = sorted(set(tokens))\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for w,i in word2id.items()}\n",
    "ids = [word2id[w] for w in tokens]\n",
    "\n",
    "# Pares (central, contexto)\n",
    "pairs = []\n",
    "for s in sentences:\n",
    "    ids_s = [word2id[w] for w in s]\n",
    "    for t, c in enumerate(ids_s):\n",
    "        L = max(0, t-window); R = min(len(ids_s), t+window+1)\n",
    "        for j in range(L, R):\n",
    "            if j==t: continue\n",
    "            pairs.append((c, ids_s[j]))\n",
    "\n",
    "V = len(vocab)\n",
    "print(\"Vocabulário:\", V, \"Pares:\", len(pairs))\n",
    "\n",
    "# Frequências para negativos (unigrama^0.75)\n",
    "freq = Counter(ids)\n",
    "unigram = np.array([freq[i] for i in range(V)], dtype=np.float64)\n",
    "noise_dist = unigram**0.75\n",
    "noise_dist /= noise_dist.sum()\n",
    "\n",
    "def sample_negatives(k):\n",
    "    return rng.choice(V, size=k, replace=True, p=noise_dist)\n",
    "\n",
    "# --- 2) Parâmetros ---\n",
    "d = 50\n",
    "lr = 0.05\n",
    "k = 5  # negativos\n",
    "# Vetores: centro (V) e contexto (U)\n",
    "V_c = (rng.standard_normal((V, d)) / np.sqrt(d))\n",
    "U_o = (rng.standard_normal((V, d)) / np.sqrt(d))\n",
    "\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "\n",
    "# --- 3) Treino ---\n",
    "def train(epochs=5):\n",
    "    global V_c, U_o\n",
    "    for ep in range(epochs):\n",
    "        rng.shuffle(pairs)\n",
    "        loss = 0.0\n",
    "        for c,o in pairs:\n",
    "            vc = V_c[c]          # (d,)\n",
    "            uo = U_o[o]          # (d,)\n",
    "            negs = sample_negatives(k)\n",
    "            # evitar amostrar o positivo como negativo (opcional)\n",
    "            negs = np.array([w for w in negs if w!=o])\n",
    "            if len(negs)<k: \n",
    "                extra = sample_negatives(k-len(negs))\n",
    "                negs = np.concatenate([negs, extra])\n",
    "\n",
    "            # positivo\n",
    "            score_pos = uo @ vc\n",
    "            sig_pos = sigmoid(score_pos)\n",
    "            loss += -np.log(sig_pos + 1e-10)\n",
    "\n",
    "            # negativos\n",
    "            u_negs = U_o[negs]            # (k,d)\n",
    "            scores_neg = u_negs @ vc      # (k,)\n",
    "            sig_neg = sigmoid(-scores_neg)\n",
    "            loss += -np.sum(np.log(sig_neg + 1e-10))\n",
    "\n",
    "            # grad vc\n",
    "            grad_v = (sig_pos - 1.0)*uo + np.sum(sigmoid(scores_neg)[:,None]*u_negs, axis=0)\n",
    "            # grad uo e u_negs\n",
    "            grad_uo = (sig_pos - 1.0)*vc\n",
    "            grad_unegs = (sigmoid(scores_neg)[:,None])*vc[None,:]\n",
    "\n",
    "            # atualizações\n",
    "            V_c[c] -= lr * grad_v\n",
    "            U_o[o] -= lr * grad_uo\n",
    "            U_o[negs] -= lr * grad_unegs\n",
    "\n",
    "        print(f\"Época {ep+1}: loss={loss/len(pairs):.4f}\")\n",
    "\n",
    "train(epochs=8)\n",
    "\n",
    "# --- 4) Similaridades\n",
    "def most_similar(word, topn=5):\n",
    "    wid = word2id[word]\n",
    "    # vetor final: média de V e U (prática comum)\n",
    "    W = (V_c + U_o)/2\n",
    "    w = W[wid]\n",
    "    sims = W @ w / (np.linalg.norm(W, axis=1)*np.linalg.norm(w)+1e-9)\n",
    "    best = np.argsort(-sims)\n",
    "    return [(id2word[i], float(sims[i])) for i in best if i!=wid][:topn]\n",
    "\n",
    "print(\"Mais similares a 'brasil':\", most_similar(\"brasil\"))\n",
    "print(\"Mais similares a 'casa':\", most_similar(\"casa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c6ec3",
   "metadata": {},
   "source": [
    "### CBOW (para contraste)\n",
    "\n",
    "No **CBOW**, agregamos os vetores de contexto (p.ex., soma ou média) para prever a central:\n",
    "$$\n",
    "\\mathbf{h} = \\sum_{j\\in \\mathcal{C}(t)} \\mathbf{u}_{w_{t+j}},\n",
    "\\qquad\n",
    "P(w_t\\mid \\mathcal{C}(t)) = \\text{softmax}(V\\,\\mathbf{h})\n",
    "$$\n",
    "Os gradientes seguem de forma análoga ao caso Skip-gram, com $\\mathbf{h}$ no lugar de $\\mathbf{v}_c$.\n",
    "\n",
    "**Observações práticas**:\n",
    "- **Skip-gram** tende a funcionar melhor para palavras raras (mais pares por central).\n",
    "- **CBOW** é mais rápido e liso, pois agrega contexto (menos ruído).\n",
    "- Janela pequena → mais **sintaxe**; janela grande → mais **semântica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1302d",
   "metadata": {},
   "source": [
    "## 3) GloVe — Global Vectors (Pennington, Socher, Manning, 2014)\n",
    "\n",
    "Constrói matriz de **coocorrência** $X \\in \\mathbb{R}^{|V|\\times |V|}$, onde $X_{ij}$ é a contagem (ponderada por distância) de $j$ no contexto de $i$.\n",
    "\n",
    "**Objetivo:**\n",
    "$$\n",
    "J = \\sum_{i,j} f(X_{ij})\\left( \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "$$\n",
    "com\n",
    "$$\n",
    "f(x)=\\begin{cases}\n",
    "(x/x_{\\max})^\\alpha, & \\text{se } x < x_{\\max}\\\\\n",
    "1, & \\text{caso contrário}\n",
    "\\end{cases}\n",
    "\\quad\\text{(tipicamente } x_{\\max}=100,\\ \\alpha=\\tfrac{3}{4}\\text{)}\n",
    "$$\n",
    "\n",
    "**Gradientes:**\n",
    "Defina o erro\n",
    "$\n",
    "E_{ij} = \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij}.\n",
    "$\n",
    "Então\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_i} = \\sum_j f(X_{ij})\\,E_{ij}\\,\\tilde{\\mathbf{w}}_j,\\quad\n",
    "\\frac{\\partial J}{\\partial \\tilde{\\mathbf{w}}_j} = \\sum_i f(X_{ij})\\,E_{ij}\\,\\mathbf{w}_i\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_i} = \\sum_j f(X_{ij})\\,E_{ij},\\quad\n",
    "\\frac{\\partial J}{\\partial \\tilde{b}_j} = \\sum_i f(X_{ij})\\,E_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41799259",
   "metadata": {},
   "source": [
    "### Intuição da função objetivo do GloVe\n",
    "\n",
    "**Objetivo:**\n",
    "$$\n",
    "J = \\sum_{i,j} f(X_{ij})\\left( \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "- $X_{ij}$ = número de vezes que a palavra $j$ aparece no contexto de $i$ (coocorrência).  \n",
    "- $\\mathbf{w}_i, \\tilde{\\mathbf{w}}_j$ = vetores da palavra $i$ e do contexto $j$.  \n",
    "- $b_i, \\tilde{b}_j$ = vieses (ajustes de escala).  \n",
    "- $f(X_{ij})$ = função de ponderação que controla o peso de cada par $(i,j)$.  \n",
    "\n",
    "---\n",
    "\n",
    "#### O que essa equação está dizendo?\n",
    "\n",
    "1. **Queremos aproximar:**\n",
    "$$\n",
    "\\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j \\;\\;\\approx\\;\\; \\log X_{ij}\n",
    "$$\n",
    "\n",
    "Ou seja:  \n",
    "o **produto interno dos vetores** (mais biases) deve se alinhar com o **log da coocorrência**.  \n",
    "\n",
    "- Se duas palavras aparecem muito juntas → $\\log X_{ij}$ alto → vetor $\\mathbf{w}_i$ precisa ficar próximo de $\\tilde{\\mathbf{w}}_j$.  \n",
    "- Se raramente aparecem juntas → $\\log X_{ij}$ baixo → o produto interno deve ser pequeno ou negativo.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Por que usar $\\log X_{ij}$ em vez de $X_{ij}$?**  \n",
    "- As contagens brutas crescem muito rápido.  \n",
    "- O log “comprime” grandes diferenças, permitindo comparar palavras comuns e raras de forma mais equilibrada.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Por que a função de ponderação $f(x)$?**  \n",
    "$$\n",
    "f(x)=\\begin{cases}\n",
    "(x/x_{\\max})^\\alpha & \\text{se } x < x_{\\max} \\\\\n",
    "1 & \\text{caso contrário}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Pares muito frequentes (ex.: “de”, “o”) **não devem dominar** o treinamento → $f(x)$ os limita.  \n",
    "- Pares muito raros (coocorrência próxima de 0) **não são confiáveis** → $f(x)$ os enfraquece.  \n",
    "- Tipicamente: $x_{\\max}=100$, $\\alpha=3/4$.\n",
    "\n",
    "---\n",
    "\n",
    "#### E os gradientes?\n",
    "\n",
    "Definindo o erro:\n",
    "$$\n",
    "E_{ij} = \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij}\n",
    "$$\n",
    "\n",
    "- **Para $\\mathbf{w}_i$:**  \n",
    "  move o vetor da palavra $i$ para mais perto ou mais longe de $\\tilde{\\mathbf{w}}_j$, dependendo do erro.  \n",
    "- **Para $\\tilde{\\mathbf{w}}_j$:**  \n",
    "  ajuste simétrico para o vetor de contexto.  \n",
    "- **Para os bias $b_i, \\tilde{b}_j$:**  \n",
    "  pequenos deslocamentos que ajudam a calibrar os valores sem precisar alterar todo o vetor.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuição global do GloVe\n",
    "\n",
    "- Cada par de palavras é uma **equação de regressão**:  \n",
    "  “o quanto $i$ e $j$ coocorrem deve ser igual ao produto dos seus vetores”.  \n",
    "- O treinamento ajusta todos os vetores para que essas equações sejam satisfeitas “o melhor possível”.  \n",
    "- O resultado são embeddings que preservam **relações semânticas lineares** (ex.: rei − homem + mulher ≈ rainha).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ccc6143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GloVe] it 01 | J/|X| = 0.0076\n",
      "[GloVe] it 02 | J/|X| = 0.0022\n",
      "[GloVe] it 03 | J/|X| = 0.0007\n",
      "[GloVe] it 04 | J/|X| = 0.0003\n",
      "[GloVe] it 05 | J/|X| = 0.0002\n",
      "[GloVe] it 10 | J/|X| = 0.0000\n",
      "[GloVe] it 20 | J/|X| = 0.0000\n",
      "[GloVe] it 30 | J/|X| = 0.0000\n",
      "[GloVe] it 40 | J/|X| = 0.0000\n",
      "[GloVe] it 50 | J/|X| = 0.0000\n",
      "[GloVe] it 60 | J/|X| = 0.0000\n",
      "\n",
      "[GloVe] vizinhos de 'rio': [('o', 0.5710707482288602), ('no', 0.5353837859189712), ('brasil', 0.35991923669257053), ('classico', 0.3367246458955786), ('jogo', 0.3336451727563668), ('de', 0.29578503725832717), ('estadio', 0.272847955258618)]\n",
      "[GloVe] vizinhos de 'futebol': [('de', 0.4755553378757443), ('laboratorio', 0.40369747393709793), ('entre', 0.34434275379207097), ('sala', 0.33290145334383386), ('doenca', 0.330268647918877), ('atendimento', 0.31925752517141215), ('com', 0.31764987376007975)]\n",
      "[GloVe] vizinhos de 'universidade': [('a', 0.6196808997322713), ('e', 0.4728297843586192), ('tem', 0.455252503383725), ('praia', 0.4310534321557508), ('finais', 0.3989097758216234), ('queijo', 0.39675900973428674), ('floresta', 0.37066421029534524)]\n"
     ]
    }
   ],
   "source": [
    "# ===== Mini-GloVe (AdaGrad) usando o mesmo 'sentences' e 'word2id' =====\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# 1) Matriz de coocorrência esparsa X com janela simétrica ponderada (1/dist)\n",
    "window = 5  # janela um pouco maior costuma ajudar semântica\n",
    "X = defaultdict(float)\n",
    "for s in sentences:\n",
    "    ids_s = [word2id[w] for w in s if w in word2id]\n",
    "    for t, wi in enumerate(ids_s):\n",
    "        L = max(0, t-window); R = min(len(ids_s), t+window+1)\n",
    "        for j in range(L, R):\n",
    "            if j == t: \n",
    "                continue\n",
    "            wj = ids_s[j]\n",
    "            dist = abs(j - t)\n",
    "            X[(wi, wj)] += 1.0 / dist\n",
    "\n",
    "V = len(word2id)\n",
    "d = 100                 # um pouco maior que no toy anterior\n",
    "W  = rng.normal(scale=0.1, size=(V, d))\n",
    "Wt = rng.normal(scale=0.1, size=(V, d))\n",
    "b  = np.zeros(V)\n",
    "bt = np.zeros(V)\n",
    "\n",
    "xmax, alpha = 100.0, 0.75\n",
    "def f(x):\n",
    "    return (x/xmax)**alpha if x < xmax else 1.0\n",
    "\n",
    "# AdaGrad\n",
    "eps = 1e-8\n",
    "gW  = np.zeros_like(W)\n",
    "gWt = np.zeros_like(Wt)\n",
    "gb  = np.zeros_like(b)\n",
    "gbt = np.zeros_like(bt)\n",
    "\n",
    "keys = list(X.keys())\n",
    "\n",
    "def glove_train(iters=70, lr=0.05):\n",
    "    for it in range(iters):\n",
    "        rng.shuffle(keys)\n",
    "        J = 0.0\n",
    "        for (i, j) in keys:\n",
    "            xij = X[(i, j)]\n",
    "            wij = W[i]; wjt = Wt[j]\n",
    "            err = wij @ wjt + b[i] + bt[j] - np.log(xij)\n",
    "            fij = f(xij)\n",
    "            J += 0.5 * fij * (err**2)\n",
    "\n",
    "            # gradientes\n",
    "            grad_wi = fij * err * wjt\n",
    "            grad_wj = fij * err * wij\n",
    "            grad_bi = fij * err\n",
    "            grad_bj = fij * err\n",
    "\n",
    "            # AdaGrad updates\n",
    "            gW[i]  += grad_wi**2\n",
    "            gWt[j] += grad_wj**2\n",
    "            gb[i]  += grad_bi**2\n",
    "            gbt[j] += grad_bj**2\n",
    "\n",
    "            W[i]  -= lr * grad_wi / (np.sqrt(gW[i])  + eps)\n",
    "            Wt[j] -= lr * grad_wj / (np.sqrt(gWt[j]) + eps)\n",
    "            b[i]  -= lr * grad_bi / (np.sqrt(gb[i])  + eps)\n",
    "            bt[j] -= lr * grad_bj / (np.sqrt(gbt[j]) + eps)\n",
    "\n",
    "        if (it+1) % 10 == 0 or it < 5:\n",
    "            print(f\"[GloVe] it {it+1:02d} | J/|X| = {J/len(keys):.4f}\")\n",
    "\n",
    "glove_train(iters=60, lr=0.05)\n",
    "\n",
    "# Vetores finais práticos: soma\n",
    "E_glove = W + Wt\n",
    "\n",
    "def most_similar_glove(word, topn=10):\n",
    "    if word not in word2id:\n",
    "        raise KeyError(f\"'{word}' não está no vocabulário do GloVe.\")\n",
    "    wid = word2id[word]\n",
    "    w = E_glove[wid]\n",
    "    norms = np.linalg.norm(E_glove, axis=1) * (np.linalg.norm(w) + 1e-9)\n",
    "    sims = (E_glove @ w) / (norms + 1e-9)\n",
    "    order = np.argsort(-sims)\n",
    "    return [(id2word[i], float(sims[i])) for i in order if i != wid][:topn]\n",
    "\n",
    "print(\"\\n[GloVe] vizinhos de 'rio':\", most_similar_glove(\"rio\")[:7])\n",
    "print(\"[GloVe] vizinhos de 'futebol':\", most_similar_glove(\"futebol\")[:7])\n",
    "print(\"[GloVe] vizinhos de 'universidade':\", most_similar_glove(\"universidade\")[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b552bb8",
   "metadata": {},
   "source": [
    "## 4) FastText (Bojanowski et al., 2017)\n",
    "\n",
    "O **FastText** foi proposto pelo Facebook em 2017 como uma extensão natural do Word2Vec.  \n",
    "A grande diferença é que **cada palavra não é tratada como indivisível**, mas como uma soma de vetores de **sub-palavras** (n-gramas de caracteres).\n",
    "\n",
    "---\n",
    "\n",
    "### Ideia central\n",
    "- Para cada palavra $w$, consideramos todos os seus **n-gramas de caracteres** (tipicamente $n \\in [3,6]$).  \n",
    "- Adicionam-se símbolos de início/fim (`<`, `>`) para capturar prefixos e sufixos.  \n",
    "\n",
    "**Exemplo (n=3):**\n",
    "- Palavra: `\"casa\"`  \n",
    "- n-gramas: `<ca`, `cas`, `asa`, `sa>`  \n",
    "\n",
    "Cada n-grama tem um vetor $\\mathbf{z}_g$.  \n",
    "O vetor final da palavra é a soma (ou média) desses vetores:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}(w)=\\sum_{g\\in G_w} \\mathbf{z}_g\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Como o treinamento funciona?\n",
    "O modelo FastText treina quase da mesma forma que o Skip-gram com *negative sampling*:  \n",
    "\n",
    "- Antes: no Skip-gram padrão, usamos o vetor da palavra central $\\mathbf{v}_c$.  \n",
    "- Agora: substituímos $\\mathbf{v}_c$ por $\\mathbf{z}(w_c)$, que é construído a partir de n-gramas.  \n",
    "\n",
    "Assim, **as mesmas equações de probabilidade e gradientes** do Word2Vec continuam válidas, mas agora a informação é **compartilhada entre palavras com pedaços semelhantes**.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que isso é importante?\n",
    "\n",
    "1. **Palavras raras**  \n",
    "   - Word2Vec e GloVe precisam ver muitas ocorrências para aprender um bom vetor.  \n",
    "   - FastText generaliza porque compartilha n-gramas entre palavras.  \n",
    "   - Ex.: \"corrida\", \"correr\", \"corredor\" → compartilham `corr`.\n",
    "\n",
    "2. **Palavras fora do vocabulário (OOV)**  \n",
    "   - Em Word2Vec/GloVe, se a palavra não está no vocabulário → não existe vetor.  \n",
    "   - Em FastText, ainda podemos gerar um embedding **somando os n-gramas** (mesmo que a palavra nunca tenha aparecido).  \n",
    "   - Ex.: \"cachorrinho\" → pode ser decomposta em sub-palavras conhecidas de \"cachorro\".\n",
    "\n",
    "3. **Morfologia rica (como no português)**  \n",
    "   - Idiomas flexivos têm muitas variações de uma mesma raiz (plural, feminino, conjugação verbal).  \n",
    "   - FastText é capaz de capturar essas regularidades sem precisar de milhões de ocorrências por forma.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparação rápida\n",
    "- **Word2Vec/GloVe:** vetor único por palavra (não entende morfologia).  \n",
    "- **FastText:** vetor = soma de sub-palavras → mais robusto para línguas com flexão.  \n",
    "- **Limitação:** continua sendo um modelo **estático** (não resolve polissemia como BERT/ELMo).\n",
    "\n",
    "---\n",
    "\n",
    "### Vantagens práticas\n",
    "- Funciona muito bem em **português** e outras línguas ricas em morfologia.  \n",
    "- Permite embeddings para **neologismos** e **palavras OOV**.  \n",
    "- Modelo rápido, aproveitando a mesma mecânica do Skip-gram com Negative Sampling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dada38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FastText] vizinhos de 'praia': [('praias', 0.6487528681755066), ('cafeteria', 0.5528571009635925), ('cafeterias', 0.526585578918457), ('amazonia', 0.4928141236305237), ('cafe', 0.49207791686058044), ('corta', 0.4876181483268738), ('estadio', 0.4657224118709564)]\n",
      "[FastText] vizinhos de 'hospital': [('grama', 0.5155500173568726), ('capital', 0.5016581416130066), ('apita', 0.47575175762176514), ('dados', 0.47549065947532654), ('treinador', 0.4560692608356476), ('cafe', 0.4556812644004822), ('gramado', 0.4540213346481323)]\n",
      "[FastText] norma do vetor OOV 'futebolzinho': 0.009463516063988209\n",
      "[FastText] vizinhos de 'futebol' pelo FastText: [('pacientes', 0.444522500038147), ('seminario', 0.4211379587650299), ('aplica', 0.4197556674480438), ('dados', 0.4185933768749237), ('comunidade', 0.39609429240226746), ('de', 0.39179563522338867), ('comunidades', 0.38463878631591797)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_model = FastText(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,   \n",
    "    sg=1       \n",
    ")\n",
    "\n",
    "def most_similar_fasttext(word, topn=10):\n",
    "    return ft_model.wv.most_similar(word, topn=topn)\n",
    "\n",
    "print(\"[FastText] vizinhos de 'praia':\", most_similar_fasttext(\"praia\")[:7])\n",
    "print(\"[FastText] vizinhos de 'hospital':\", most_similar_fasttext(\"hospital\")[:7])\n",
    "\n",
    "oov = \"futebolzinho\"\n",
    "vec_oov = ft_model.wv[oov]  \n",
    "print(f\"[FastText] norma do vetor OOV '{oov}':\", float(np.linalg.norm(vec_oov)))\n",
    "print(\"[FastText] vizinhos de 'futebol' pelo FastText:\", most_similar_fasttext(\"futebol\")[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614ba086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Comparação] W2V vizinhos de 'rio': [('negro', 0.7376405481555982), ('encontra', 0.711974304423906), ('porto', 0.6401831007401755), ('universitario', 0.5904406810028948), ('janeiro', 0.5602900814670095), ('nada', 0.46245816998878786), ('no', 0.442251491364128)]\n",
      "[Comparação] GloVe vizinhos de 'rio': [('o', 0.5710707482288602), ('no', 0.5353837859189712), ('brasil', 0.35991923669257053), ('classico', 0.3367246458955786), ('jogo', 0.3336451727563668), ('de', 0.29578503725832717), ('estadio', 0.272847955258618)]\n",
      "[Comparação] FastText vizinhos de 'rio': [('laboratorio', 0.5692906975746155), ('cafeterias', 0.5472309589385986), ('prontuario', 0.5312071442604065), ('frio', 0.5243507027626038), ('dados', 0.4971526563167572), ('rios', 0.48958051204681396), ('estadio', 0.48705339431762695)]\n"
     ]
    }
   ],
   "source": [
    "W_w2v = (V_c + U_o) / 2\n",
    "\n",
    "def vec_w2v(token):\n",
    "    if token not in word2id:\n",
    "        raise KeyError(f\"'{token}' não está no vocabulário do modelo W2V caseiro.\")\n",
    "    return W_w2v[word2id[token]]\n",
    "\n",
    "def vec_glove(token):\n",
    "    if token not in word2id:\n",
    "        raise KeyError(f\"'{token}' não está no vocabulário do GloVe.\")\n",
    "    return E_glove[word2id[token]]\n",
    "\n",
    "def vec_fasttext(token):\n",
    "    return ft_model.wv[token]  # lida com OOV via subword\n",
    "\n",
    "def most_similar_generic(model_name, token, topn=10):\n",
    "    if model_name == \"w2v\":\n",
    "        w = vec_w2v(token)\n",
    "        E = W_w2v\n",
    "    elif model_name == \"glove\":\n",
    "        w = vec_glove(token)\n",
    "        E = E_glove\n",
    "    elif model_name == \"fasttext\":\n",
    "        return ft_model.wv.most_similar(token, topn=topn)\n",
    "    else:\n",
    "        raise ValueError(\"model_name deve ser 'w2v', 'glove' ou 'fasttext'.\")\n",
    "    sims = E @ w / (np.linalg.norm(E, axis=1) * (np.linalg.norm(w) + 1e-9) + 1e-9)\n",
    "    order = np.argsort(-sims)\n",
    "    wid = word2id.get(token, None)\n",
    "    result = []\n",
    "    for i in order:\n",
    "        if wid is not None and i == wid:\n",
    "            continue\n",
    "        result.append((id2word[i], float(sims[i])))\n",
    "        if len(result) >= topn:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "print(\"[Comparação] W2V vizinhos de 'rio':\", most_similar_generic(\"w2v\",\"rio\")[:7])\n",
    "print(\"[Comparação] GloVe vizinhos de 'rio':\", most_similar_generic(\"glove\",\"rio\")[:7])\n",
    "print(\"[Comparação] FastText vizinhos de 'rio':\", most_similar_generic(\"fasttext\",\"rio\")[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e69cba",
   "metadata": {},
   "source": [
    "## 5) Avaliação de embeddings\n",
    "\n",
    "**Intrínseca**\n",
    "- **Similaridade**: correlação com anotações humanas (WordSim, RG65 etc.).\n",
    "- **Analogia**: avaliar se $\\mathbf{w}_{\\text{king}} - \\mathbf{w}_{\\text{man}} + \\mathbf{w}_{\\text{woman}} \\approx \\mathbf{w}_{\\text{queen}}$.\n",
    "- **Visualização**: TSNE/UMAP de vizinhanças semânticas.\n",
    "\n",
    "**Extrínseca**\n",
    "- Plug-and-play em tarefas (NER, POS, classificação) e medir F1/acc.\n",
    "\n",
    "> Em PT-BR, tentem analogias como “Brasil : Brasília :: França : ?”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0260ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Analogy W2V] 'rio' : 'maracana' :: 'sao paulo' : ? [('referencia', 0.5935763774089288), ('usa', 0.5147199488882565), ('e', 0.5080159046442877), ('em', 0.43705912410461695), ('trilhas', 0.4295243267130231)]\n",
      "[Analogy GloVe] 'rio' : 'maracana' :: 'sao paulo' : ? [('manaus', 0.5674081380262637), ('cor', 0.5178318935988463), ('ipanema', 0.39842362740131737), ('bonita', 0.3836451174195685), ('startups', 0.36872891980181605)]\n",
      "[Analogy FT] 'rio de janeiro' : 'maracana' :: 'sao paulo' : ? [('pao', 0.32736703753471375), ('ameaca', 0.30053210258483887), ('atendimento', 0.2784554362297058), ('learning', 0.27785807847976685), ('parque', 0.274782657623291)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def tokenize_phrase(text):\n",
    "    # separa por espaço, mantém tokens tal como no corpus (sem acento/caixa já condizentes)\n",
    "    return [t for t in re.split(r\"\\s+\", text.strip()) if t]\n",
    "\n",
    "def phrase_vec(model, phrase):\n",
    "    toks = tokenize_phrase(phrase)\n",
    "    vecs = []\n",
    "    for t in toks:\n",
    "        try:\n",
    "            if model == \"w2v\":\n",
    "                vecs.append(vec_w2v(t))\n",
    "            elif model == \"glove\":\n",
    "                vecs.append(vec_glove(t))\n",
    "            elif model == \"fasttext\":\n",
    "                vecs.append(vec_fasttext(t))\n",
    "            else:\n",
    "                raise ValueError\n",
    "        except KeyError:\n",
    "            # ignora tokens OOV no W2V/GloVe; FastText deve cobrir quase tudo\n",
    "            pass\n",
    "    if not vecs:\n",
    "        raise KeyError(f\"Nenhum token com vetor em '{phrase}'.\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def analogy(a, b, c, model=\"w2v\", topn=10):\n",
    "    # Resolve analogia: a : b :: c : ?\n",
    "    va = phrase_vec(model, a)\n",
    "    vb = phrase_vec(model, b)\n",
    "    vc = phrase_vec(model, c)\n",
    "    target = vb - va + vc\n",
    "\n",
    "    # matriz de referência\n",
    "    if model == \"w2v\":\n",
    "        E = W_w2v\n",
    "        idx2tok = id2word\n",
    "    elif model == \"glove\":\n",
    "        E = E_glove\n",
    "        idx2tok = id2word\n",
    "    elif model == \"fasttext\":\n",
    "        # Para FastText, criamos uma matriz E para termos do vocabulário conhecido\n",
    "        vocab_ft = list(ft_model.wv.key_to_index.keys())\n",
    "        E = np.vstack([ft_model.wv[w] for w in vocab_ft])\n",
    "        idx2tok = {i:w for i,w in enumerate(vocab_ft)}\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    sims = E @ target / (np.linalg.norm(E, axis=1) * (np.linalg.norm(target) + 1e-9) + 1e-9)\n",
    "    order = np.argsort(-sims)\n",
    "\n",
    "    # Remove termos presentes na entrada (quando possível mapear para índice)\n",
    "    exclude = set()\n",
    "    for phr in [a,b,c]:\n",
    "        for t in tokenize_phrase(phr):\n",
    "            # mapear para índice correspondente\n",
    "            if model in (\"w2v\",\"glove\"):\n",
    "                if t in word2id:\n",
    "                    exclude.add(word2id[t])\n",
    "            else:\n",
    "                # FastText: mapeamento por string\n",
    "                pass\n",
    "\n",
    "    ans = []\n",
    "    for i in order:\n",
    "        tok = idx2tok[i]\n",
    "        # excluir entradas\n",
    "        if model in (\"w2v\",\"glove\"):\n",
    "            if i in exclude:\n",
    "                continue\n",
    "        if tok in set(tokenize_phrase(a) + tokenize_phrase(b) + tokenize_phrase(c)):\n",
    "            continue\n",
    "        ans.append((tok, float(sims[i])))\n",
    "        if len(ans) >= topn:\n",
    "            break\n",
    "    return ans\n",
    "\n",
    "# Exemplos: cidades e estádios (tokens disponíveis no corpus)\n",
    "print(\"\\n[Analogy W2V] 'rio' : 'maracana' :: 'sao paulo' : ?\", analogy(\"rio\",\"maracana\",\"sao paulo\",\"w2v\",topn=5))\n",
    "print(\"[Analogy GloVe] 'rio' : 'maracana' :: 'sao paulo' : ?\", analogy(\"rio\",\"maracana\",\"sao paulo\",\"glove\",topn=5))\n",
    "print(\"[Analogy FT] 'rio de janeiro' : 'maracana' :: 'sao paulo' : ?\", analogy(\"rio de janeiro\",\"maracana\",\"sao paulo\",\"fasttext\",topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dbc93",
   "metadata": {},
   "source": [
    "## Leituras adicionais\n",
    "- Mikolov, T. et al. (2013). *Efficient Estimation of Word Representations in Vector Space.* arXiv.\n",
    "- Pennington, J.; Socher, R.; Manning, C. (2014). *GloVe: Global Vectors for Word Representation.* EMNLP.\n",
    "- Bojanowski, P. et al. (2017). *Enriching Word Vectors with Subword Information.* TACL.\n",
    "- Levy, O. et al. (2015). *Improving Distributional Similarity with Lessons Learned from Word Embeddings.*\n",
    "- Church, K.; Hanks, P. (1990). *Word Association Norms, Mutual Information, and Lexicography.* (PPMI clássica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f12cc",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781.\n",
    "- Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors for Word Representation. In *EMNLP 2014*, 1532–1543.\n",
    "- Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information. *TACL*, 5, 135–146.\n",
    "- Firth, J. R. (1957). *A Synopsis of Linguistic Theory 1930–1955*. Oxford.\n",
    "- Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. *TACL*, 3, 211–225."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923b495",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
