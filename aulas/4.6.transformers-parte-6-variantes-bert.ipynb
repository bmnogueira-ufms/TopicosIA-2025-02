{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01878e9f",
   "metadata": {
    "id": "01878e9f"
   },
   "source": [
    "# 6) Variantes do BERT\n",
    "\n",
    "Desde 2018, surgiram diversas versÃµes otimizadas do BERT com objetivos diferentes â€”  \n",
    "melhorar desempenho, reduzir custo ou mudar o prÃ©-treinamento.\n",
    "\n",
    "| Modelo | Principais caracterÃ­sticas | Ganhos |\n",
    "|---------|----------------------------|---------|\n",
    "| **BERT-base / BERT-large** | Modelo original (12 / 24 camadas, 110M / 340M parÃ¢metros). | Base de referÃªncia. |\n",
    "| **DistilBERT** | 40% menor, 60% mais rÃ¡pido, via *knowledge distillation*. | EficiÃªncia. |\n",
    "| **RoBERTa** | Remove NSP, usa *dynamic masking*, treina em muito mais dados. | +Robusto, melhor generalizaÃ§Ã£o. |\n",
    "| **ALBERT** | ParÃ¢metros compartilhados + fatorizaÃ§Ã£o de embeddings. | Reduz drasticamente o tamanho (de 110M â†’ 12M). |\n",
    "| **DeBERTa** | *Disentangled attention* + correÃ§Ã£o de posiÃ§Ã£o absoluta. | Melhor compreensÃ£o sintÃ¡tica e semÃ¢ntica. |\n",
    "| **BERTimbau** ğŸ‡§ğŸ‡· | BERT treinado em portuguÃªs (brWac + Wikipedia). | Melhor performance em PT-BR. |\n",
    "\n",
    "---\n",
    "\n",
    "## Escolha prÃ¡tica\n",
    "\n",
    "| SituaÃ§Ã£o | Modelo sugerido |\n",
    "|-----------|-----------------|\n",
    "| Poucos recursos de GPU | `distilbert-base-uncased` |\n",
    "| Tarefa em portuguÃªs | `neuralmind/bert-base-portuguese-cased` |\n",
    "| Dataset grande e precisÃ£o mÃ¡xima | `roberta-large` ou `deberta-v3-large` |\n",
    "| Deploy em mobile / produÃ§Ã£o leve | `tinybert` ou `albert-base-v2` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968605a9",
   "metadata": {
    "id": "968605a9"
   },
   "source": [
    "## 6.1 â€” Fine-tuning do BERTimbau (PT-BR) para ClassificaÃ§Ã£o de Texto\n",
    "\n",
    "O **BERTimbau** Ã© o BERT prÃ©-treinado em corpora de PortuguÃªs (Wikipedia + brWac).  \n",
    "Para tarefas de **classificaÃ§Ã£o** (sentimento, tÃ³picos, intenÃ§Ã£o, etc.), fazemos **fine-tuning** adicionando uma **camada linear** sobre o vetor de **[CLS]** (internamente, o `pooled_output`).\n",
    "\n",
    "**Pipeline**\n",
    "1. Carregar o **tokenizer** e o **modelo** `neuralmind/bert-base-portuguese-cased` (ou *uncased*).\n",
    "2. Preparar os dados (`text`, `label`).\n",
    "3. Tokenizar (`[CLS] ... [SEP]`), definir `max_length`.\n",
    "4. Treinar com **taxa de aprendizado pequena** (ex.: 2e-5), poucas Ã©pocas (2â€“4).\n",
    "5. Avaliar (accuracy/F1) e testar com frases novas.\n",
    "\n",
    "**Dicas prÃ¡ticas**\n",
    "- Use o modelo **cased** para preservar acentuaÃ§Ã£o e caixa em PT-BR.\n",
    "- Se a base for pequena, considere **congelar** as primeiras camadas do encoder para estabilizar.\n",
    "- Classes desbalanceadas? Use `class_weights` ou *weighted loss*.\n",
    "- MÃ©tricas: **accuracy** e **F1** (macro/weighted).\n",
    "\n",
    "**Entradas esperadas pelo cÃ³digo**\n",
    "- VocÃª pode:\n",
    "  - (A) informar um **dataset do Hugging Face** com colunas `text` e `label`, ou  \n",
    "  - (B) apontar para **CSVs** (`train.csv`, `val.csv`) com colunas `text,label`, ou  \n",
    "  - (C) usar um **mini-dataset didÃ¡tico** embutido (fallback) sÃ³ para demonstrar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bfa0c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "executionInfo": {
     "elapsed": 395897,
     "status": "error",
     "timestamp": 1761269104721,
     "user": {
      "displayName": "Bruno Magalhaes Nogueira",
      "userId": "18320277366917905276"
     },
     "user_tz": 240
    },
    "id": "f5bfa0c3",
    "outputId": "c1ddce92-e634-4997-85d8-dd08e4965c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1-828630596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# ------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2347\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4.3.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     raise ImportError(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Fine-tuning BERTimbau (PortuguÃªs) para ClassificaÃ§Ã£o de Texto\n",
    "# ============================================================\n",
    "import os, math, random, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (Colab) instalar dependÃªncias\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -U \"transformers>=4.39\" \"datasets>=2.14\" \"accelerate>=0.28\" \"evaluate>=0.4\" \"pandas>=1.5\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Imports principais\n",
    "# ------------------------------------------------------------\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "# Helper: TrainingArguments compatÃ­vel com a sua versÃ£o\n",
    "def build_training_arguments(**kwargs) -> TrainingArguments:\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    allowed = set(sig.parameters.keys()); allowed.discard(\"self\")\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n",
    "    dropped = [k for k in kwargs if k not in allowed]\n",
    "    if dropped:\n",
    "        print(\"[Aviso] parÃ¢metros ignorados nesta versÃ£o:\", dropped)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) FONTE DOS DADOS (escolha UMA)\n",
    "# ------------------------------------------------------------\n",
    "# (A) Dataset Hugging Face (deve ter colunas 'text' e 'label')\n",
    "HF_DATASET = None\n",
    "HF_CONFIG  = None  # ex.: \"default\" ou subtarefa (se houver)\n",
    "\n",
    "# (B) CSVs locais com colunas: text,label (rÃ³tulo pode ser string)\n",
    "CSV_TRAIN = None  # ex.: \"/content/train.csv\"\n",
    "CSV_VAL   = None  # ex.: \"/content/val.csv\"\n",
    "\n",
    "# (C) Fallback didÃ¡tico embutido (PT-BR)\n",
    "FALLBACK_PT = [\n",
    "    (\"o filme Ã© excelente, emocionante e muito bem dirigido.\", 1),\n",
    "    (\"pÃ©ssimo atendimento, nÃ£o volto mais.\", 0),\n",
    "    (\"a comida estava maravilhosa, sabores incrÃ­veis.\", 1),\n",
    "    (\"produto chegou quebrado e atrasado, experiÃªncia horrÃ­vel.\", 0),\n",
    "    (\"serviÃ§o rÃ¡pido e eficiente, gostei bastante.\", 1),\n",
    "    (\"interface confusa e cheia de bugs.\", 0),\n",
    "    (\"uma experiÃªncia fantÃ¡stica do comeÃ§o ao fim!\", 1),\n",
    "    (\"nÃ£o recomendo, custo-benefÃ­cio muito ruim.\", 0),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Carregar dados\n",
    "# ------------------------------------------------------------\n",
    "train_texts, train_labels = [], []\n",
    "val_texts,   val_labels   = [], []\n",
    "\n",
    "def load_from_hf(name, config=None):\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(name, config) if config else load_dataset(name)\n",
    "    # tenta achar colunas text/label comuns\n",
    "    # vocÃª pode adaptar aqui se seu dataset tiver outros nomes\n",
    "    def pick_cols(split):\n",
    "        cand_text = [c for c in [\"text\", \"sentence\", \"texto\", \"review\", \"content\"] if c in ds[split].column_names]\n",
    "        cand_label= [c for c in [\"label\", \"labels\", \"sentiment\", \"classe\"] if c in ds[split].column_names]\n",
    "        assert cand_text and cand_label, f\"NÃ£o encontrei colunas 'text' e 'label' no split {split}. Colunas: {ds[split].column_names}\"\n",
    "        return cand_text[0], cand_label[0]\n",
    "\n",
    "    tcol_tr, lcol_tr = pick_cols(\"train\")\n",
    "    tcol_va, lcol_va = pick_cols(\"validation\") if \"validation\" in ds else pick_cols(\"test\")\n",
    "\n",
    "    Xtr = ds[\"train\"][tcol_tr];  Ytr = ds[\"train\"][lcol_tr]\n",
    "    Xva = ds[\"validation\"][tcol_va] if \"validation\" in ds else ds[\"test\"][tcol_va]\n",
    "    Yva = ds[\"validation\"][lcol_va] if \"validation\" in ds else ds[\"test\"][lcol_va]\n",
    "    return list(Xtr), list(Ytr), list(Xva), list(Yva)\n",
    "\n",
    "def load_from_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    assert \"text\" in df.columns and \"label\" in df.columns, f\"O CSV {path} deve ter colunas: text,label\"\n",
    "    return df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "try:\n",
    "    if HF_DATASET:\n",
    "        print(f\"Carregando dataset HF: {HF_DATASET} ({HF_CONFIG})\")\n",
    "        train_texts, train_labels, val_texts, val_labels = load_from_hf(HF_DATASET, HF_CONFIG)\n",
    "    elif CSV_TRAIN and CSV_VAL:\n",
    "        print(\"Carregando CSVs locaisâ€¦\")\n",
    "        train_texts, train_labels = load_from_csv(CSV_TRAIN)\n",
    "        val_texts,   val_labels   = load_from_csv(CSV_VAL)\n",
    "    else:\n",
    "        print(\"Usando fallback didÃ¡tico embutido (PT-BR).\")\n",
    "        pairs = FALLBACK_PT[:]\n",
    "        random.shuffle(pairs)\n",
    "        # split 75/25\n",
    "        n = int(0.75 * len(pairs))\n",
    "        tr, va = pairs[:n], pairs[n:]\n",
    "        train_texts = [t for t, y in tr]; train_labels = [y for t, y in tr]\n",
    "        val_texts   = [t for t, y in va]; val_labels   = [y for t, y in va]\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Falha ao carregar dados: {e}\")\n",
    "\n",
    "print(f\"Tamanho: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Saneamento (garante list[str], remove NaN/None/bytes) + map labels\n",
    "# ------------------------------------------------------------\n",
    "def _is_nan(x):\n",
    "    try: return bool(np.isnan(x))\n",
    "    except Exception: return False\n",
    "\n",
    "def to_str(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        try: x = x.decode(\"utf-8\", \"ignore\")\n",
    "        except Exception: x = str(x)\n",
    "    if isinstance(x, (np.generic,)): x = x.item()\n",
    "    if isinstance(x, (float, np.floating)) and _is_nan(x): return None\n",
    "    s = str(x).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def clean_xy(X, y, name=\"split\"):\n",
    "    Xo, yo = [], []\n",
    "    bad = 0\n",
    "    for t, l in zip(X, y):\n",
    "        s = to_str(t)\n",
    "        if s is None: bad += 1; continue\n",
    "        Xo.append(s)\n",
    "        yo.append(l)\n",
    "    if bad: print(f\"[{name}] {bad} amostras removidas por texto invÃ¡lido.\")\n",
    "    return Xo, yo\n",
    "\n",
    "train_texts, train_labels = clean_xy(train_texts, train_labels, \"train\")\n",
    "val_texts,   val_labels   = clean_xy(val_texts,   val_labels,   \"val\")\n",
    "\n",
    "# Mapear labels (strings â†’ ids)\n",
    "uniq = sorted({str(l) for l in (list(train_labels) + list(val_labels))})\n",
    "label2id = {lab:i for i, lab in enumerate(uniq)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "train_labels = [label2id[str(l)] for l in train_labels]\n",
    "val_labels   = [label2id[str(l)] for l in val_labels]\n",
    "num_labels = len(label2id)\n",
    "print(\"Labels:\", label2id)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Tokenizer (BERTimbau) e tokenizaÃ§Ã£o\n",
    "# ------------------------------------------------------------\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"   # ou \"â€¦-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 160\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_enc = tokenize_batch(train_texts)\n",
    "val_enc   = tokenize_batch(val_texts)\n",
    "\n",
    "class TorchTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc, labels):\n",
    "        self.enc = enc\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TorchTextDataset(train_enc, train_labels)\n",
    "val_ds   = TorchTextDataset(val_enc,   val_labels)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Modelo e (opcional) congelamento parcial do encoder\n",
    "# ------------------------------------------------------------\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "FREEZE_N_LAYERS = 0  # ex.: 6 para congelar 6 camadas iniciais\n",
    "if FREEZE_N_LAYERS > 0:\n",
    "    # Congelar embeddings + primeiras N camadas do encoder\n",
    "    for p in model.bert.embeddings.parameters():\n",
    "        p.requires_grad = False\n",
    "    for i in range(FREEZE_N_LAYERS):\n",
    "        for p in model.bert.encoder.layer[i].parameters():\n",
    "            p.requires_grad = False\n",
    "    print(f\"Camadas congeladas: embeddings + {FREEZE_N_LAYERS} primeiras camadas.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) MÃ©tricas (accuracy + F1 se disponÃ­vel)\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import evaluate\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric  = evaluate.load(\"f1\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        r1 = acc_metric.compute(predictions=preds, references=labels)\n",
    "        r2 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "        return {\"accuracy\": r1[\"accuracy\"], \"f1\": r2[\"f1\"]}\n",
    "except Exception:\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = (preds == labels).mean()\n",
    "        return {\"accuracy\": float(acc)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Treinamento (Trainer)\n",
    "# ------------------------------------------------------------\n",
    "EPOCHS = 3 if len(train_texts) >= 1000 else 5\n",
    "BATCH  = 16 if torch.cuda.is_available() else 8\n",
    "\n",
    "args = build_training_arguments(\n",
    "    output_dir=\"bertimbau-cls-ptbr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\n=== Iniciando fine-tuning do BERTimbau ===\")\n",
    "trainer.train()\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\nResultados de validaÃ§Ã£o:\", eval_out)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) InferÃªncia em frases PT-BR\n",
    "# ------------------------------------------------------------\n",
    "def predict(texts):\n",
    "    model.eval()\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    decoded = [(t, id2label[int(p)], probs[i]) for i,(t,p) in enumerate(zip(texts, preds))]\n",
    "    return decoded\n",
    "\n",
    "amostras = [\n",
    "    \"o atendimento foi excelente e rÃ¡pido.\",\n",
    "    \"que decepÃ§Ã£o, nÃ£o recomendo a ninguÃ©m.\",\n",
    "    \"funciona bem, mas poderia ser mais intuitivo.\"\n",
    "]\n",
    "for texto, pred, prob in predict(amostras):\n",
    "    print(f\"- {texto}\\n  -> classe: {pred} | probs={np.round(prob, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c243a",
   "metadata": {
    "id": "045c243a"
   },
   "source": [
    "## 6.2 - O que Ã© o `pooled_output` no BERT?\n",
    "\n",
    "\n",
    "O BERT Ã© um **modelo de codificaÃ§Ã£o de sequÃªncia**, ou seja, ele recebe uma lista de tokens e gera um **vetor contextualizado para cada token**.\n",
    "\n",
    "Por exemplo, uma entrada como:\n",
    "```text\n",
    "[CLS] o filme foi Ã³timo [SEP]\n",
    "```\n",
    "\n",
    "gera uma matriz de saÃ­da de dimensÃµes:\n",
    "$$\n",
    "\\text{last_hidden_state} \\in \\mathbb{R}^{(\\text{seq\\_len} \\times d_{model}}\n",
    "$$\n",
    "onde cada linha corresponde ao embedding contextual de um token.\n",
    "\n",
    "---\n",
    "\n",
    "### Relembrando... O papel do `[CLS]`\n",
    "\n",
    "O primeiro token especial, `[CLS]` (*classification*), nÃ£o representa uma palavra real.  \n",
    "Ele Ã© adicionado **no inÃ­cio da sequÃªncia** e serve como um **resumo global da sentenÃ§a**.\n",
    "\n",
    "Durante o treinamento, o BERT aprende a \"preencher\" o vetor do `[CLS]` com informaÃ§Ãµes que sintetizam o significado da sequÃªncia inteira.\n",
    "\n",
    "Assim, o vetor correspondente ao `[CLS]` na saÃ­da final Ã© usado como **entrada para tarefas de classificaÃ§Ã£o**, *Next Sentence Prediction*, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### Mas afinal... O que Ã© o `pooled_output`?\n",
    "\n",
    "Depois do *encoder*, o BERT retorna dois valores principais:\n",
    "\n",
    "1. **`last_hidden_state`** â†’ todos os vetores dos tokens  \n",
    "   â†’ shape: `(batch_size, seq_len, hidden_size)`  \n",
    "   â†’ exemplo: `(8, 128, 768)`\n",
    "\n",
    "2. **`pooled_output`** â†’ vetor Ãºnico da sequÃªncia  \n",
    "   â†’ shape: `(batch_size, hidden_size)`  \n",
    "   â†’ exemplo: `(8, 768)`\n",
    "\n",
    "O `pooled_output` Ã© obtido da seguinte forma:\n",
    "\n",
    "```python\n",
    "pooled_output = tanh(W * hidden_state_[CLS] + b)\n",
    "```\n",
    "\n",
    "Ou seja:\n",
    "- Pega-se **somente o vetor do token `[CLS]`** da Ãºltima camada (`hidden_state_[0]`);\n",
    "- Passa-se por uma **camada linear** (W, b);\n",
    "- Aplica-se **tanh** (funÃ§Ã£o de ativaÃ§Ã£o suave);\n",
    "- O resultado Ã© o **`pooled_output`** â€” a representaÃ§Ã£o final da sequÃªncia.\n",
    "\n",
    "```text\n",
    "SaÃ­da do encoder (Ãºltima camada)\n",
    "â†“\n",
    "[CLS]   O     filme   foi   Ã³timo   [SEP]\n",
    " â†“       â†“       â†“       â†“      â†“\n",
    "h_cls   h_1     h_2     h_3    h_4\n",
    " â†“\n",
    "Linear + tanh\n",
    " â†“\n",
    "pooled_output (vetor Ãºnico da sequÃªncia)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### AplicaÃ§Ãµes\n",
    "\n",
    "| Tarefa | Usa o quÃª | SaÃ­da |\n",
    "|--------|------------|-------|\n",
    "| ClassificaÃ§Ã£o de texto | `pooled_output` | 1 vetor por sentenÃ§a |\n",
    "| NER / POS tagging | `last_hidden_state` | 1 vetor por token |\n",
    "| Question answering | `last_hidden_state` | 1 vetor por token (para prever inÃ­cio/fim) |\n",
    "\n",
    "---\n",
    "\n",
    "### Dica prÃ¡tica\n",
    "\n",
    "No `transformers`, quando vocÃª roda:\n",
    "\n",
    "```python\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    "vocÃª obtÃ©m:\n",
    "\n",
    "```python\n",
    "outputs.last_hidden_state   # embeddings de todos os tokens\n",
    "outputs.pooler_output       # o pooled_output (vetor do [CLS])\n",
    "```\n",
    "\n",
    "Se vocÃª quiser extrair manualmente o vetor `[CLS]` sem o pooling linear:\n",
    "```python\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "```\n",
    "\n",
    "Isso Ã© Ãºtil, por exemplo, se quiser testar diferentes *pooling strategies* (mÃ©dia, max, attention-pooling, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## Mostrando `pooled_output` na prÃ¡tica\n",
    "\n",
    "Vamos fazer o seguinte:\n",
    "1) Tokenizar algumas frases;\n",
    "2) Rodar o BERT (BERTimbau) e inspecionar:\n",
    "   - `last_hidden_state` (um vetor por token),\n",
    "   - o vetor do `[CLS]` cru (`last_hidden_state[:, 0, :]`),\n",
    "   - o `pooled_output` (Linear + `tanh` aplicado ao `[CLS]`);\n",
    "3) Calcular **similaridades cosseno** entre:\n",
    "   - `[CLS]` cru  â†” `pooled_output`,\n",
    "   - `[CLS]` cru  â†” **mean pooling** (mÃ©dia sobre os tokens vÃ¡lidos),\n",
    "   - `pooled_output` â†” **mean pooling**.\n",
    "\n",
    "> IntuiÃ§Ã£o:\n",
    "> - O `pooled_output` Ã© uma **transformaÃ§Ã£o nÃ£o-linear** do `[CLS]` (Linear + `tanh`);\n",
    "> - Ã€s vezes, **mean pooling** de todos os tokens (ignorando `PAD`) funciona melhor em algumas tarefas â€” Ã© bom comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0cbb1",
   "metadata": {
    "id": "b0f0cbb1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# Se quiser trocar por \"bert-base-uncased\", basta alterar o nome:\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "sentences = [\n",
    "    \"O filme foi excelente e muito emocionante!\",\n",
    "    \"O atendimento foi pÃ©ssimo e me deixou insatisfeito.\",\n",
    "    \"Funciona bem, mas poderia ser mais rÃ¡pido.\",\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# TokenizaÃ§Ã£o + forward\n",
    "# ---------------------------\n",
    "enc = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**enc, return_dict=True)\n",
    "    last_hidden = outputs.last_hidden_state               # (B, T, H)\n",
    "    pooled_output = outputs.pooler_output                 # (B, H) = tanh(W * h_cls + b)\n",
    "\n",
    "# ---------------------------\n",
    "# Extrair o vetor [CLS] cru (posiÃ§Ã£o 0) e mean pooling\n",
    "# ---------------------------\n",
    "cls_raw = last_hidden[:, 0, :]                            # (B, H)\n",
    "\n",
    "# mean pooling com mÃ¡scara (ignora PAD)\n",
    "mask = enc[\"attention_mask\"].unsqueeze(-1).float()        # (B, T, 1)\n",
    "sum_tokens = (last_hidden * mask).sum(dim=1)              # (B, H)\n",
    "len_tokens = mask.sum(dim=1).clamp(min=1e-6)              # (B, 1) evita div/0\n",
    "mean_pool = sum_tokens / len_tokens                       # (B, H)\n",
    "\n",
    "# ---------------------------\n",
    "# Similaridades cosseno para comparar representaÃ§Ãµes\n",
    "# ---------------------------\n",
    "def cos(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=-1).detach().cpu()\n",
    "\n",
    "sim_cls_pooled = cos(cls_raw, pooled_output)              # (B,)\n",
    "sim_cls_mean   = cos(cls_raw, mean_pool)                  # (B,)\n",
    "sim_pool_mean  = cos(pooled_output, mean_pool)            # (B,)\n",
    "\n",
    "# (Opcional) Reaplicar a pooler manualmente (quando disponÃ­vel) para mostrar equivalÃªncia\n",
    "recomputed_ok = False\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # algumas versÃµes expÃµem a pooler como model.pooler ou model.bert.pooler\n",
    "        pool = getattr(model, \"pooler\", None) or getattr(model, \"bert\", None).pooler\n",
    "        pooled_re = pool(last_hidden)                     # (B, H)\n",
    "        diff = (pooled_re - pooled_output).abs().max().item()\n",
    "        recomputed_ok = diff < 1e-6\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------\n",
    "# Exibir resultados\n",
    "# ---------------------------\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Shapes:\")\n",
    "print(\"  last_hidden_state:\", tuple(last_hidden.shape))\n",
    "print(\"  [CLS] raw        :\", tuple(cls_raw.shape))\n",
    "print(\"  pooled_output    :\", tuple(pooled_output.shape))\n",
    "print(\"  mean_pool        :\", tuple(mean_pool.shape))\n",
    "\n",
    "print(\"\\nSimilaridades cosseno (por amostra):\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"\\nFrase {i+1}: {s}\")\n",
    "    print(f\"  cos([CLS], pooled) = {sim_cls_pooled[i]:.4f}\")\n",
    "    print(f\"  cos([CLS], mean)   = {sim_cls_mean[i]:.4f}\")\n",
    "    print(f\"  cos(pooled, mean)  = {sim_pool_mean[i]:.4f}\")\n",
    "\n",
    "if recomputed_ok:\n",
    "    print(\"\\n[OK] pooler interno reaplicado == pooled_output (diferenÃ§a < 1e-6).\")\n",
    "else:\n",
    "    print(\"\\n[INFO] NÃ£o foi possÃ­vel (ou nÃ£o faz sentido nesta versÃ£o) reaplicar a pooler internamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7113b0",
   "metadata": {
    "id": "eb7113b0"
   },
   "source": [
    "## 6.3 - DistilBERT e a tÃ©cnica de DistilaÃ§Ã£o de Conhecimento\n",
    "\n",
    "Modelos BERT sÃ£o poderosos, mas muito pesados:  \n",
    "o **BERT-base** tem cerca de **110 milhÃµes de parÃ¢metros**, exigindo grande custo de memÃ³ria e tempo de inferÃªncia.\n",
    "\n",
    "Para aplicaÃ§Ãµes em tempo real (chatbots, busca, mobile), isso Ã© um gargalo.  \n",
    "O **DistilBERT** foi criado como uma **versÃ£o compacta do BERT**, mantendo a maior parte do desempenho com metade do tamanho.\n",
    "\n",
    "---\n",
    "\n",
    "### O que Ã© DistilaÃ§Ã£o de Conhecimento\n",
    "\n",
    "A ideia vem de *Knowledge Distillation* (Hinton et al., 2015):  \n",
    "transferir o â€œconhecimentoâ€ de um modelo grande (*teacher*) para um modelo menor (*student*).\n",
    "\n",
    "O processo segue 3 etapas:\n",
    "\n",
    "1. **Treinar o professor** (ex.: BERT-base) normalmente.\n",
    "2. **Treinar o aluno** (DistilBERT) usando:\n",
    "   - As **saÃ­das reais** do professor (probabilidades sobre as classes ou tokens);\n",
    "   - E as **saÃ­das intermediÃ¡rias** (como embeddings e atenÃ§Ãµes) para que o aluno aprenda a imitÃ¡-las.\n",
    "\n",
    "A perda total combina trÃªs termos:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\alpha_{\\text{soft}} \\cdot \\text{CE}(y_{\\text{teacher}}, y_{\\text{student}}) + \\alpha_{\\\\text{hard}} \\cdot \\text{CE}(y_{\\text{true}}, y_{\\text{student}}) + \\alpha_{\\text{hidden}} \\cdot \\|h_{\\text{teacher}} - h_{\\text{student}}\\|^2\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- **soft loss** â†’ distilaÃ§Ã£o entre distribuiÃ§Ãµes suavizadas (`softmax` com temperatura $begin:math:text$T>1$end:math:text$);  \n",
    "- **hard loss** â†’ erro normal de prediÃ§Ã£o;  \n",
    "- **hidden loss** â†’ aproximaÃ§Ã£o entre embeddings internos.\n",
    "\n",
    "---\n",
    "\n",
    "### Entendendo a funÃ§Ã£o de perda do DistilBERT\n",
    "\n",
    "A distilaÃ§Ã£o de conhecimento combina **trÃªs tipos de aprendizado** â€” supervisionado, por imitaÃ§Ã£o e estrutural â€” em uma Ãºnica funÃ§Ã£o de custo.\n",
    "\n",
    "A funÃ§Ã£o total Ã©:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} =\n",
    "\\alpha_{\\text{soft}} \\cdot \\text{CE}(y_{\\text{teacher}}, y_{\\text{student}})\n",
    "+ \\alpha_{\\text{hard}} \\cdot \\text{CE}(y_{\\text{true}}, y_{\\text{student}})\n",
    "+ \\alpha_{\\text{hidden}} \\cdot \\|h_{\\text{teacher}} - h_{\\text{student}}\\|^2\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- **CE** = *Cross-Entropy Loss*  \n",
    "- $y_{\\text{teacher}}$: saÃ­da (probabilidades) do modelo professor  \n",
    "- $y_{\\text{student}}$: saÃ­da (probabilidades) do modelo aluno  \n",
    "- $y_{\\text{true}}$: rÃ³tulo real  \n",
    "- $h_{\\text{teacher}}, h_{\\text{student}}$: vetores das camadas internas (*hidden states*)  \n",
    "- $\\alpha_{\\text{soft}}, \\alpha_{\\text{hard}}, \\alpha_{\\text{hidden}}$: pesos de cada termo  \n",
    "\n",
    "---\n",
    "\n",
    "#### Primeiro termo â€” *Soft Loss* (ImitaÃ§Ã£o do Professor)\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{soft}} \\cdot \\text{CE}(y_{\\text{teacher}}, y_{\\text{student}})\n",
    "$$\n",
    "\n",
    "O aluno aprende a **imitar a distribuiÃ§Ã£o de probabilidades** do professor, e nÃ£o apenas o rÃ³tulo final.\n",
    "\n",
    "O *teacher* gera uma distribuiÃ§Ã£o de probabilidades sobre o vocabulÃ¡rio (via *softmax*).  \n",
    "Essas probabilidades sÃ£o â€œsuavizadasâ€ com uma **temperatura $T > 1$**:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}\n",
    "$$\n",
    "\n",
    "Valores maiores de $T$ tornam a distribuiÃ§Ã£o menos â€œduraâ€, expondo mais *informaÃ§Ã£o relacional* â€” por exemplo, o professor mostra que \"Ã³timo\" e \"excelente\" sÃ£o parecidos, mas \"pÃ©ssimo\" Ã© muito diferente.\n",
    "\n",
    "Assim, o aluno aprende:\n",
    "> â€œcomo o professor pensaâ€, nÃ£o apenas â€œqual classe ele escolheuâ€.\n",
    "\n",
    "---\n",
    "\n",
    "#### Segundo termo â€” *Hard Loss* (SupervisÃ£o tradicional)\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{hard}} \\cdot \\text{CE}(y_{\\text{true}}, y_{\\text{student}})\n",
    "$$\n",
    "\n",
    "Ã‰ a **perda normal de classificaÃ§Ã£o**, usando os rÃ³tulos verdadeiros do dataset.  \n",
    "Esse termo garante que o aluno continue aprendendo a tarefa original enquanto imita o professor.\n",
    "\n",
    "---\n",
    "\n",
    "#### Terceiro termo â€” *Hidden-State Alignment Loss*\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{hidden}} \\cdot \\|h_{\\text{teacher}} - h_{\\text{student}}\\|^2\n",
    "$$\n",
    "\n",
    "AlÃ©m de copiar as saÃ­das finais, o DistilBERT tambÃ©m aprende a **replicar as representaÃ§Ãµes internas** do BERT.\n",
    "\n",
    "Durante o prÃ©-treinamento:\n",
    "- as camadas do aluno sÃ£o alinhadas com camadas equivalentes do professor;\n",
    "- o aluno tenta minimizar a **distÃ¢ncia L2** entre embeddings correspondentes.\n",
    "\n",
    "Esse termo faz o aluno â€œpensarâ€ de maneira parecida, camada a camada.\n",
    "\n",
    "---\n",
    "\n",
    "#### Combinando os termos\n",
    "\n",
    "| Termo | Tipo de aprendizado | Papel no treino |\n",
    "|--------|---------------------|-----------------|\n",
    "| $\\alpha_{\\text{soft}} \\cdot CE(y_t, y_s)$ | ImitaÃ§Ã£o (*distilaÃ§Ã£o*) | Fazer o aluno reproduzir o comportamento do professor |\n",
    "| $\\alpha_{\\text{hard}} \\cdot CE(y_{true}, y_s)$ | Supervisionado | Garantir que o aluno continue resolvendo a tarefa original |\n",
    "| $\\alpha_{\\text{hidden}} \\cdot \\|h_t - h_s\\|^2$ | Estrutural | Fazer o aluno representar internamente o conhecimento do professor |\n",
    "\n",
    "\n",
    "```text\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Teacher    â”‚\n",
    "        â”‚ (BERT-base) â”‚\n",
    "        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â”‚  y_teacher, h_teacher\n",
    "             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Student    â”‚\n",
    "        â”‚ (DistilBERT)â”‚\n",
    "        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚ 3 perdas combinadas:                 â”‚\n",
    "     â”‚   1) Soft â†’ imitar distribuiÃ§Ãµes     â”‚\n",
    "     â”‚   2) Hard â†’ prever rÃ³tulo correto    â”‚\n",
    "     â”‚   3) Hidden â†’ copiar representaÃ§Ãµes  â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Valores tÃ­picos\n",
    "\n",
    "No treinamento original do DistilBERT (Sanh et al., 2019):\n",
    "\n",
    "- $T = 2.0$  \n",
    "- $\\alpha_{\\text{soft}} = 0.5$  \n",
    "- $\\alpha_{\\text{hard}} = 0.5$  \n",
    "- $\\alpha_{\\text{hidden}} = 1.0$\n",
    "\n",
    "Esses valores equilibram **imitaÃ§Ã£o** e **fidelidade Ã  tarefa**.\n",
    "\n",
    "#### Em resumo...\n",
    "\n",
    "> O DistilBERT aprende nÃ£o apenas com **respostas finais**, mas com **o raciocÃ­nio interno do professor**.  \n",
    "> Ele tenta ser um aluno mais rÃ¡pido, mas com o mesmo â€œjeito de pensarâ€.\n",
    "\n",
    "---\n",
    "\n",
    "### Como o DistilBERT Ã© treinado\n",
    "\n",
    "O DistilBERT (Sanh et al., 2019) segue esta configuraÃ§Ã£o:\n",
    "\n",
    "| Item | BERT-base | DistilBERT |\n",
    "|------|------------|------------|\n",
    "| Camadas (encoder) | 12 | 6 |\n",
    "| CabeÃ§as de atenÃ§Ã£o | 12 | 12 |\n",
    "| Hidden size | 768 | 768 |\n",
    "| ParÃ¢metros | 110M | 66M |\n",
    "| Velocidade | â€” | 60% mais rÃ¡pido |\n",
    "| Tamanho | â€” | 40% menor |\n",
    "\n",
    "O **student** Ã© inicializado com **camadas alternadas do teacher**:  \n",
    "as camadas 2, 4, 6, 8, 10, 12 do BERT sÃ£o copiadas.\n",
    "\n",
    "Durante o prÃ©-treinamento:\n",
    "- o *teacher* (BERT-base) fica congelado;\n",
    "- o *student* aprende:\n",
    "  - **Masked Language Modeling (MLM)**,  \n",
    "  - **DistilaÃ§Ã£o de logits** (soft targets do teacher),  \n",
    "  - e **similaridade entre estados ocultos**.\n",
    "\n",
    "O DistilBERT **nÃ£o possui o token [CLS] pooler nem a cabeÃ§a NSP (Next Sentence Prediction)**.  \n",
    "Ou seja, ele Ã© otimizado apenas para o objetivo de **MLM + distilaÃ§Ã£o**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Desempenho\n",
    "\n",
    "Mesmo com metade das camadas, o DistilBERT mantÃ©m cerca de **97% da acurÃ¡cia do BERT-base** em benchmarks como GLUE, com:\n",
    "- 40% menos parÃ¢metros,\n",
    "- 60% menos custo computacional,\n",
    "- 2Ã— mais rÃ¡pido na inferÃªncia.\n",
    "\n",
    "```text\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚               BERT-base                  â”‚\n",
    "        â”‚ 12 camadas, 110M params (teacher)        â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
    "                â”‚              â”‚               â”‚\n",
    "                â–¼              â–¼               â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚              DistilBERT                  â”‚\n",
    "        â”‚ 6 camadas, 66M params (student)          â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†‘ aprende com â†“\n",
    "     (soft logits + hidden states + labels reais)\n",
    "```\n",
    "\n",
    "- **DistilBERT = BERT menor + mesmo vocabulÃ¡rio + sem NSP**\n",
    "- **Treinado com distilaÃ§Ã£o de conhecimento**\n",
    "- **Resultado:** mais leve, mais rÃ¡pido, quase mesma performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d23c1a",
   "metadata": {
    "id": "40d23c1a"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fine-tuning DistilBERT para ClassificaÃ§Ã£o de Texto\n",
    "# ============================================================\n",
    "import os, math, random, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (Colab) instalar dependÃªncias\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -U \"transformers>=4.39\" \"datasets>=2.14\" \"accelerate>=0.28\" \"evaluate>=0.4\" \"pandas>=1.5\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Imports principais\n",
    "# ------------------------------------------------------------\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "# Helper: TrainingArguments compatÃ­vel com sua versÃ£o\n",
    "def build_training_arguments(**kwargs) -> TrainingArguments:\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    allowed = set(sig.parameters.keys()); allowed.discard(\"self\")\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n",
    "    dropped = [k for k in kwargs if k not in allowed]\n",
    "    if dropped:\n",
    "        print(\"[Aviso] parÃ¢metros ignorados nesta versÃ£o:\", dropped)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Escolha da FONTE DE DADOS (selecione UMA)\n",
    "# ------------------------------------------------------------\n",
    "# (A) Dataset Hugging Face (deve ter colunas text/label ou similares)\n",
    "HF_DATASET = \"yelp_polarity\"  # ou \"imdb\", \"ag_news\", etc. ou None\n",
    "HF_CONFIG  = None\n",
    "\n",
    "# (B) CSVs locais com colunas: text,label\n",
    "CSV_TRAIN = None  # ex: \"/content/train.csv\"\n",
    "CSV_VAL   = None  # ex: \"/content/val.csv\"\n",
    "\n",
    "# (C) Fallback didÃ¡tico (PT/EN misto, sÃ³ p/ demo)\n",
    "FALLBACK = [\n",
    "    (\"o filme Ã© excelente e muito bem dirigido\", 1),\n",
    "    (\"pÃ©ssimo atendimento, nÃ£o volto mais\", 0),\n",
    "    (\"the product is amazing and works great\", 1),\n",
    "    (\"awful experience, totally disappointed\", 0),\n",
    "    (\"serviÃ§o rÃ¡pido e eficiente, gostei\", 1),\n",
    "    (\"interface confusa e cheia de bugs\", 0),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Carregar dados\n",
    "# ------------------------------------------------------------\n",
    "train_texts, train_labels = [], []\n",
    "val_texts,   val_labels   = [], []\n",
    "\n",
    "def load_from_hf(name, config=None):\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(name, config) if config else load_dataset(name)\n",
    "    # tenta detectar colunas\n",
    "    def pick_cols(split):\n",
    "        cols = ds[split].column_names\n",
    "        cand_text = [c for c in [\"text\", \"sentence\", \"review\", \"content\"] if c in cols] or [cols[0]]\n",
    "        cand_label= [c for c in [\"label\", \"labels\", \"sentiment\", \"stars\"] if c in cols] or [cols[1]]\n",
    "        return cand_text[0], cand_label[0]\n",
    "    t_tr, l_tr = pick_cols(\"train\")\n",
    "    t_va, l_va = pick_cols(\"test\") if \"validation\" not in ds else pick_cols(\"validation\")\n",
    "    Xtr = ds[\"train\"][t_tr];  Ytr = ds[\"train\"][l_tr]\n",
    "    Xva = ds[\"test\"][t_va] if \"validation\" not in ds else ds[\"validation\"][t_va]\n",
    "    Yva = ds[\"test\"][l_va] if \"validation\" not in ds else ds[\"validation\"][l_va]\n",
    "    return list(Xtr), list(Ytr), list(Xva), list(Yva)\n",
    "\n",
    "def load_from_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    assert \"text\" in df.columns and \"label\" in df.columns, f\"O CSV {path} deve ter colunas: text,label\"\n",
    "    return df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "try:\n",
    "    if HF_DATASET:\n",
    "        print(f\"Carregando dataset HF: {HF_DATASET} ({HF_CONFIG})\")\n",
    "        train_texts, train_labels, val_texts, val_labels = load_from_hf(HF_DATASET, HF_CONFIG)\n",
    "    elif CSV_TRAIN and CSV_VAL:\n",
    "        print(\"Carregando CSVs locaisâ€¦\")\n",
    "        train_texts, train_labels = load_from_csv(CSV_TRAIN)\n",
    "        val_texts,   val_labels   = load_from_csv(CSV_VAL)\n",
    "    else:\n",
    "        print(\"Usando fallback didÃ¡tico embutido.\")\n",
    "        pairs = FALLBACK[:]\n",
    "        random.shuffle(pairs)\n",
    "        n = int(0.7 * len(pairs))\n",
    "        tr, va = pairs[:n], pairs[n:]\n",
    "        train_texts = [t for t,_ in tr]; train_labels = [y for _,y in tr]\n",
    "        val_texts   = [t for t,_ in va]; val_labels   = [y for _,y in va]\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Falha ao carregar dados: {e}\")\n",
    "\n",
    "print(f\"Tamanho: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Saneamento (garante list[str], remove NaN/None/bytes) + map de labels\n",
    "# ------------------------------------------------------------\n",
    "def _is_nan(x):\n",
    "    try: return bool(np.isnan(x))\n",
    "    except Exception: return False\n",
    "\n",
    "def to_str(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        try: x = x.decode(\"utf-8\", \"ignore\")\n",
    "        except Exception: x = str(x)\n",
    "    if isinstance(x, (np.generic,)): x = x.item()\n",
    "    if isinstance(x, (float, np.floating)) and _is_nan(x): return None\n",
    "    s = str(x).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def clean_xy(X, y, name=\"split\"):\n",
    "    Xo, yo = [], []\n",
    "    bad = 0\n",
    "    for t, l in zip(X, y):\n",
    "        s = to_str(t)\n",
    "        if s is None: bad += 1; continue\n",
    "        Xo.append(s); yo.append(l)\n",
    "    if bad: print(f\"[{name}] {bad} amostras removidas por texto invÃ¡lido.\")\n",
    "    return Xo, yo\n",
    "\n",
    "train_texts, train_labels = clean_xy(train_texts, train_labels, \"train\")\n",
    "val_texts,   val_labels   = clean_xy(val_texts,   val_labels,   \"val\")\n",
    "\n",
    "# Mapeia labels (strings â†’ ids)\n",
    "uniq = sorted({str(l) for l in (list(train_labels)+list(val_labels))})\n",
    "label2id = {lab:i for i,lab in enumerate(uniq)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "train_labels = [label2id[str(l)] for l in train_labels]\n",
    "val_labels   = [label2id[str(l)] for l in val_labels]\n",
    "num_labels = len(label2id)\n",
    "print(\"Labels:\", label2id)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Tokenizer (DistilBERT) e tokenizaÃ§Ã£o\n",
    "# ------------------------------------------------------------\n",
    "MODEL_NAME = \"distilbert-base-uncased\"    # troque aqui se quiser outro DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 160\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_enc = tokenize_batch(train_texts)\n",
    "val_enc   = tokenize_batch(val_texts)\n",
    "\n",
    "class TorchTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc, labels):\n",
    "        self.enc = enc; self.labels = labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TorchTextDataset(train_enc, train_labels)\n",
    "val_ds   = TorchTextDataset(val_enc,   val_labels)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Modelo\n",
    "# ------------------------------------------------------------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) MÃ©tricas (accuracy + F1 se disponÃ­vel)\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import evaluate\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric  = evaluate.load(\"f1\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        r1 = acc_metric.compute(predictions=preds, references=labels)\n",
    "        r2 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "        return {\"accuracy\": r1[\"accuracy\"], \"f1\": r2[\"f1\"]}\n",
    "except Exception:\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = (preds == labels).mean()\n",
    "        return {\"accuracy\": float(acc)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Treinamento (Trainer)\n",
    "# ------------------------------------------------------------\n",
    "EPOCHS = 2 if len(train_texts) > 1000 else 4\n",
    "BATCH  = 16 if torch.cuda.is_available() else 8\n",
    "\n",
    "args = build_training_arguments(\n",
    "    output_dir=\"distilbert-cls\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=3e-5,          # DistilBERT costuma aceitar 3e-5 bem\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\n=== Iniciando fine-tuning do DistilBERT ===\")\n",
    "trainer.train()\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\nResultados de validaÃ§Ã£o:\", eval_out)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) InferÃªncia em frases\n",
    "# ------------------------------------------------------------\n",
    "def predict(texts):\n",
    "    model.eval()\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    decoded = [(t, id2label[int(p)], probs[i]) for i,(t,p) in enumerate(zip(texts, preds))]\n",
    "    return decoded\n",
    "\n",
    "samples = [\n",
    "    \"The product quality is amazing and the delivery was fast.\",\n",
    "    \"Horrible support. I will never buy again.\",\n",
    "    \"serviÃ§o excelente e muito rÃ¡pido, recomendo\",\n",
    "    \"nÃ£o gostei, veio com defeito.\"\n",
    "]\n",
    "for texto, pred, prob in predict(samples):\n",
    "    print(f\"- {texto}\\n  -> classe: {pred} | probs={np.round(prob, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe100c",
   "metadata": {
    "id": "94fe100c"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "runtime_attributes": {
    "runtime_version": "2025.07"
   }
  },
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
