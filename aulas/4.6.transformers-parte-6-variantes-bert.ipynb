{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01878e9f",
   "metadata": {},
   "source": [
    "# 6) Variantes do BERT\n",
    "\n",
    "Desde 2018, surgiram diversas vers√µes otimizadas do BERT com objetivos diferentes ‚Äî  \n",
    "melhorar desempenho, reduzir custo ou mudar o pr√©-treinamento.\n",
    "\n",
    "| Modelo | Principais caracter√≠sticas | Ganhos |\n",
    "|---------|----------------------------|---------|\n",
    "| **BERT-base / BERT-large** | Modelo original (12 / 24 camadas, 110M / 340M par√¢metros). | Base de refer√™ncia. |\n",
    "| **DistilBERT** | 40% menor, 60% mais r√°pido, via *knowledge distillation*. | Efici√™ncia. |\n",
    "| **RoBERTa** | Remove NSP, usa *dynamic masking*, treina em muito mais dados. | +Robusto, melhor generaliza√ß√£o. |\n",
    "| **ALBERT** | Par√¢metros compartilhados + fatoriza√ß√£o de embeddings. | Reduz drasticamente o tamanho (de 110M ‚Üí 12M). |\n",
    "| **DeBERTa** | *Disentangled attention* + corre√ß√£o de posi√ß√£o absoluta. | Melhor compreens√£o sint√°tica e sem√¢ntica. |\n",
    "| **BERTimbau** üáßüá∑ | BERT treinado em portugu√™s (brWac + Wikipedia). | Melhor performance em PT-BR. |\n",
    "\n",
    "---\n",
    "\n",
    "## Escolha pr√°tica\n",
    "\n",
    "| Situa√ß√£o | Modelo sugerido |\n",
    "|-----------|-----------------|\n",
    "| Poucos recursos de GPU | `distilbert-base-uncased` |\n",
    "| Tarefa em portugu√™s | `neuralmind/bert-base-portuguese-cased` |\n",
    "| Dataset grande e precis√£o m√°xima | `roberta-large` ou `deberta-v3-large` |\n",
    "| Deploy em mobile / produ√ß√£o leve | `tinybert` ou `albert-base-v2` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968605a9",
   "metadata": {},
   "source": [
    "## 6.1 ‚Äî Fine-tuning do BERTimbau (PT-BR) para Classifica√ß√£o de Texto\n",
    "\n",
    "O **BERTimbau** √© o BERT pr√©-treinado em corpora de Portugu√™s (Wikipedia + brWac).  \n",
    "Para tarefas de **classifica√ß√£o** (sentimento, t√≥picos, inten√ß√£o, etc.), fazemos **fine-tuning** adicionando uma **camada linear** sobre o vetor de **[CLS]** (internamente, o `pooled_output`).\n",
    "\n",
    "**Pipeline**\n",
    "1. Carregar o **tokenizer** e o **modelo** `neuralmind/bert-base-portuguese-cased` (ou *uncased*).\n",
    "2. Preparar os dados (`text`, `label`).\n",
    "3. Tokenizar (`[CLS] ... [SEP]`), definir `max_length`.\n",
    "4. Treinar com **taxa de aprendizado pequena** (ex.: 2e-5), poucas √©pocas (2‚Äì4).\n",
    "5. Avaliar (accuracy/F1) e testar com frases novas.\n",
    "\n",
    "**Dicas pr√°ticas**\n",
    "- Use o modelo **cased** para preservar acentua√ß√£o e caixa em PT-BR.\n",
    "- Se a base for pequena, considere **congelar** as primeiras camadas do encoder para estabilizar.\n",
    "- Classes desbalanceadas? Use `class_weights` ou *weighted loss*.\n",
    "- M√©tricas: **accuracy** e **F1** (macro/weighted).\n",
    "\n",
    "**Entradas esperadas pelo c√≥digo**\n",
    "- Voc√™ pode:\n",
    "  - (A) informar um **dataset do Hugging Face** com colunas `text` e `label`, ou  \n",
    "  - (B) apontar para **CSVs** (`train.csv`, `val.csv`) com colunas `text,label`, ou  \n",
    "  - (C) usar um **mini-dataset did√°tico** embutido (fallback) s√≥ para demonstrar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fine-tuning BERTimbau (Portugu√™s) para Classifica√ß√£o de Texto\n",
    "# ============================================================\n",
    "import os, math, random, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (Colab) instalar depend√™ncias\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -U \"transformers>=4.39\" \"datasets>=2.14\" \"accelerate>=0.28\" \"evaluate>=0.4\" \"pandas>=1.5\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Imports principais\n",
    "# ------------------------------------------------------------\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "# Helper: TrainingArguments compat√≠vel com a sua vers√£o\n",
    "def build_training_arguments(**kwargs) -> TrainingArguments:\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    allowed = set(sig.parameters.keys()); allowed.discard(\"self\")\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n",
    "    dropped = [k for k in kwargs if k not in allowed]\n",
    "    if dropped:\n",
    "        print(\"[Aviso] par√¢metros ignorados nesta vers√£o:\", dropped)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) FONTE DOS DADOS (escolha UMA)\n",
    "# ------------------------------------------------------------\n",
    "# (A) Dataset Hugging Face (deve ter colunas 'text' e 'label')\n",
    "HF_DATASET = None\n",
    "HF_CONFIG  = None  # ex.: \"default\" ou subtarefa (se houver)\n",
    "\n",
    "# (B) CSVs locais com colunas: text,label (r√≥tulo pode ser string)\n",
    "CSV_TRAIN = None  # ex.: \"/content/train.csv\"\n",
    "CSV_VAL   = None  # ex.: \"/content/val.csv\"\n",
    "\n",
    "# (C) Fallback did√°tico embutido (PT-BR)\n",
    "FALLBACK_PT = [\n",
    "    (\"o filme √© excelente, emocionante e muito bem dirigido.\", 1),\n",
    "    (\"p√©ssimo atendimento, n√£o volto mais.\", 0),\n",
    "    (\"a comida estava maravilhosa, sabores incr√≠veis.\", 1),\n",
    "    (\"produto chegou quebrado e atrasado, experi√™ncia horr√≠vel.\", 0),\n",
    "    (\"servi√ßo r√°pido e eficiente, gostei bastante.\", 1),\n",
    "    (\"interface confusa e cheia de bugs.\", 0),\n",
    "    (\"uma experi√™ncia fant√°stica do come√ßo ao fim!\", 1),\n",
    "    (\"n√£o recomendo, custo-benef√≠cio muito ruim.\", 0),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Carregar dados\n",
    "# ------------------------------------------------------------\n",
    "train_texts, train_labels = [], []\n",
    "val_texts,   val_labels   = [], []\n",
    "\n",
    "def load_from_hf(name, config=None):\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(name, config) if config else load_dataset(name)\n",
    "    # tenta achar colunas text/label comuns\n",
    "    # voc√™ pode adaptar aqui se seu dataset tiver outros nomes\n",
    "    def pick_cols(split):\n",
    "        cand_text = [c for c in [\"text\", \"sentence\", \"texto\", \"review\", \"content\"] if c in ds[split].column_names]\n",
    "        cand_label= [c for c in [\"label\", \"labels\", \"sentiment\", \"classe\"] if c in ds[split].column_names]\n",
    "        assert cand_text and cand_label, f\"N√£o encontrei colunas 'text' e 'label' no split {split}. Colunas: {ds[split].column_names}\"\n",
    "        return cand_text[0], cand_label[0]\n",
    "\n",
    "    tcol_tr, lcol_tr = pick_cols(\"train\")\n",
    "    tcol_va, lcol_va = pick_cols(\"validation\") if \"validation\" in ds else pick_cols(\"test\")\n",
    "\n",
    "    Xtr = ds[\"train\"][tcol_tr];  Ytr = ds[\"train\"][lcol_tr]\n",
    "    Xva = ds[\"validation\"][tcol_va] if \"validation\" in ds else ds[\"test\"][tcol_va]\n",
    "    Yva = ds[\"validation\"][lcol_va] if \"validation\" in ds else ds[\"test\"][lcol_va]\n",
    "    return list(Xtr), list(Ytr), list(Xva), list(Yva)\n",
    "\n",
    "def load_from_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    assert \"text\" in df.columns and \"label\" in df.columns, f\"O CSV {path} deve ter colunas: text,label\"\n",
    "    return df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "try:\n",
    "    if HF_DATASET:\n",
    "        print(f\"Carregando dataset HF: {HF_DATASET} ({HF_CONFIG})\")\n",
    "        train_texts, train_labels, val_texts, val_labels = load_from_hf(HF_DATASET, HF_CONFIG)\n",
    "    elif CSV_TRAIN and CSV_VAL:\n",
    "        print(\"Carregando CSVs locais‚Ä¶\")\n",
    "        train_texts, train_labels = load_from_csv(CSV_TRAIN)\n",
    "        val_texts,   val_labels   = load_from_csv(CSV_VAL)\n",
    "    else:\n",
    "        print(\"Usando fallback did√°tico embutido (PT-BR).\")\n",
    "        pairs = FALLBACK_PT[:]\n",
    "        random.shuffle(pairs)\n",
    "        # split 75/25\n",
    "        n = int(0.75 * len(pairs))\n",
    "        tr, va = pairs[:n], pairs[n:]\n",
    "        train_texts = [t for t, y in tr]; train_labels = [y for t, y in tr]\n",
    "        val_texts   = [t for t, y in va]; val_labels   = [y for t, y in va]\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Falha ao carregar dados: {e}\")\n",
    "\n",
    "print(f\"Tamanho: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Saneamento (garante list[str], remove NaN/None/bytes) + map labels\n",
    "# ------------------------------------------------------------\n",
    "def _is_nan(x):\n",
    "    try: return bool(np.isnan(x))\n",
    "    except Exception: return False\n",
    "\n",
    "def to_str(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        try: x = x.decode(\"utf-8\", \"ignore\")\n",
    "        except Exception: x = str(x)\n",
    "    if isinstance(x, (np.generic,)): x = x.item()\n",
    "    if isinstance(x, (float, np.floating)) and _is_nan(x): return None\n",
    "    s = str(x).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def clean_xy(X, y, name=\"split\"):\n",
    "    Xo, yo = [], []\n",
    "    bad = 0\n",
    "    for t, l in zip(X, y):\n",
    "        s = to_str(t)\n",
    "        if s is None: bad += 1; continue\n",
    "        Xo.append(s)\n",
    "        yo.append(l)\n",
    "    if bad: print(f\"[{name}] {bad} amostras removidas por texto inv√°lido.\")\n",
    "    return Xo, yo\n",
    "\n",
    "train_texts, train_labels = clean_xy(train_texts, train_labels, \"train\")\n",
    "val_texts,   val_labels   = clean_xy(val_texts,   val_labels,   \"val\")\n",
    "\n",
    "# Mapear labels (strings ‚Üí ids)\n",
    "uniq = sorted({str(l) for l in (list(train_labels) + list(val_labels))})\n",
    "label2id = {lab:i for i, lab in enumerate(uniq)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "train_labels = [label2id[str(l)] for l in train_labels]\n",
    "val_labels   = [label2id[str(l)] for l in val_labels]\n",
    "num_labels = len(label2id)\n",
    "print(\"Labels:\", label2id)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Tokenizer (BERTimbau) e tokeniza√ß√£o\n",
    "# ------------------------------------------------------------\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"   # ou \"‚Ä¶-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 160\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_enc = tokenize_batch(train_texts)\n",
    "val_enc   = tokenize_batch(val_texts)\n",
    "\n",
    "class TorchTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc, labels):\n",
    "        self.enc = enc\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TorchTextDataset(train_enc, train_labels)\n",
    "val_ds   = TorchTextDataset(val_enc,   val_labels)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Modelo e (opcional) congelamento parcial do encoder\n",
    "# ------------------------------------------------------------\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "FREEZE_N_LAYERS = 0  # ex.: 6 para congelar 6 camadas iniciais\n",
    "if FREEZE_N_LAYERS > 0:\n",
    "    # Congelar embeddings + primeiras N camadas do encoder\n",
    "    for p in model.bert.embeddings.parameters():\n",
    "        p.requires_grad = False\n",
    "    for i in range(FREEZE_N_LAYERS):\n",
    "        for p in model.bert.encoder.layer[i].parameters():\n",
    "            p.requires_grad = False\n",
    "    print(f\"Camadas congeladas: embeddings + {FREEZE_N_LAYERS} primeiras camadas.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) M√©tricas (accuracy + F1 se dispon√≠vel)\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import evaluate\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric  = evaluate.load(\"f1\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        r1 = acc_metric.compute(predictions=preds, references=labels)\n",
    "        r2 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "        return {\"accuracy\": r1[\"accuracy\"], \"f1\": r2[\"f1\"]}\n",
    "except Exception:\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = (preds == labels).mean()\n",
    "        return {\"accuracy\": float(acc)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Treinamento (Trainer)\n",
    "# ------------------------------------------------------------\n",
    "EPOCHS = 3 if len(train_texts) >= 1000 else 5\n",
    "BATCH  = 16 if torch.cuda.is_available() else 8\n",
    "\n",
    "args = build_training_arguments(\n",
    "    output_dir=\"bertimbau-cls-ptbr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\n=== Iniciando fine-tuning do BERTimbau ===\")\n",
    "trainer.train()\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\nResultados de valida√ß√£o:\", eval_out)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Infer√™ncia em frases PT-BR\n",
    "# ------------------------------------------------------------\n",
    "def predict(texts):\n",
    "    model.eval()\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    decoded = [(t, id2label[int(p)], probs[i]) for i,(t,p) in enumerate(zip(texts, preds))]\n",
    "    return decoded\n",
    "\n",
    "amostras = [\n",
    "    \"o atendimento foi excelente e r√°pido.\",\n",
    "    \"que decep√ß√£o, n√£o recomendo a ningu√©m.\",\n",
    "    \"funciona bem, mas poderia ser mais intuitivo.\"\n",
    "]\n",
    "for texto, pred, prob in predict(amostras):\n",
    "    print(f\"- {texto}\\n  -> classe: {pred} | probs={np.round(prob, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c243a",
   "metadata": {},
   "source": [
    "## 6.2 - O que √© o `pooled_output` no BERT?\n",
    "\n",
    "\n",
    "O BERT √© um **modelo de codifica√ß√£o de sequ√™ncia**, ou seja, ele recebe uma lista de tokens e gera um **vetor contextualizado para cada token**.\n",
    "\n",
    "Por exemplo, uma entrada como:\n",
    "```text\n",
    "[CLS] o filme foi √≥timo [SEP]\n",
    "```\n",
    "\n",
    "gera uma matriz de sa√≠da de dimens√µes:\n",
    "$begin:math:display$\n",
    "\\\\text{last_hidden_state} \\\\in \\\\mathbb{R}^{(\\\\text{seq\\\\_len} \\\\times d_{model})}\n",
    "$end:math:display$\n",
    "onde cada linha corresponde ao embedding contextual de um token.\n",
    "\n",
    "---\n",
    "\n",
    "### Relembrando... O papel do `[CLS]`\n",
    "\n",
    "O primeiro token especial, `[CLS]` (*classification*), n√£o representa uma palavra real.  \n",
    "Ele √© adicionado **no in√≠cio da sequ√™ncia** e serve como um **resumo global da senten√ßa**.\n",
    "\n",
    "Durante o treinamento, o BERT aprende a \"preencher\" o vetor do `[CLS]` com informa√ß√µes que sintetizam o significado da sequ√™ncia inteira.\n",
    "\n",
    "Assim, o vetor correspondente ao `[CLS]` na sa√≠da final √© usado como **entrada para tarefas de classifica√ß√£o**, *Next Sentence Prediction*, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### Mas afinal... O que √© o `pooled_output`?\n",
    "\n",
    "Depois do *encoder*, o BERT retorna dois valores principais:\n",
    "\n",
    "1. **`last_hidden_state`** ‚Üí todos os vetores dos tokens  \n",
    "   ‚Üí shape: `(batch_size, seq_len, hidden_size)`  \n",
    "   ‚Üí exemplo: `(8, 128, 768)`\n",
    "\n",
    "2. **`pooled_output`** ‚Üí vetor √∫nico da sequ√™ncia  \n",
    "   ‚Üí shape: `(batch_size, hidden_size)`  \n",
    "   ‚Üí exemplo: `(8, 768)`\n",
    "\n",
    "O `pooled_output` √© obtido da seguinte forma:\n",
    "\n",
    "```python\n",
    "pooled_output = tanh(W * hidden_state_[CLS] + b)\n",
    "```\n",
    "\n",
    "Ou seja:\n",
    "- Pega-se **somente o vetor do token `[CLS]`** da √∫ltima camada (`hidden_state_[0]`);\n",
    "- Passa-se por uma **camada linear** (W, b);\n",
    "- Aplica-se **tanh** (fun√ß√£o de ativa√ß√£o suave);\n",
    "- O resultado √© o **`pooled_output`** ‚Äî a representa√ß√£o final da sequ√™ncia.\n",
    "\n",
    "```text\n",
    "Sa√≠da do encoder (√∫ltima camada)\n",
    "‚Üì\n",
    "[CLS]   O     filme   foi   √≥timo   [SEP]\n",
    " ‚Üì       ‚Üì       ‚Üì       ‚Üì      ‚Üì\n",
    "h_cls   h_1     h_2     h_3    h_4\n",
    " ‚Üì\n",
    "Linear + tanh\n",
    " ‚Üì\n",
    "pooled_output (vetor √∫nico da sequ√™ncia)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Aplica√ß√µes\n",
    "\n",
    "| Tarefa | Usa o qu√™ | Sa√≠da |\n",
    "|--------|------------|-------|\n",
    "| Classifica√ß√£o de texto | `pooled_output` | 1 vetor por senten√ßa |\n",
    "| NER / POS tagging | `last_hidden_state` | 1 vetor por token |\n",
    "| Question answering | `last_hidden_state` | 1 vetor por token (para prever in√≠cio/fim) |\n",
    "\n",
    "---\n",
    "\n",
    "### Dica pr√°tica\n",
    "\n",
    "No `transformers`, quando voc√™ roda:\n",
    "\n",
    "```python\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    "voc√™ obt√©m:\n",
    "\n",
    "```python\n",
    "outputs.last_hidden_state   # embeddings de todos os tokens\n",
    "outputs.pooler_output       # o pooled_output (vetor do [CLS])\n",
    "```\n",
    "\n",
    "Se voc√™ quiser extrair manualmente o vetor `[CLS]` sem o pooling linear:\n",
    "```python\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "```\n",
    "\n",
    "Isso √© √∫til, por exemplo, se quiser testar diferentes *pooling strategies* (m√©dia, max, attention-pooling, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## Mostrando `pooled_output` na pr√°tica\n",
    "\n",
    "Vamos fazer o seguinte:\n",
    "1) Tokenizar algumas frases;\n",
    "2) Rodar o BERT (BERTimbau) e inspecionar:\n",
    "   - `last_hidden_state` (um vetor por token),\n",
    "   - o vetor do `[CLS]` cru (`last_hidden_state[:, 0, :]`),\n",
    "   - o `pooled_output` (Linear + `tanh` aplicado ao `[CLS]`);\n",
    "3) Calcular **similaridades cosseno** entre:\n",
    "   - `[CLS]` cru  ‚Üî `pooled_output`,\n",
    "   - `[CLS]` cru  ‚Üî **mean pooling** (m√©dia sobre os tokens v√°lidos),\n",
    "   - `pooled_output` ‚Üî **mean pooling**.\n",
    "\n",
    "> Intui√ß√£o:\n",
    "> - O `pooled_output` √© uma **transforma√ß√£o n√£o-linear** do `[CLS]` (Linear + `tanh`);\n",
    "> - √Äs vezes, **mean pooling** de todos os tokens (ignorando `PAD`) funciona melhor em algumas tarefas ‚Äî √© bom comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# Se quiser trocar por \"bert-base-uncased\", basta alterar o nome:\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "sentences = [\n",
    "    \"O filme foi excelente e muito emocionante!\",\n",
    "    \"O atendimento foi p√©ssimo e me deixou insatisfeito.\",\n",
    "    \"Funciona bem, mas poderia ser mais r√°pido.\",\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# Tokeniza√ß√£o + forward\n",
    "# ---------------------------\n",
    "enc = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**enc, return_dict=True)\n",
    "    last_hidden = outputs.last_hidden_state               # (B, T, H)\n",
    "    pooled_output = outputs.pooler_output                 # (B, H) = tanh(W * h_cls + b)\n",
    "\n",
    "# ---------------------------\n",
    "# Extrair o vetor [CLS] cru (posi√ß√£o 0) e mean pooling\n",
    "# ---------------------------\n",
    "cls_raw = last_hidden[:, 0, :]                            # (B, H)\n",
    "\n",
    "# mean pooling com m√°scara (ignora PAD)\n",
    "mask = enc[\"attention_mask\"].unsqueeze(-1).float()        # (B, T, 1)\n",
    "sum_tokens = (last_hidden * mask).sum(dim=1)              # (B, H)\n",
    "len_tokens = mask.sum(dim=1).clamp(min=1e-6)              # (B, 1) evita div/0\n",
    "mean_pool = sum_tokens / len_tokens                       # (B, H)\n",
    "\n",
    "# ---------------------------\n",
    "# Similaridades cosseno para comparar representa√ß√µes\n",
    "# ---------------------------\n",
    "def cos(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=-1).detach().cpu()\n",
    "\n",
    "sim_cls_pooled = cos(cls_raw, pooled_output)              # (B,)\n",
    "sim_cls_mean   = cos(cls_raw, mean_pool)                  # (B,)\n",
    "sim_pool_mean  = cos(pooled_output, mean_pool)            # (B,)\n",
    "\n",
    "# (Opcional) Reaplicar a pooler manualmente (quando dispon√≠vel) para mostrar equival√™ncia\n",
    "recomputed_ok = False\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # algumas vers√µes exp√µem a pooler como model.pooler ou model.bert.pooler\n",
    "        pool = getattr(model, \"pooler\", None) or getattr(model, \"bert\", None).pooler\n",
    "        pooled_re = pool(last_hidden)                     # (B, H)\n",
    "        diff = (pooled_re - pooled_output).abs().max().item()\n",
    "        recomputed_ok = diff < 1e-6\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------\n",
    "# Exibir resultados\n",
    "# ---------------------------\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Shapes:\")\n",
    "print(\"  last_hidden_state:\", tuple(last_hidden.shape))\n",
    "print(\"  [CLS] raw        :\", tuple(cls_raw.shape))\n",
    "print(\"  pooled_output    :\", tuple(pooled_output.shape))\n",
    "print(\"  mean_pool        :\", tuple(mean_pool.shape))\n",
    "\n",
    "print(\"\\nSimilaridades cosseno (por amostra):\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"\\nFrase {i+1}: {s}\")\n",
    "    print(f\"  cos([CLS], pooled) = {sim_cls_pooled[i]:.4f}\")\n",
    "    print(f\"  cos([CLS], mean)   = {sim_cls_mean[i]:.4f}\")\n",
    "    print(f\"  cos(pooled, mean)  = {sim_pool_mean[i]:.4f}\")\n",
    "\n",
    "if recomputed_ok:\n",
    "    print(\"\\n[OK] pooler interno reaplicado == pooled_output (diferen√ßa < 1e-6).\")\n",
    "else:\n",
    "    print(\"\\n[INFO] N√£o foi poss√≠vel (ou n√£o faz sentido nesta vers√£o) reaplicar a pooler internamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7113b0",
   "metadata": {},
   "source": [
    "## 6.3 - DistilBERT e a t√©cnica de Distila√ß√£o de Conhecimento\n",
    "\n",
    "Modelos BERT s√£o poderosos, mas muito pesados:  \n",
    "o **BERT-base** tem cerca de **110 milh√µes de par√¢metros**, exigindo grande custo de mem√≥ria e tempo de infer√™ncia.\n",
    "\n",
    "Para aplica√ß√µes em tempo real (chatbots, busca, mobile), isso √© um gargalo.  \n",
    "O **DistilBERT** foi criado como uma **vers√£o compacta do BERT**, mantendo a maior parte do desempenho com metade do tamanho.\n",
    "\n",
    "---\n",
    "\n",
    "### O que √© Distila√ß√£o de Conhecimento\n",
    "\n",
    "A ideia vem de *Knowledge Distillation* (Hinton et al., 2015):  \n",
    "transferir o ‚Äúconhecimento‚Äù de um modelo grande (*teacher*) para um modelo menor (*student*).\n",
    "\n",
    "O processo segue 3 etapas:\n",
    "\n",
    "1. **Treinar o professor** (ex.: BERT-base) normalmente.\n",
    "2. **Treinar o aluno** (DistilBERT) usando:\n",
    "   - As **sa√≠das reais** do professor (probabilidades sobre as classes ou tokens);\n",
    "   - E as **sa√≠das intermedi√°rias** (como embeddings e aten√ß√µes) para que o aluno aprenda a imit√°-las.\n",
    "\n",
    "A perda total combina tr√™s termos:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\alpha_{\\text{soft}} \\cdot \\text{CE}(y_{\\text{teacher}}, y_{\\text{student}}) + \\alpha_{\\\\text{hard}} \\cdot \\text{CE}(y_{\\text{true}}, y_{\\text{student}}) + \\alpha_{\\text{hidden}} \\cdot \\|h_{\\text{teacher}} - h_{\\text{student}}\\|^2\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- **soft loss** ‚Üí distila√ß√£o entre distribui√ß√µes suavizadas (`softmax` com temperatura $begin:math:text$T>1$end:math:text$);  \n",
    "- **hard loss** ‚Üí erro normal de predi√ß√£o;  \n",
    "- **hidden loss** ‚Üí aproxima√ß√£o entre embeddings internos.\n",
    "\n",
    "---\n",
    "\n",
    "### Entendendo a fun√ß√£o de perda do DistilBERT\n",
    "\n",
    "A distila√ß√£o de conhecimento combina **tr√™s tipos de aprendizado** ‚Äî supervisionado, por imita√ß√£o e estrutural ‚Äî em uma √∫nica fun√ß√£o de custo.\n",
    "\n",
    "A fun√ß√£o total √©:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} =\n",
    "\\alpha_{\\text{soft}} \\cdot \\text{CE}(y_{\\text{teacher}}, y_{\\text{student}})\n",
    "+ \\alpha_{\\text{hard}} \\cdot \\text{CE}(y_{\\text{true}}, y_{\\text{student}})\n",
    "+ \\alpha_{\\text{hidden}} \\cdot \\|h_{\\text{teacher}} - h_{\\text{student}}\\|^2\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- **CE** = *Cross-Entropy Loss*  \n",
    "- $y_{\\text{teacher}}$: sa√≠da (probabilidades) do modelo professor  \n",
    "- $y_{\\text{student}}$: sa√≠da (probabilidades) do modelo aluno  \n",
    "- $y_{\\text{true}}$: r√≥tulo real  \n",
    "- $h_{\\text{teacher}}, h_{\\text{student}}$: vetores das camadas internas (*hidden states*)  \n",
    "- $\\alpha_{\\text{soft}}, \\alpha_{\\text{hard}}, \\alpha_{\\text{hidden}}$: pesos de cada termo  \n",
    "\n",
    "---\n",
    "\n",
    "#### Primeiro termo ‚Äî *Soft Loss* (Imita√ß√£o do Professor)\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{soft}} \\cdot \\text{CE}(y_{\\text{teacher}}, y_{\\text{student}})\n",
    "$$\n",
    "\n",
    "O aluno aprende a **imitar a distribui√ß√£o de probabilidades** do professor, e n√£o apenas o r√≥tulo final.\n",
    "\n",
    "O *teacher* gera uma distribui√ß√£o de probabilidades sobre o vocabul√°rio (via *softmax*).  \n",
    "Essas probabilidades s√£o ‚Äúsuavizadas‚Äù com uma **temperatura $T > 1$**:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}\n",
    "$$\n",
    "\n",
    "Valores maiores de $T$ tornam a distribui√ß√£o menos ‚Äúdura‚Äù, expondo mais *informa√ß√£o relacional* ‚Äî por exemplo, o professor mostra que \"√≥timo\" e \"excelente\" s√£o parecidos, mas \"p√©ssimo\" √© muito diferente.\n",
    "\n",
    "Assim, o aluno aprende:\n",
    "> ‚Äúcomo o professor pensa‚Äù, n√£o apenas ‚Äúqual classe ele escolheu‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "#### Segundo termo ‚Äî *Hard Loss* (Supervis√£o tradicional)\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{hard}} \\cdot \\text{CE}(y_{\\text{true}}, y_{\\text{student}})\n",
    "$$\n",
    "\n",
    "√â a **perda normal de classifica√ß√£o**, usando os r√≥tulos verdadeiros do dataset.  \n",
    "Esse termo garante que o aluno continue aprendendo a tarefa original enquanto imita o professor.\n",
    "\n",
    "---\n",
    "\n",
    "#### Terceiro termo ‚Äî *Hidden-State Alignment Loss*\n",
    "\n",
    "$$\n",
    "\\alpha_{\\text{hidden}} \\cdot \\|h_{\\text{teacher}} - h_{\\text{student}}\\|^2\n",
    "$$\n",
    "\n",
    "Al√©m de copiar as sa√≠das finais, o DistilBERT tamb√©m aprende a **replicar as representa√ß√µes internas** do BERT.\n",
    "\n",
    "Durante o pr√©-treinamento:\n",
    "- as camadas do aluno s√£o alinhadas com camadas equivalentes do professor;\n",
    "- o aluno tenta minimizar a **dist√¢ncia L2** entre embeddings correspondentes.\n",
    "\n",
    "Esse termo faz o aluno ‚Äúpensar‚Äù de maneira parecida, camada a camada.\n",
    "\n",
    "---\n",
    "\n",
    "#### Combinando os termos\n",
    "\n",
    "| Termo | Tipo de aprendizado | Papel no treino |\n",
    "|--------|---------------------|-----------------|\n",
    "| $\\alpha_{\\text{soft}} \\cdot CE(y_t, y_s)$ | Imita√ß√£o (*distila√ß√£o*) | Fazer o aluno reproduzir o comportamento do professor |\n",
    "| $\\alpha_{\\text{hard}} \\cdot CE(y_{true}, y_s)$ | Supervisionado | Garantir que o aluno continue resolvendo a tarefa original |\n",
    "| $\\alpha_{\\text{hidden}} \\cdot \\|h_t - h_s\\|^2$ | Estrutural | Fazer o aluno representar internamente o conhecimento do professor |\n",
    "\n",
    "\n",
    "```text\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  Teacher    ‚îÇ\n",
    "        ‚îÇ (BERT-base) ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             ‚îÇ  y_teacher, h_teacher\n",
    "             ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  Student    ‚îÇ\n",
    "        ‚îÇ (DistilBERT)‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ 3 perdas combinadas:                 ‚îÇ\n",
    "     ‚îÇ   1) Soft ‚Üí imitar distribui√ß√µes     ‚îÇ\n",
    "     ‚îÇ   2) Hard ‚Üí prever r√≥tulo correto    ‚îÇ\n",
    "     ‚îÇ   3) Hidden ‚Üí copiar representa√ß√µes  ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Valores t√≠picos\n",
    "\n",
    "No treinamento original do DistilBERT (Sanh et al., 2019):\n",
    "\n",
    "- $T = 2.0$  \n",
    "- $\\alpha_{\\text{soft}} = 0.5$  \n",
    "- $\\alpha_{\\text{hard}} = 0.5$  \n",
    "- $\\alpha_{\\text{hidden}} = 1.0$\n",
    "\n",
    "Esses valores equilibram **imita√ß√£o** e **fidelidade √† tarefa**.\n",
    "\n",
    "#### Em resumo...\n",
    "\n",
    "> O DistilBERT aprende n√£o apenas com **respostas finais**, mas com **o racioc√≠nio interno do professor**.  \n",
    "> Ele tenta ser um aluno mais r√°pido, mas com o mesmo ‚Äújeito de pensar‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### Como o DistilBERT √© treinado\n",
    "\n",
    "O DistilBERT (Sanh et al., 2019) segue esta configura√ß√£o:\n",
    "\n",
    "| Item | BERT-base | DistilBERT |\n",
    "|------|------------|------------|\n",
    "| Camadas (encoder) | 12 | 6 |\n",
    "| Cabe√ßas de aten√ß√£o | 12 | 12 |\n",
    "| Hidden size | 768 | 768 |\n",
    "| Par√¢metros | 110M | 66M |\n",
    "| Velocidade | ‚Äî | 60% mais r√°pido |\n",
    "| Tamanho | ‚Äî | 40% menor |\n",
    "\n",
    "O **student** √© inicializado com **camadas alternadas do teacher**:  \n",
    "as camadas 2, 4, 6, 8, 10, 12 do BERT s√£o copiadas.\n",
    "\n",
    "Durante o pr√©-treinamento:\n",
    "- o *teacher* (BERT-base) fica congelado;\n",
    "- o *student* aprende:\n",
    "  - **Masked Language Modeling (MLM)**,  \n",
    "  - **Distila√ß√£o de logits** (soft targets do teacher),  \n",
    "  - e **similaridade entre estados ocultos**.\n",
    "\n",
    "O DistilBERT **n√£o possui o token [CLS] pooler nem a cabe√ßa NSP (Next Sentence Prediction)**.  \n",
    "Ou seja, ele √© otimizado apenas para o objetivo de **MLM + distila√ß√£o**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Desempenho\n",
    "\n",
    "Mesmo com metade das camadas, o DistilBERT mant√©m cerca de **97% da acur√°cia do BERT-base** em benchmarks como GLUE, com:\n",
    "- 40% menos par√¢metros,\n",
    "- 60% menos custo computacional,\n",
    "- 2√ó mais r√°pido na infer√™ncia.\n",
    "\n",
    "```text\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ               BERT-base                  ‚îÇ\n",
    "        ‚îÇ 12 camadas, 110M params (teacher)        ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ              ‚îÇ               ‚îÇ\n",
    "                ‚ñº              ‚ñº               ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ              DistilBERT                  ‚îÇ\n",
    "        ‚îÇ 6 camadas, 66M params (student)          ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚Üë aprende com ‚Üì\n",
    "     (soft logits + hidden states + labels reais)\n",
    "```\n",
    "\n",
    "- **DistilBERT = BERT menor + mesmo vocabul√°rio + sem NSP**\n",
    "- **Treinado com distila√ß√£o de conhecimento**\n",
    "- **Resultado:** mais leve, mais r√°pido, quase mesma performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d23c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fine-tuning DistilBERT para Classifica√ß√£o de Texto\n",
    "# ============================================================\n",
    "import os, math, random, inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (Colab) instalar depend√™ncias\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -U \"transformers>=4.39\" \"datasets>=2.14\" \"accelerate>=0.28\" \"evaluate>=0.4\" \"pandas>=1.5\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Imports principais\n",
    "# ------------------------------------------------------------\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "# Helper: TrainingArguments compat√≠vel com sua vers√£o\n",
    "def build_training_arguments(**kwargs) -> TrainingArguments:\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    allowed = set(sig.parameters.keys()); allowed.discard(\"self\")\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n",
    "    dropped = [k for k in kwargs if k not in allowed]\n",
    "    if dropped:\n",
    "        print(\"[Aviso] par√¢metros ignorados nesta vers√£o:\", dropped)\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Escolha da FONTE DE DADOS (selecione UMA)\n",
    "# ------------------------------------------------------------\n",
    "# (A) Dataset Hugging Face (deve ter colunas text/label ou similares)\n",
    "HF_DATASET = \"yelp_polarity\"  # ou \"imdb\", \"ag_news\", etc. ou None\n",
    "HF_CONFIG  = None\n",
    "\n",
    "# (B) CSVs locais com colunas: text,label\n",
    "CSV_TRAIN = None  # ex: \"/content/train.csv\"\n",
    "CSV_VAL   = None  # ex: \"/content/val.csv\"\n",
    "\n",
    "# (C) Fallback did√°tico (PT/EN misto, s√≥ p/ demo)\n",
    "FALLBACK = [\n",
    "    (\"o filme √© excelente e muito bem dirigido\", 1),\n",
    "    (\"p√©ssimo atendimento, n√£o volto mais\", 0),\n",
    "    (\"the product is amazing and works great\", 1),\n",
    "    (\"awful experience, totally disappointed\", 0),\n",
    "    (\"servi√ßo r√°pido e eficiente, gostei\", 1),\n",
    "    (\"interface confusa e cheia de bugs\", 0),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Carregar dados\n",
    "# ------------------------------------------------------------\n",
    "train_texts, train_labels = [], []\n",
    "val_texts,   val_labels   = [], []\n",
    "\n",
    "def load_from_hf(name, config=None):\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(name, config) if config else load_dataset(name)\n",
    "    # tenta detectar colunas\n",
    "    def pick_cols(split):\n",
    "        cols = ds[split].column_names\n",
    "        cand_text = [c for c in [\"text\", \"sentence\", \"review\", \"content\"] if c in cols] or [cols[0]]\n",
    "        cand_label= [c for c in [\"label\", \"labels\", \"sentiment\", \"stars\"] if c in cols] or [cols[1]]\n",
    "        return cand_text[0], cand_label[0]\n",
    "    t_tr, l_tr = pick_cols(\"train\")\n",
    "    t_va, l_va = pick_cols(\"test\") if \"validation\" not in ds else pick_cols(\"validation\")\n",
    "    Xtr = ds[\"train\"][t_tr];  Ytr = ds[\"train\"][l_tr]\n",
    "    Xva = ds[\"test\"][t_va] if \"validation\" not in ds else ds[\"validation\"][t_va]\n",
    "    Yva = ds[\"test\"][l_va] if \"validation\" not in ds else ds[\"validation\"][l_va]\n",
    "    return list(Xtr), list(Ytr), list(Xva), list(Yva)\n",
    "\n",
    "def load_from_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    assert \"text\" in df.columns and \"label\" in df.columns, f\"O CSV {path} deve ter colunas: text,label\"\n",
    "    return df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    "\n",
    "try:\n",
    "    if HF_DATASET:\n",
    "        print(f\"Carregando dataset HF: {HF_DATASET} ({HF_CONFIG})\")\n",
    "        train_texts, train_labels, val_texts, val_labels = load_from_hf(HF_DATASET, HF_CONFIG)\n",
    "    elif CSV_TRAIN and CSV_VAL:\n",
    "        print(\"Carregando CSVs locais‚Ä¶\")\n",
    "        train_texts, train_labels = load_from_csv(CSV_TRAIN)\n",
    "        val_texts,   val_labels   = load_from_csv(CSV_VAL)\n",
    "    else:\n",
    "        print(\"Usando fallback did√°tico embutido.\")\n",
    "        pairs = FALLBACK[:]\n",
    "        random.shuffle(pairs)\n",
    "        n = int(0.7 * len(pairs))\n",
    "        tr, va = pairs[:n], pairs[n:]\n",
    "        train_texts = [t for t,_ in tr]; train_labels = [y for _,y in tr]\n",
    "        val_texts   = [t for t,_ in va]; val_labels   = [y for _,y in va]\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Falha ao carregar dados: {e}\")\n",
    "\n",
    "print(f\"Tamanho: train={len(train_texts)}  val={len(val_texts)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Saneamento (garante list[str], remove NaN/None/bytes) + map de labels\n",
    "# ------------------------------------------------------------\n",
    "def _is_nan(x):\n",
    "    try: return bool(np.isnan(x))\n",
    "    except Exception: return False\n",
    "\n",
    "def to_str(x):\n",
    "    if x is None: return None\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        try: x = x.decode(\"utf-8\", \"ignore\")\n",
    "        except Exception: x = str(x)\n",
    "    if isinstance(x, (np.generic,)): x = x.item()\n",
    "    if isinstance(x, (float, np.floating)) and _is_nan(x): return None\n",
    "    s = str(x).strip()\n",
    "    return s if s else None\n",
    "\n",
    "def clean_xy(X, y, name=\"split\"):\n",
    "    Xo, yo = [], []\n",
    "    bad = 0\n",
    "    for t, l in zip(X, y):\n",
    "        s = to_str(t)\n",
    "        if s is None: bad += 1; continue\n",
    "        Xo.append(s); yo.append(l)\n",
    "    if bad: print(f\"[{name}] {bad} amostras removidas por texto inv√°lido.\")\n",
    "    return Xo, yo\n",
    "\n",
    "train_texts, train_labels = clean_xy(train_texts, train_labels, \"train\")\n",
    "val_texts,   val_labels   = clean_xy(val_texts,   val_labels,   \"val\")\n",
    "\n",
    "# Mapeia labels (strings ‚Üí ids)\n",
    "uniq = sorted({str(l) for l in (list(train_labels)+list(val_labels))})\n",
    "label2id = {lab:i for i,lab in enumerate(uniq)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "train_labels = [label2id[str(l)] for l in train_labels]\n",
    "val_labels   = [label2id[str(l)] for l in val_labels]\n",
    "num_labels = len(label2id)\n",
    "print(\"Labels:\", label2id)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Tokenizer (DistilBERT) e tokeniza√ß√£o\n",
    "# ------------------------------------------------------------\n",
    "MODEL_NAME = \"distilbert-base-uncased\"    # troque aqui se quiser outro DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 160\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_enc = tokenize_batch(train_texts)\n",
    "val_enc   = tokenize_batch(val_texts)\n",
    "\n",
    "class TorchTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc, labels):\n",
    "        self.enc = enc; self.labels = labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TorchTextDataset(train_enc, train_labels)\n",
    "val_ds   = TorchTextDataset(val_enc,   val_labels)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Modelo\n",
    "# ------------------------------------------------------------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) M√©tricas (accuracy + F1 se dispon√≠vel)\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    import evaluate\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric  = evaluate.load(\"f1\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        r1 = acc_metric.compute(predictions=preds, references=labels)\n",
    "        r2 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "        return {\"accuracy\": r1[\"accuracy\"], \"f1\": r2[\"f1\"]}\n",
    "except Exception:\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = (preds == labels).mean()\n",
    "        return {\"accuracy\": float(acc)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Treinamento (Trainer)\n",
    "# ------------------------------------------------------------\n",
    "EPOCHS = 2 if len(train_texts) > 1000 else 4\n",
    "BATCH  = 16 if torch.cuda.is_available() else 8\n",
    "\n",
    "args = build_training_arguments(\n",
    "    output_dir=\"distilbert-cls\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=3e-5,          # DistilBERT costuma aceitar 3e-5 bem\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\n=== Iniciando fine-tuning do DistilBERT ===\")\n",
    "trainer.train()\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\nResultados de valida√ß√£o:\", eval_out)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Infer√™ncia em frases\n",
    "# ------------------------------------------------------------\n",
    "def predict(texts):\n",
    "    model.eval()\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        probs = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    decoded = [(t, id2label[int(p)], probs[i]) for i,(t,p) in enumerate(zip(texts, preds))]\n",
    "    return decoded\n",
    "\n",
    "samples = [\n",
    "    \"The product quality is amazing and the delivery was fast.\",\n",
    "    \"Horrible support. I will never buy again.\",\n",
    "    \"servi√ßo excelente e muito r√°pido, recomendo\",\n",
    "    \"n√£o gostei, veio com defeito.\"\n",
    "]\n",
    "for texto, pred, prob in predict(samples):\n",
    "    print(f\"- {texto}\\n  -> classe: {pred} | probs={np.round(prob, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe100c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
