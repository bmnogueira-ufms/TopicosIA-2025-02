{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe54817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.57.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n",
      "---\n",
      "Name: torch\n",
      "Version: 2.2.2\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, openai-whisper, sentence-transformers, torchaudio\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5287ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/bruno/opt/anaconda3/envs/env-misc/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ee63e",
   "metadata": {},
   "source": [
    "# 4.7 — Transformers (Parte 7): GPT e Modelos Autoregressivos (*decoder-only*)\n",
    "\n",
    "**Objetivos da aula**\n",
    "- Entender o paradigma *decoder-only* e como ele difere do *encoder-only* (BERT).\n",
    "- Compreender a **modelagem de linguagem autoregressiva**: prever o próximo token.\n",
    "- Explorar a **família GPT** (GPT, GPT-2, GPT-3, GPT-4 e open-source “GPT-like”).\n",
    "- Fazer **inferência** com GPT-2 (Hugging Face) e **controlar a geração** (temperatura, top-k, top-p).\n",
    "- Realizar um **mini fine-tuning** para *causal language modeling* (CLM).\n",
    "- Medir *loss* e **perplexity** e discutir limitações, riscos e boas práticas.\n",
    "\n",
    "**Pré-requisitos**\n",
    "- Conceitos básicos de Transformers, *self-attention* e *positional encoding*.\n",
    "- Familiaridade com BERT e fine-tuning usando `transformers` (Hugging Face)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d45f5d",
   "metadata": {},
   "source": [
    "## 1) Relembrando... De BERT ao GPT: a virada dos modelos generativos\n",
    "\n",
    "Os modelos baseados em **Transformers** revolucionaram o processamento de linguagem natural ao introduzirem a ideia de atenção paralela entre todos os tokens de uma sequência.  \n",
    "Entretanto, as primeiras grandes implementações — como **BERT** (2018) — tinham um foco principal em **compreensão** de texto, e não em **geração**.\n",
    "\n",
    "---\n",
    "\n",
    "### O papel do BERT e suas limitações\n",
    "\n",
    "O **BERT (Bidirectional Encoder Representations from Transformers)** foi projetado como um **modelo encoder-only**:\n",
    "- Treinado com o objetivo de **Masked Language Modeling (MLM)**, em que partes da sentença são mascaradas e o modelo precisa prever as palavras ocultas.  \n",
    "- Seu aprendizado é **bidirecional**, ou seja, cada token “enxerga” tanto o passado quanto o futuro da sequência.  \n",
    "- Essa característica torna o BERT excelente para **tarefas de interpretação**, como:\n",
    "  - classificação de textos,  \n",
    "  - análise de sentimentos,  \n",
    "  - reconhecimento de entidades nomeadas,  \n",
    "  - perguntas e respostas (*extractive QA*).\n",
    "\n",
    "Apesar de sua enorme contribuição, o BERT apresentava **restrições claras**:\n",
    "- Não podia ser usado naturalmente para **gerar texto contínuo** (como escrever, traduzir ou responder perguntas abertas).  \n",
    "- O processo de *masking* dificultava o uso em aplicações **autoregressivas** — ou seja, prever o próximo token em tempo real.  \n",
    "- Cada tarefa exigia um **cabeçalho (head) especializado**, dificultando a generalização e o reuso direto do modelo em contextos diferentes.\n",
    "\n",
    "---\n",
    "\n",
    "### A motivação para os modelos GPT\n",
    "\n",
    "O **GPT (Generative Pre-trained Transformer)** nasceu como resposta direta a essas limitações.  \n",
    "Enquanto o BERT se baseava em **atenção bidirecional (encoder)**, o GPT inverteu a lógica e apostou em **atenção causal unidirecional (decoder)**.\n",
    "\n",
    "A mudança de paradigma trouxe ganhos decisivos:\n",
    "- O modelo passou a **gerar texto palavra a palavra**, aprendendo a prever o **próximo token** em vez de preencher lacunas.  \n",
    "- Isso o tornou naturalmente adequado para **tarefas abertas e criativas**, como diálogo, resumo, tradução e redação assistida.  \n",
    "- A mesma arquitetura poderia agora **aprender uma variedade de tarefas supervisionadas** apenas reformatando o texto de entrada — sem alterar a estrutura interna.\n",
    "\n",
    "Em resumo:\n",
    "\n",
    "| Aspecto | BERT (Encoder-only) | GPT (Decoder-only) |\n",
    "|:--|:--|:--|\n",
    "| Tipo de atenção | Bidirecional | Causal (unidirecional) |\n",
    "| Objetivo de treino | Prever tokens mascarados (MLM) | Prever o próximo token (LM) |\n",
    "| Tipo de tarefa | Compreensão (análise, classificação) | Geração (continuação, síntese) |\n",
    "| Direção da informação | Passado + futuro | Somente passado |\n",
    "| Saída típica | Vetores de representação | Texto gerado |\n",
    "\n",
    "\n",
    "O **GPT surge como uma evolução natural do Transformer**, voltando às suas raízes *decoder-only*.  \n",
    "Ele herda a eficiência do mecanismo de atenção introduzido no BERT, mas a aplica de forma **autoregressiva**, permitindo **geração coerente, contextual e adaptável**.  \n",
    "Esse movimento marca a transição dos modelos **discriminativos** (como BERT) para os **generativos**, pavimentando o caminho para a era dos **Large Language Models** (GPT-2, GPT-3, ChatGPT, LLaMA, Claude, Gemini, entre outros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012bef2f",
   "metadata": {},
   "source": [
    "### 1.1) Encoder-only (BERT) vs Decoder-only (GPT)\n",
    "\n",
    "**BERT (encoder-only)**:\n",
    "- Atenção bidirecional (*masked language modeling*, MLM).\n",
    "- Aprende representações profundas para **compreensão** (classificação, NER, QA com *span extraction*).\n",
    "- Treinamento: prever **tokens mascarados** + (opcional) *Next Sentence Prediction (NSP)*.\n",
    "\n",
    "**GPT (decoder-only)**:\n",
    "- Atenção **causal** (unidirecional): cada posição só “vê” tokens anteriores.\n",
    "- Treinamento **autoregressivo**: prever o **próximo token**.\n",
    "- Excelente para **geração** de texto, continuação de prompts e *in-context learning*.\n",
    "\n",
    "Em termos de probabilidade:\n",
    "$$\n",
    "P(w_1,\\dots,w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1,\\dots,w_{t-1})\n",
    "$$\n",
    "No GPT, modelamos $P(w_t \\mid w_{<t})$ e aplicamos *teacher forcing* durante o treino (prever o próximo token com base no prefixo correto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e045ca8",
   "metadata": {},
   "source": [
    "## 2) Arquitetura GPT (Decoder-only) e Máscara Causal\n",
    "\n",
    "**Bloco GPT:**\n",
    "- *Embedding* de tokens + **positional embeddings** aprendíveis.\n",
    "- Pilha de camadas *decoder*: \n",
    "  - *Masked self-attention* (atenção causal com máscara triangular superior).\n",
    "  - *Feed-forward network* (MLP com GELU).\n",
    "  - *Residual connections* + *LayerNorm* (normalização por camada).\n",
    "- Projeção final para o vocabulário (logits).\n",
    "\n",
    "**Máscara causal**: impede “vazamento” de informações do futuro.\n",
    "- Se o token está na posição *t*, ele só pode atender (atender = *attend*) às posições $\\leq t$\\)$.\n",
    "- Implementada como uma matriz triangular **superior** com $-\\infty$ onde não pode atender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aadab5",
   "metadata": {},
   "source": [
    "## 2.1) Arquitetura do GPT-1 e fine-tuning supervisionado\n",
    "\n",
    "A figura abaixo ilustra a **arquitetura original do GPT-1** (2018) e as formas como o modelo foi ajustado para diferentes tarefas supervisionadas.  \n",
    "O GPT-1 foi o primeiro modelo a demonstrar que um **Transformer *decoder-only***, pré-treinado com **modelagem de linguagem autoregressiva**, podia ser reutilizado para várias tarefas de NLP — bastando estruturar a entrada textual adequadamente e adicionar uma **camada linear de classificação** sobre o último token.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/bmnogueira-ufms/TopicosIA-2025-02/main/images/gpt_arquitetura.png\" width=\"50%%\">\n",
    "  Fonte: Kamath et al. (2024)\n",
    "</p>\n",
    "\n",
    "**Arquitetura do GPT:**\n",
    "- O modelo GPT-1 consiste em **12 blocos *decoder*** empilhados, cada um contendo:\n",
    "  - *Masked Multi-Head Self-Attention* (atenção causal),\n",
    "  - camadas *Feed-Forward* com ativação GELU,\n",
    "  - *Layer Normalization* e conexões residuais.\n",
    "- Na base, os tokens são convertidos em vetores de embedding somando:\n",
    "  - **Embeddings de palavras** (aprendidos),\n",
    "  - **Embeddings de posição** (também aprendidos).\n",
    "\n",
    "**Fine-tuning supervisionado:**\n",
    "Durante o ajuste para tarefas específicas, o texto de entrada é formatado conforme a natureza da tarefa:\n",
    "\n",
    "| Tarefa | Estrutura de entrada | Saída gerada |\n",
    "|:--|:--|:--|\n",
    "| **Classificação** | `Start ⟶ Text ⟶ Extract` | Predição linear no token `Extract` |\n",
    "| **Entailment / Inferência textual** | `Start ⟶ Premise ⟶ Delim ⟶ Hypothesis ⟶ Extract` | Classificação no token final |\n",
    "| **Similaridade textual** | `Start ⟶ Text₁ ⟶ Delim ⟶ Text₂ ⟶ Extract` | Combinação de embeddings antes do `Linear` |\n",
    "| **Múltipla escolha** | `Start ⟶ Context ⟶ Delim ⟶ Answer₁…N ⟶ Extract` | Cada resposta passa por uma *linear layer*, e a pontuação é normalizada via *softmax* |\n",
    "\n",
    "> **Ideia central:** o modelo não precisa de arquitetura diferente para cada tarefa — apenas de um *prompt* ou sequência de entrada bem estruturada.  \n",
    "> Esse conceito inspirou o *paradigma de instruções* e o *prompt-based learning* que evoluíram nas famílias GPT-2, GPT-3 e ChatGPT.\n",
    "\n",
    "#### Tokens especiais\n",
    "\n",
    "O GPT-1 utiliza três tipos de marcadores auxiliares:\n",
    "\n",
    "| Token | Significado | Função |\n",
    "|:--|:--|:--|\n",
    "| **Start** | Marcador inicial da sequência | Indica o início do exemplo e ajuda o modelo a delimitar o contexto. |\n",
    "| **Delim** | Delimitador | Separa partes distintas da entrada (por exemplo, “premissa” e “hipótese”). |\n",
    "| **Extract** | Token de extração | Indica o ponto em que a predição deve ser feita (posição do *head* linear). |\n",
    "\n",
    "A **camada linear final** (`Linear`) é conectada ao vetor de saída correspondente ao token **Extract**, sobre o qual se aplica uma operação de *softmax* para gerar as probabilidades das classes.\n",
    "\n",
    "O GPT-1 não precisava de cabeçalhos nem módulos especializados para cada tarefa.  \n",
    "Ele apenas aprendia **a continuar uma sequência textual estruturada**.  \n",
    "Esse conceito deu origem ao **prompt-based learning** e, mais tarde, ao **in-context learning** dos modelos GPT-3 e ChatGPT.\n",
    "\n",
    "#### Task Classifier e Test Prediction\n",
    "\n",
    "Durante o **fine-tuning supervisionado**, o modelo pré-treinado precisa se adaptar a uma tarefa específica.  \n",
    "Para isso, adicionamos e utilizamos duas componentes fundamentais: o **task classifier** e o **test prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "##### Task Classifier\n",
    "\n",
    "O **task classifier** é uma camada adicional (geralmente linear + *softmax*) conectada à saída do modelo Transformer.  \n",
    "Seu papel é transformar as **representações contextuais** produzidas pelo modelo em **predições específicas da tarefa**.\n",
    "\n",
    "| Exemplo de tarefa | Entrada | Saída do *task classifier* |\n",
    "|:--|:--|:--|\n",
    "| Classificação de sentimento | Texto completo | Positivo / Negativo |\n",
    "| NER (Reconhecimento de Entidades) | Sentença tokenizada | Tag por token (`B-PER`, `I-ORG`, etc.) |\n",
    "| Inferência textual | Premissa + Hipótese | Entailment / Contradiction / Neutral |\n",
    "\n",
    "No **BERT**, essa camada é aplicada sobre o vetor do token `[CLS]`.  \n",
    "No **GPT**, como o modelo já é autoregressivo, a saída final do último token (*Extract* ou *último hidden state*) é usada como entrada para o classificador.\n",
    "\n",
    "---\n",
    "\n",
    "##### Test Prediction\n",
    "\n",
    "O termo **test prediction** refere-se à **etapa de inferência ou avaliação**, quando o modelo — já treinado — é aplicado ao conjunto de teste.\n",
    "\n",
    "Durante essa fase:\n",
    "1. As entradas são processadas pelo Transformer sem atualização de pesos.  \n",
    "2. O *task classifier* transforma as ativações do modelo em rótulos ou probabilidades.  \n",
    "3. As predições são comparadas com as verdadeiras para calcular métricas como *accuracy*, *F1-score* ou *perplexity*.\n",
    "\n",
    "---\n",
    "\n",
    "##### Diferença essencial\n",
    "\n",
    "| Etapa | Função | Ocorrência |\n",
    "|:--|:--|:--|\n",
    "| **Task Classifier** | Transforma as representações internas em saídas específicas da tarefa | Durante o *fine-tuning* |\n",
    "| **Test Prediction** | Gera as predições finais sobre dados de teste | Durante a avaliação (inferência) |\n",
    "\n",
    "Em resumo, o **task classifier** é o mecanismo de saída que “traduz” o conhecimento do modelo,  \n",
    "enquanto o **test prediction** é o momento em que verificamos **o quanto ele aprendeu de fato**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Procedimento de treinamento (GPT-1)\n",
    "\n",
    "O GPT-1 foi treinado em **duas fases**:\n",
    "\n",
    "1. **Pré-treinamento não supervisionado** (*unsupervised pre-training*):  \n",
    "   - O modelo aprende a prever o próximo token em uma sequência de texto grande, segundo o objetivo de *Language Modeling*:\n",
    "     $$\n",
    "     \\mathcal{L}_1(U) = \\sum_i \\log P(u_i \\mid u_{i-k}, ..., u_{i-1}; \\Theta)\n",
    "     $$\n",
    "     onde $U = $u_1, ..., u_n$$ é o corpus não supervisionado, $k$ o tamanho do contexto e $\\Theta$ os parâmetros do modelo.\n",
    "\n",
    "2. **Fine-tuning supervisionado** (*supervised fine-tuning*):  \n",
    "   - Após o pré-treinamento, o modelo é adaptado a tarefas específicas com dados rotulados $C = (x, y)$:\n",
    "     $$\n",
    "     P(y \\mid x_1, ..., x_m) = \\text{softmax}(h_m W_y)\n",
    "     $$\n",
    "     e o objetivo supervisionado é:\n",
    "     $$\n",
    "     \\mathcal{L}_2(C) = \\sum_{(x,y)} \\log P(y \\mid x_1, ..., x_m)\n",
    "     $$\n",
    "   - Durante o fine-tuning, o uso do *Language Modeling* como objetivo auxiliar melhora a generalização:\n",
    "     $$\n",
    "     \\mathcal{L}_3 = \\mathcal{L}_2(C) + \\lambda \\times \\mathcal{L}_1(U)\n",
    "     $$\n",
    "\n",
    "> **Conclusão:** O GPT-1 demonstrou que o *pre-training + fine-tuning* em um modelo gerativo era suficiente para atingir *transfer learning* em várias tarefas de NLP — conceito que viria a dominar o campo nos anos seguintes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 GPT-2 — Escalando a arquitetura\n",
    "\n",
    "O **GPT-2** (2019) manteve o mesmo design arquitetural, mas:\n",
    "- Expandiu de **117M para 1.5B parâmetros** (≈10× maior).\n",
    "- Foi treinado em um corpus mais amplo e diverso (*WebText*), com foco apenas em *unsupervised language modeling*, sem fine-tuning supervisionado.\n",
    "\n",
    "Formalmente, o treinamento de GPT-2 visa estimar:\n",
    "$$\n",
    "p(\\text{output} \\mid \\text{input}, \\text{task})\n",
    "$$\n",
    "onde o modelo é condicionado tanto ao texto quanto a instruções que definem a tarefa (“traduza para o francês”, “resuma o texto”, etc.).  \n",
    "Isso permitiu que o modelo aprendesse a **generalizar entre múltiplos domínios**, aproximando-se da ideia de *multi-task learning sem rótulos explícitos*.\n",
    "\n",
    "**Principais resultados e observações:**\n",
    "- Mostrou que **aumentar parâmetros e dados** melhora desempenho quase linearmente.  \n",
    "- O GPT-2 **subajustou** (*underfitted*) o dataset WebText — indicando que ainda não havia saturação de capacidade.  \n",
    "- Superou *benchmarks* em **7 de 8 datasets de language modeling**, incluindo grande salto de desempenho no **LAMBADA** (perplexity 99.8 → 8.6).\n",
    "\n",
    "> **Conclusão:** GPT-2 provou empiricamente o valor do *scaling law* — quanto mais parâmetros e dados, melhor a generalização e menor a perplexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 GPT-3 — O paradigma do *in-context learning*\n",
    "\n",
    "O **GPT-3** (2020) manteve a base arquitetural dos anteriores, mas:\n",
    "- Escalou para **175 bilhões de parâmetros**;\n",
    "- Ampliou a **janela de contexto** (2048 tokens);\n",
    "- Foi treinado em **corpus muito maior e mais variado**, sem fine-tuning supervisionado.\n",
    "\n",
    "Em vez de ajustar pesos para cada tarefa, o GPT-3 introduziu o **aprendizado no contexto** (*in-context learning*), operando em três modos:\n",
    "\n",
    "| Tipo | Descrição | Exemplo |\n",
    "|:--|:--|:--|\n",
    "| **Zero-shot** | O modelo realiza a tarefa apenas com base na instrução textual. | “Traduza para o francês: The book is new.” |\n",
    "| **One-shot** | Um exemplo é fornecido antes da tarefa. | “Inglês → Francês: cat → chat. Dog → ?” |\n",
    "| **Few-shot** | Vários exemplos são dados no prompt. | Lista de pares de tradução antes da nova frase. |\n",
    "\n",
    "O GPT-3 demonstrou desempenho *state-of-the-art* em **múltiplas tarefas sem fine-tuning**, incluindo:\n",
    "- **QA** (perguntas e respostas);\n",
    "- **Tradução**;\n",
    "- **Compreensão de leitura**;\n",
    "- **Raciocínio básico e aritmética textual**.\n",
    "\n",
    "Porém, ainda apresentou limitações:\n",
    "- **Coerência em textos longos** e **repetição excessiva**;\n",
    "- Dificuldade em *inference tasks* (entailment, NLI);\n",
    "- Custo computacional elevado e pouca interpretabilidade.\n",
    "\n",
    "> **Risco e impacto social:** GPT-3 introduziu o desafio da geração *human-like* e o potencial de uso indevido (fake news, phishing, etc.), destacando a importância de segurança e regulação de LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Variantes e sucessores modernos\n",
    "\n",
    "A filosofia *decoder-only* do GPT influenciou uma nova geração de modelos de larga escala:\n",
    "\n",
    "| Modelo | Parâmetros | Treinamento / Dataset | Características principais |\n",
    "|:--|:--|:--|:--|\n",
    "| **Gopher (DeepMind, 2021)** | 280B | 300B tokens (MassiveText) | Superou GPT-3 em 81% de 100 tarefas. |\n",
    "| **Chinchilla (DeepMind, 2022)** | 70B | 1.4T tokens | Compute-óptimo, menor custo e melhor desempenho. |\n",
    "| **LLaMA (Meta, 2023)** | 8B–70B | Dados web, código, Wikipedia | Modelos abertos, fine-tuning + RLHF para diálogo. |\n",
    "| **Claude 3 (Anthropic, 2023)** | 20B–2T | Dados multimodais | Modelos Opus, Sonnet e Haiku: balanceiam custo e velocidade. |\n",
    "| **Command R (Cohere, 2024)** | 35B | 128k contexto | Otimizado para RAG e uso de ferramentas externas. |\n",
    "| **Gemma (Google, 2024)** | 2B–7B | 6T tokens | Foco em segurança, raciocínio e desempenho multilingue. |\n",
    "\n",
    "> **Tendência geral:** reduzir custos mantendo desempenho → *compute-optimal training*, *long-context modeling* e *instrução supervisionada* (SFT + RLHF).\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Principais lições aprendidas\n",
    "\n",
    "1. **GPT-1** provou que o *language modeling pre-training* é um poderoso mecanismo de transferência.  \n",
    "2. **GPT-2** mostrou que escalar parâmetros e dados leva a ganhos contínuos e robustos — sem saturação aparente.  \n",
    "3. **GPT-3** inaugurou o *in-context learning*, tornando possível generalização para novas tarefas apenas via *prompt*.  \n",
    "4. **Modelos subsequentes** (Gopher, Chinchilla, LLaMA, Claude, Command R, Gemma) consolidaram essa linha, equilibrando tamanho, custo e desempenho.  \n",
    "5. A evolução da série GPT definiu o paradigma atual dos **LLMs autoregressivos**, no qual *prompt design*, *instruções* e *context windows ampliadas* substituem o fine-tuning clássico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba416a",
   "metadata": {},
   "source": [
    "## 3) Família GPT (linha do tempo resumida)\n",
    "\n",
    "| Modelo  | Ano  | Parâmetros (maior variante) | Treino / Dados               | Destaques                                       |\n",
    "|---------|------|------------------------------|------------------------------|--------------------------------------------------|\n",
    "| GPT     | 2018 | ~117M                        | BooksCorpus                  | Prova de conceito de *decoder-only* autoregressivo |\n",
    "| GPT-2   | 2019 | 1.5B                         | WebText                      | Geração coerente; mostrou o poder do *scaling*   |\n",
    "| GPT-3   | 2020 | 175B                         | Diversos (web-scale)         | *Few-shot*, *in-context learning*                |\n",
    "| GPT-3.5 | 2022 | —                            | + Instruções                 | Basis para “ChatGPT” (instruções + RLHF)         |\n",
    "| GPT-4   | 2023 | —                            | Multimodal                   | Raciocínio melhor, contexto maior                |\n",
    "\n",
    "**Observações**:\n",
    "- O “salto” de desempenho surge de **mais dados, mais parâmetros e melhores instruções** (técnicas como RLHF, *system prompts*, *SFT* com *instruction-tuning*).\n",
    "- Em open-source, há modelos “GPT-like” (p.ex., GPT-Neo, GPT-J, LLaMA-based, MPT) que seguem o **paradigma decoder-only**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe26992",
   "metadata": {},
   "source": [
    "## 4) Ambiente e bibliotecas\n",
    "\n",
    "Vamos usar a biblioteca **Hugging Face Transformers** para:\n",
    "- Carregar *tokenizer* e modelo **GPT-2**.\n",
    "- Fazer **inferência** (geração) controlando temperatura, *top-k*, *top-p*.\n",
    "- Rodar um **mini fine-tuning** para CLM (Causal Language Modeling).\n",
    "\n",
    "> **Dica**: para execução mais rápida, use a variante `gpt2` (pequena). Se a máquina for modesta, reduza *max_length*, *batch_size* e *block_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108c332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install -q transformers datasets accelerate\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, GPT2TokenizerFast,\n",
    "    set_seed, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf3449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: tensor([18599, 15960,   861, 46387,   827, 13772,   259,  3455,    14]) ... tam = 9\n",
      "Decodificado: Inteligência Artificial está transformando a educação.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"gpt2\"\n",
    "model_name = \"pierreguillou/gpt2-small-portuguese\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 não tem token de padding por padrão; definimos para evitar avisos no Trainer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, use_safetensors=True)\n",
    "model.to(device)\n",
    "\n",
    "# Inspeção rápida da tokenização\n",
    "text = \"Inteligência Artificial está transformando a educação.\"\n",
    "enc = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"IDs:\", enc[\"input_ids\"][0][:10], \"... tam =\", enc[\"input_ids\"].shape[-1])\n",
    "print(\"Decodificado:\", tokenizer.decode(enc[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ab965",
   "metadata": {},
   "source": [
    "## 5) Geração autoregressiva: greedy vs amostragem\n",
    "\n",
    "**Greedy**: escolhe sempre o token de maior probabilidade.\n",
    "- Pro: mais determinístico.\n",
    "- Contra: pode “entravar” e repetir.\n",
    "\n",
    "**Amostragem com temperatura e *top-k*/*top-p***:\n",
    "- `temperature`: suaviza as probabilidades (↑ = mais aleatório).\n",
    "- `top_k`: restringe aos *k* tokens mais prováveis.\n",
    "- `top_p` (*nucleus sampling*): restringe ao menor conjunto de tokens cuja soma de probabilidades ≥ *p*.\n",
    "\n",
    "> Boas práticas: combine `temperature` com `top_p` (ou `top_k`) para controlar fluência e diversidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90658a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Greedy ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A inteligência artificial nas escolas do Brasil, como a Escola de Aperfeiçoamento de Oficiais (ECAO), a Escola de Aperfeiçoamento de Oficiais (ECAO), a Escola de Aperfeiçoamento de Oficiais (ECAO), a Escola de Aperfeiçoamento de Oficiais (ECAO), a Escola de\n",
      "\n",
      "=== Sampling (temp=0.8, top_k=50, top_p=0.9) ===\n",
      "A inteligência artificial nas escolas do Brasil, no Brasil, nos Estados Unidos, na África, na Ásia, na África, na Ásia e no mundo, é um dos pilares da estratégia militar de inteligência artificial que está sendo desenvolvida nos Estados Unidos.\n",
      "\n",
      "Os analistas militares da inteligência artificial no Brasil, na\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, **gen_kwargs):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"A inteligência artificial nas escolas do Brasil\"\n",
    "\n",
    "print(\"=== Greedy ===\")\n",
    "print(generate(prompt, max_length=60, do_sample=False))\n",
    "\n",
    "print(\"\\n=== Sampling (temp=0.8, top_k=50, top_p=0.9) ===\")\n",
    "print(generate(prompt, max_length=60, do_sample=True, temperature=0.8, top_k=50, top_p=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9187ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- temperature=0.3, top_p=0.8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que os neurônios sejam movidos para um local específico de propagação, e o tempo de propagação é medido em tempo.\n",
      "\n",
      "O tempo de propagação é medido em tempo por meio de um gráfico de tempo. O tempo de propagação é medido em tempo por meio de um gráfico de tempo.\n",
      "\n",
      "O tempo de propagação é medido em tempo por meio de um gráfico\n",
      "\n",
      "--- temperature=0.3, top_p=0.9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que os neurônios sejam movidos para uma região específica de memória, onde os neurônios podem ser movidos para uma região específica de memória.\n",
      "\n",
      "A memória é uma estrutura que é usada para armazenar informações em memória. A memória é uma estrutura que é usada para armazenar informações em memória. A memória é uma estrutura que é usada para armazenar informações em memória.\n",
      "\n",
      "\n",
      "\n",
      "--- temperature=0.3, top_p=0.95 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que as células nervosas se conectem com os neurônios e, em seguida, se comunicam através de uma rede neural.\n",
      "\n",
      "O cérebro humano é composto por neurônios, que são divididos em neurônios individuais e por neurônios individuais. Os neurônios individuais são responsáveis por processar informações em um determinado espaço de tempo, enquanto os neurônios individuais são responsáveis por processar informações em um determinado\n",
      "\n",
      "--- temperature=0.7, top_p=0.8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que o cérebro se mova de uma forma mais eficiente, de acordo com os experimentos, e que o cérebro não seja afetado por outras formas de estímulos.\n",
      "\n",
      "A primeira experiência em um computador de laboratório foi realizada em 1972, quando um IBM PC IBM PC, o IBM PC-DOS, foi usado para criar um programa de computador pessoal para o desenvolvimento\n",
      "\n",
      "--- temperature=0.7, top_p=0.9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que os neurônios vivam em um ambiente, como um ambiente de baixa tensão. Os neurônios podem ter uma capacidade limitada de processamento, e, portanto, não podem ser classificados em redes neurais, em vez de redes neurais simples.\n",
      "\n",
      "A abordagem de redes neurais para a aprendizagem e aprendizado de máquina é um dos principais campos de pesquisa da área. As redes\n",
      "\n",
      "--- temperature=0.7, top_p=0.95 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que as redes neurais locais e locais tenham uma melhor capacidade de processar informações e aprender padrões de aprendizado em tempo real.\n",
      "\n",
      "As redes neurais locais podem ser treinadas em tempo real ou simultaneidade, com diferentes níveis de detalhe, permitindo a construção de padrões de aprendizado que são mais complexos do que os padrões de aprendizado de um sistema. Além disso,\n",
      "\n",
      "--- temperature=1.0, top_p=0.8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que a rede atualize seu estado de saúde ao longo de todo o ciclo.\n",
      "\n",
      "\n",
      "As redes neurais são projetadas para armazenar informação de forma precisa para que o computador tenha tempo suficiente para aprender e reproduzir o estado de uma rede. Como a informação é transferida através de um processo de aprendizado, o sistema de decisão de qual informação a ser transmitida é\n",
      "\n",
      "--- temperature=1.0, top_p=0.9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: As redes neurais transformadoras permitem que uma rede neural que não está completamente mapeada de uma rede neural, como o cérebro, experimentasse os estados alterados por algum estímulo. As redes neurais não somente podem ser utilizadas para criar um processo que induz uma resposta, como também podem alterar o padrão de resposta para outros processos cognitivos (por exemplo, mudanças na temperatura, distância, etc.).\n",
      "\n",
      "--- temperature=1.0, top_p=0.95 ---\n",
      "Resumo: As redes neurais transformadoras permitem aos humanos detectar, interagir, orientar a ação, organizar a atividade, armazenar informações para serem usadas na forma de memória e processar os dados. A tecnologia de redes neurais artificiais foi desenvolvida pela Universidade Federal de Santa Catarina para resolver os desafios que os humanos enfrentam diante da comunicação de informações. O cérebro humano utiliza a tecnologia de Redes neurais como um meio de interagir com\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Resumo: As redes neurais transformadoras permitem\"\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "top_ps = [0.8, 0.9, 0.95]\n",
    "\n",
    "for T in temperatures:\n",
    "    for P in top_ps:\n",
    "        print(f\"\\n--- temperature={T}, top_p={P} ---\")\n",
    "        print(generate(prompt, max_length=80, do_sample=True, temperature=T, top_p=P))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37a150",
   "metadata": {},
   "source": [
    "## 6) Fine-tuning (CLM) com GPT-2 em corpus pequeno\n",
    "\n",
    "**Objetivo**: adaptar o GPT-2 para um *estilo* ou *domínio* (ex.: resumos, ementas, notícias curtas), usando **Causal Language Modeling**:\n",
    "- Treinaremos para prever o **próximo token** (mlm=False).\n",
    "- Uniremos textos e criaremos *blocos* de tamanho fixo (*block_size*), como janelas deslizantes.\n",
    "\n",
    "> **Atenção**: com corpus pequeno, espere **overfitting** e ganho limitado. Use isso como **demonstração didática**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e9c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2ca1f07514412f93d5dbb79b969607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fd9dd889494a9489facea3324313b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 35\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Resumo: Este trabalho apresenta uma introdução a modelos transformer aplicados a NLP.\",\n",
    "    \"Discussão: A arquitetura decoder-only baseia-se em atenção causal e geração autoregressiva.\",\n",
    "    \"Resultados: Observamos melhora na coerência dos textos com ajuste de temperatura e top-p.\",\n",
    "    \"Aplicações: Educação, atendimento automatizado, redação assistida e sumarização.\",\n",
    "    \"Limitações: Viés, alucinação e sensibilidade ao prompt exigem avaliação cuidadosa.\"\n",
    "] * 50  # replicamos para ter um pouco mais de dados\n",
    "\n",
    "raw_ds = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Tokenização\n",
    "block_size = 128  # diminua se estiver com pouca memória\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized = raw_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Juntar tokens em blocos contíguos (causal LM)\n",
    "def group_texts(examples):\n",
    "    # concatena e corta em blocos de block_size\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_ds = tokenized.map(group_texts, batched=True)\n",
    "lm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d2714d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4g/hm9d3pn57nv1p17qlv8k47mc0000gn/T/ipykernel_63881/2718657328.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x1af466810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"runs-gpt2-clm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"runs-gpt2-clm\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=1,          # didático; aumente com GPU/dados\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     warmup_ratio=0.05,\n",
    "#     logging_steps=20,\n",
    "#     save_strategy=\"no\",\n",
    "#     evaluation_strategy=\"no\",    # poderia separar um split de validação para eval\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f279f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 0, 'bos_token_id': 0, 'pad_token_id': 0}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=3.501142289903429, metrics={'train_runtime': 60.522, 'train_samples_per_second': 0.578, 'train_steps_per_second': 0.149, 'total_flos': 2286305280000.0, 'train_loss': 3.501142289903429, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "debf786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss final (train): 3.5011\n",
      "Perplexity aproximada: 33.15\n"
     ]
    }
   ],
   "source": [
    "# Avaliação simples no próprio treino (didático): calcula perplexity a partir do loss\n",
    "final_loss = train_result.training_loss\n",
    "perplexity = math.exp(final_loss) if final_loss < 20 else float(\"inf\")\n",
    "print(f\"Loss final (train): {final_loss:.4f}\")\n",
    "print(f\"Perplexity aproximada: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Resumo:\"\n",
    "# print(\"=== Antes do ajuste (baseline pode variar) ===\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# model.to(device)\n",
    "# print(generate(prompt, max_length=60, do_sample=True, temperature=0.8, top_p=0.9))\n",
    "print(\"\\n=== Depois do fine-tuning ===\")\n",
    "print(generate(prompt, max_length=60, do_sample=True, temperature=0.8, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82460c",
   "metadata": {},
   "source": [
    "## 7) Boas práticas, riscos e limitações\n",
    "\n",
    "- **Overfitting** com pouco dado: o modelo memoriza padrões; use *early stopping*, *regularização*, *data augmentation*.\n",
    "- **Viés e alucinação**: modelos autoregressivos podem inventar fatos; sempre valide saídas.\n",
    "- **Controle de geração**: ajuste `temperature`, `top_p`/`top_k` e **comprimento máximo** para balancear coerência/diversidade.\n",
    "- **Context window**: limite de tokens do modelo; resumos muito longos podem truncar o início do prompt.\n",
    "- **Avaliação**: use métrica intrínseca (perplexity) e extrínseca (tarefas específicas); para estilo, considere avaliação humana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a155508",
   "metadata": {},
   "source": [
    "## 8) Notas finais e variações\n",
    "\n",
    "- **Modelos “GPT-like” em PT-BR**: ao usar um checkpoint treinado em português, espere melhor qualidade sintática/lexical; porém, verifique licença e tamanho do vocabulário.\n",
    "- **Chat/instruções**: GPT-3.5/4 (e equivalentes open-source com *instruction-tuning*) incluem treinamento adicional com **instruções** e **RLHF**, que melhora “seguir ordens”.\n",
    "- **Segurança**: adicione filtros de conteúdo e *guardrails* para aplicações reais.\n",
    "- **Eficiência**: para treinos maiores, use *LoRA/QLoRA*, *gradient checkpointing* e *mixed precision*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e3739",
   "metadata": {},
   "source": [
    "## 9) Exercícios\n",
    "\n",
    "1. **Temperatura e top-p**: gere 3 variações de um mesmo prompt e compare fluência e factualidade.\n",
    "2. **Prompt design**: crie *prefixos* (“Instruções: ...”, “Resuma: ...”) e observe o impacto no estilo.\n",
    "3. **Domínio específico**: substitua o corpus sintético por um conjunto pequeno real (ex.: ementas curtas) e repita o fine-tuning.\n",
    "4. **Perplexity**: separe um *validation split* e reporte perplexity de validação a cada N passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b5e6027",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
