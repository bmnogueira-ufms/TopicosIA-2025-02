{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defb1d2c",
   "metadata": {},
   "source": [
    "## Código da última aula..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da3be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap do modelo carregado ✔\n"
     ]
    }
   ],
   "source": [
    "import math, random, heapq, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilidades básicas\n",
    "# =========================\n",
    "def set_seed(s=42):\n",
    "    \"\"\"\n",
    "    Define semente para reprodutibilidade de experimentos\n",
    "    (Python, PyTorch CPU e CUDA).\n",
    "    \"\"\"\n",
    "    random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "\n",
    "set_seed(7)\n",
    "\n",
    "# Seleciona automaticamente GPU (se disponível) ou CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def shape(x, name=None):\n",
    "    \"\"\"\n",
    "    Helper para depuração: imprime shape, dtype e device de um tensor.\n",
    "    Útil para verificar se as dimensões estão como esperado em cada etapa.\n",
    "    \"\"\"\n",
    "    tag = f\"[{name}] \" if name else \"\"\n",
    "    print(f\"{tag}shape={tuple(x.shape)} dtype={x.dtype} device={x.device}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Máscaras\n",
    "# =========================\n",
    "def causal_mask(seq_len, device=None):\n",
    "    \"\"\"\n",
    "    Gera máscara causal (triangular inferior), usada para impedir que\n",
    "    o decodificador \"olhe\" posições futuras.\n",
    "\n",
    "    Retorna:\n",
    "        Tensor (1, 1, seq, seq) com 1=permitido e 0=proibido (futuro).\n",
    "    \"\"\"\n",
    "    m = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.uint8, device=device))\n",
    "    return m.view(1, 1, seq_len, seq_len)\n",
    "\n",
    "\n",
    "def padding_mask(token_ids, pad_token_id=0):\n",
    "    \"\"\"\n",
    "    Cria máscara para ignorar posições de padding em uma sequência.\n",
    "\n",
    "    Parâmetros:\n",
    "        token_ids: (B, seq) com IDs de tokens.\n",
    "        pad_token_id: ID usado para <pad>.\n",
    "\n",
    "    Retorna:\n",
    "        (B, 1, 1, seq) com 1=token válido e 0=PAD.\n",
    "    \"\"\"\n",
    "    return (token_ids != pad_token_id).unsqueeze(1).unsqueeze(2).to(torch.uint8)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Positional Encoding (senoidal)\n",
    "# =========================\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Codificação posicional senoidal (Vaswani et al., 2017).\n",
    "    Fornece informação de posição ao Transformer (que é paralelo e\n",
    "    não possui ordem intrínseca).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, device=None):\n",
    "        super().__init__()\n",
    "        # Matriz de posições: (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "\n",
    "        # Vetor de posições: (max_len, 1)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "        # Frequências: termos 1/10000^(2i/d_model) para i par\n",
    "        div = torch.exp(\n",
    "            torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Preenche dimensões pares com seno e ímpares com cosseno\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "\n",
    "        # Guarda como buffer (não-treinável), com dimensão de batch fictícia\n",
    "        # (1, max_len, d_model) para facilitar broadcast com (B, seq, d_model).\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0), persistent=False)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Soma a codificação posicional aos embeddings.\n",
    "\n",
    "        Parâmetros:\n",
    "            x: (B, seq, d_model)\n",
    "        Retorna:\n",
    "            (B, seq, d_model) = x + PE[:seq]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Multi-Head Attention\n",
    "# =========================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa a atenção multi-cabeças:\n",
    "      - Projeta Q, K, V para h cabeças menores (d_k/d_v),\n",
    "      - Calcula atenção escalonada por cabeça,\n",
    "      - Concatena e projeta de volta para d_model.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, num_heads=8, dropout=0.1, bias=True, return_attn_weights=True):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model deve ser múltiplo de num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.h = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.return_attn_weights = return_attn_weights\n",
    "\n",
    "        # Projeções aprendidas para Q, K e V (antes de dividir em cabeças)\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Projeção de saída (concatenação das cabeças → d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Reorganiza (B, T, d_model) → (B, h, T, d_k)\n",
    "        para processar as cabeças em paralelo.\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        return x.view(B, T, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def _combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Inverso de _split_heads:\n",
    "        (B, h, T, d_v) → (B, T, h*d_v) = (B, T, d_model)\n",
    "        \"\"\"\n",
    "        B, h, T, dv = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, h * dv)\n",
    "\n",
    "    def forward(self, Q_in, K_in, V_in, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Parâmetros:\n",
    "            Q_in, K_in, V_in: (B, seq, d_model)\n",
    "            attn_mask: (B, 1 ou h, Tq, Tk) com 1=permitido, 0=proibido\n",
    "        Retorna:\n",
    "            out: (B, Tq, d_model)\n",
    "            attn (opcional): (B, h, Tq, Tk) — pesos de atenção por cabeça\n",
    "        \"\"\"\n",
    "        B, Tq, _ = Q_in.shape\n",
    "        _,  Tk, _ = K_in.shape\n",
    "\n",
    "        # Projeções e divisão em cabeças\n",
    "        Q = self._split_heads(self.W_Q(Q_in))  # (B, h, Tq, d_k)\n",
    "        K = self._split_heads(self.W_K(K_in))  # (B, h, Tk, d_k)\n",
    "        V = self._split_heads(self.W_V(V_in))  # (B, h, Tk, d_v)\n",
    "\n",
    "        # Similaridade escalonada: Q · K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)  # (B, h, Tq, Tk)\n",
    "\n",
    "        # Aplica máscara: posições proibidas recebem -inf (softmax → 0)\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Distribuição de atenção por cabeça\n",
    "        attn = F.softmax(scores, dim=-1)   # (B, h, Tq, Tk)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Combina atenção com valores: saída por cabeça (B, h, Tq, d_v)\n",
    "        heads_out = torch.matmul(attn, V)\n",
    "\n",
    "        # Concatena cabeças e projeta de volta para d_model\n",
    "        concat = self._combine_heads(heads_out)  # (B, Tq, d_model)\n",
    "        out = self.W_O(concat)                   # (B, Tq, d_model)\n",
    "\n",
    "        if self.return_attn_weights:\n",
    "            return out, attn\n",
    "        return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) FFN posição-a-posição\n",
    "# =========================\n",
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP aplicada independentemente a cada posição (token):\n",
    "    Linear(d_model→d_ff) → ReLU/GELU → Dropout → Linear(d_ff→d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(d_model, d_ff)\n",
    "        self.lin2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()  # poderia ser nn.GELU() (comum em BERT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, seq, d_model) → (B, seq, d_model)\n",
    "        \"\"\"\n",
    "        return self.lin2(self.dropout(self.act(self.lin1(x))))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) EncoderLayer e DecoderLayer\n",
    "# =========================\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Um bloco do Encoder: Self-Attention + FFN (cada um com Add & Norm).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1, return_attn_weights=False):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout, return_attn_weights=return_attn_weights)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.return_attn = return_attn_weights\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B, seq, d_model)\n",
    "        src_mask: (B,1,1,seq) ou (B,h,seq,seq), 1=permitido, 0=pad/bloqueio\n",
    "        \"\"\"\n",
    "        # Self-Attention (Q=K=V=x)\n",
    "        mha_out, attn = self.mha(x, x, x, attn_mask=src_mask)\n",
    "        x = self.norm1(x + self.dropout1(mha_out))   # Add & Norm\n",
    "\n",
    "        # Feed-Forward posição-a-posição\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_out))   # Add & Norm\n",
    "\n",
    "        if self.return_attn:\n",
    "            return x, attn\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Um bloco do Decoder:\n",
    "      1) Masked Self-Attn (autoregressiva),\n",
    "      2) Cross-Attn (Q do decoder, K/V do encoder),\n",
    "      3) FFN,\n",
    "    cada qual seguido por Add & Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1, return_attn_weights=False):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(d_model, num_heads, dropout, return_attn_weights=return_attn_weights)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.cross_mha = MultiHeadAttention(d_model, num_heads, dropout, return_attn_weights=return_attn_weights)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.return_attn = return_attn_weights\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B, tgt_seq, d_model) — sequência alvo parcial\n",
    "        memory: (B, src_seq, d_model) — saída do encoder\n",
    "        tgt_mask: (B,1,tgt,tgt) causal ∧ padding\n",
    "        memory_mask: (B,1,tgt,src) padding mask do source expandida\n",
    "        \"\"\"\n",
    "        # 1) Masked Self-Attention (autoregressiva)\n",
    "        self_out, self_attn = self.self_mha(x, x, x, attn_mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_out))\n",
    "\n",
    "        # 2) Cross-Attention (decoder consulta o encoder)\n",
    "        cross_out, cross_attn = self.cross_mha(x, memory, memory, attn_mask=memory_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_out))\n",
    "\n",
    "        # 3) Feed-Forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(ffn_out))\n",
    "\n",
    "        if self.return_attn:\n",
    "            return x, {\"self\": self_attn, \"cross\": cross_attn}\n",
    "        return x\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) MiniTransformer (Encoder+Decoder + weight tying)\n",
    "# =========================\n",
    "class MiniTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo seq2seq minimalista no estilo Transformer (Encoder-Decoder).\n",
    "    Inclui:\n",
    "      - Embeddings + PE\n",
    "      - Pilhas de EncoderLayer e DecoderLayer\n",
    "      - Projeção final para logits de vocabulário (com weight tying)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, d_ff=2048, num_layers=2, dropout=0.1,\n",
    "                 pad_token_id=0, max_len=256, return_attn_weights=False, device=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.device = device\n",
    "\n",
    "        # Embeddings + Positional Encoding\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = SinusoidalPositionalEncoding(d_model, max_len=max_len, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Pilhas Encoder/Decoder\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout, return_attn_weights=return_attn_weights)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, return_attn_weights=return_attn_weights)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Projeção final para logits sobre o vocabulário\n",
    "        self.proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying: compartilha pesos entre embedding e projeção\n",
    "        self.proj.weight = self.tok_emb.weight\n",
    "\n",
    "        # Escalonamento sugerido pelo paper\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        self.return_attn = return_attn_weights\n",
    "\n",
    "    # --- Encode / Decode / Forward ---\n",
    "    def encode(self, src_ids):\n",
    "        \"\"\"\n",
    "        Codifica a sequência de entrada.\n",
    "        Retorna:\n",
    "          memory: (B, n, d_model) — representações do encoder\n",
    "          src_mask: (B,1,1,n) — padding mask do source\n",
    "          enc_attn: lista de mapas de atenção por camada (se habilitado)\n",
    "        \"\"\"\n",
    "        # src_ids: (B, n)\n",
    "        src_mask = padding_mask(src_ids, pad_token_id=self.pad_token_id)  # (B,1,1,n)\n",
    "\n",
    "        x = self.tok_emb(src_ids) * self.scale\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        enc_attn = []\n",
    "        for layer in self.encoder:\n",
    "            out = layer(x, src_mask=src_mask)\n",
    "            if isinstance(out, tuple):\n",
    "                x, attn = out\n",
    "                enc_attn.append(attn)\n",
    "            else:\n",
    "                x = out\n",
    "        return x, src_mask, enc_attn  # memory, mask, atenções\n",
    "\n",
    "    def decode(self, tgt_ids, memory, src_mask):\n",
    "        \"\"\"\n",
    "        Decodifica a sequência alvo condicionada à memória do encoder.\n",
    "        Retorna:\n",
    "          logits: (B, m, |V|)\n",
    "          dec_attn: lista de dicts {\"self\":(B,h,m,m), \"cross\":(B,h,m,n)} por camada\n",
    "        \"\"\"\n",
    "        # tgt_ids: (B, m)\n",
    "        B, m = tgt_ids.shape\n",
    "\n",
    "        # Máscara causal (B,1,m,m) e padding do target (B,1,1,m)\n",
    "        causal = causal_mask(m, device=tgt_ids.device)                    # (1,1,m,m)\n",
    "        tgt_pad = padding_mask(tgt_ids, pad_token_id=self.pad_token_id)   # (B,1,1,m)\n",
    "        tgt_mask = causal & tgt_pad                                       # broadcast → (B,1,m,m)\n",
    "\n",
    "        # Memory mask: expande padding do source para cada posição de tgt: (B,1,m,n)\n",
    "        memory_mask = src_mask.expand(B, 1, m, src_mask.size(-1))\n",
    "\n",
    "        x = self.tok_emb(tgt_ids) * self.scale\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        dec_attn = []\n",
    "        for layer in self.decoder:\n",
    "            out = layer(x, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "            if isinstance(out, tuple):\n",
    "                x, attn = out\n",
    "                dec_attn.append(attn)\n",
    "            else:\n",
    "                x = out\n",
    "\n",
    "        logits = self.proj(x)  # (B, m, |V|)\n",
    "        return logits, dec_attn\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids):\n",
    "        \"\"\"\n",
    "        Pipeline completo: encode(source) → decode(target | memory)\n",
    "        Retorna:\n",
    "          logits e (opcionalmente) mapas de atenção de encoder e decoder.\n",
    "        \"\"\"\n",
    "        memory, src_mask, enc_attn = self.encode(src_ids)\n",
    "        logits, dec_attn = self.decode(tgt_ids, memory, src_mask)\n",
    "        return logits, {\"enc\": enc_attn, \"dec\": dec_attn}\n",
    "\n",
    "\n",
    "###############  Daqui em diante é material novo. Explicaremos a seguir ###############\n",
    "\n",
    "# =========================\n",
    "# 7) Loss com label smoothing\n",
    "# =========================\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação de cross-entropy com label smoothing.\n",
    "    Suaviza a distribuição-alvo para reduzir overfitting e calibrar probabilidades.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=0.1, ignore_index=0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        Parâmetros:\n",
    "            logits: (B, T, V) — scores não normalizados do decoder\n",
    "            target: (B, T)    — IDs verdadeiros (alvos)\n",
    "        Retorna:\n",
    "            Escalar de loss (por padrão, média sobre tokens não-pad).\n",
    "        \"\"\"\n",
    "        B, T, V = logits.shape\n",
    "        logits = logits.view(B*T, V)\n",
    "        target = target.view(B*T)\n",
    "\n",
    "        # Ignora posições com <pad>\n",
    "        mask = (target != self.ignore_index)\n",
    "        if not mask.any():\n",
    "            # Evita NaN se tudo for pad\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "        valid_logits = logits[mask]\n",
    "        valid_target = target[mask]\n",
    "\n",
    "        # Constrói distribuição-alvo suavizada\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(valid_logits)\n",
    "            true_dist.fill_(self.eps / (V - 1))\n",
    "            true_dist.scatter_(1, valid_target.unsqueeze(1), 1.0 - self.eps)\n",
    "\n",
    "        # CE com distribuição suavizada\n",
    "        log_probs = F.log_softmax(valid_logits, dim=-1)\n",
    "        loss = -(true_dist * log_probs).sum(dim=-1)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Noam scheduler\n",
    "# =========================\n",
    "class NoamScheduler:\n",
    "    \"\"\"\n",
    "    Agendador de taxa de aprendizado do Transformer original:\n",
    "      lrate = d_model^-0.5 * min(step^-0.5, step * warmup^-1.5)\n",
    "    Cresce linearmente até warmup_steps e depois decai como 1/sqrt(step).\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup = warmup_steps\n",
    "        self.step_num = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Avança um passo no agendador e atualiza a LR do otimizador.\n",
    "        Retorna a LR atual (útil para logging).\n",
    "        \"\"\"\n",
    "        self.step_num += 1\n",
    "        lr = (self.d_model ** -0.5) * min(\n",
    "            self.step_num ** -0.5,\n",
    "            self.step_num * (self.warmup ** -1.5)\n",
    "        )\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) Dataset sintético (cópia) + loader simples\n",
    "# =========================\n",
    "PAD, BOS, EOS = 0, 1, 2\n",
    "\n",
    "def gen_sequence(vocab_size=50, min_len=4, max_len=12):\n",
    "    \"\"\"\n",
    "    Gera uma sequência aleatória de IDs (evitando PAD/BOS/EOS).\n",
    "    \"\"\"\n",
    "    L = random.randint(min_len, max_len)\n",
    "    return [random.randint(3, vocab_size-1) for _ in range(L)]  # evita PAD/BOS/EOS\n",
    "\n",
    "\n",
    "def make_example(vocab_size=50, min_len=4, max_len=12):\n",
    "    \"\"\"\n",
    "    Tarefa de cópia: alvo = cópia do source.\n",
    "    Usa:\n",
    "      tgt_in  = [BOS] + src\n",
    "      tgt_out = src + [EOS]\n",
    "    \"\"\"\n",
    "    src = gen_sequence(vocab_size, min_len, max_len)\n",
    "    tgt_in  = [BOS] + src[:]\n",
    "    tgt_out = src[:] + [EOS]\n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "\n",
    "def pad_batch(batch, pad_id=PAD):\n",
    "    \"\"\"\n",
    "    Faz padding uniforme em um lote de exemplos de tamanhos variáveis.\n",
    "    Retorna tensores (B, max_len) para src, tgt_in, tgt_out.\n",
    "    \"\"\"\n",
    "    max_src  = max(len(s)  for s,_,__ in batch)\n",
    "    max_tin  = max(len(ti) for _,ti,__ in batch)\n",
    "    max_tout = max(len(to) for _,__,to in batch)\n",
    "\n",
    "    src_ids, tgt_in_ids, tgt_out_ids = [], [], []\n",
    "    for s, ti, to in batch:\n",
    "        src_ids.append(s  + [pad_id]*(max_src  - len(s)))\n",
    "        tgt_in_ids.append(ti + [pad_id]*(max_tin - len(ti)))\n",
    "        tgt_out_ids.append(to + [pad_id]*(max_tout- len(to)))\n",
    "    return (torch.tensor(src_ids, dtype=torch.long),\n",
    "            torch.tensor(tgt_in_ids, dtype=torch.long),\n",
    "            torch.tensor(tgt_out_ids, dtype=torch.long))\n",
    "\n",
    "\n",
    "def data_iter(num_batches=200, batch_size=32, vocab_size=50):\n",
    "    \"\"\"\n",
    "    Iterador simples que gera lotes de dados sintéticos com padding.\n",
    "    Útil para testar pipeline de treino/decoding.\n",
    "    \"\"\"\n",
    "    for _ in range(num_batches):\n",
    "        batch = [make_example(vocab_size=vocab_size) for _ in range(batch_size)]\n",
    "        yield pad_batch(batch)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10) Decodificadores (greedy e beam)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, src_ids, max_len=50):\n",
    "    \"\"\"\n",
    "    Decodificação gulosa (greedy): escolhe argmax a cada passo.\n",
    "    Observação: para uso didático; beam search costuma gerar sequências melhores.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    memory, src_mask, _ = model.encode(src_ids)\n",
    "    B = src_ids.size(0)\n",
    "    ys = torch.full((B, 1), BOS, dtype=torch.long, device=src_ids.device)\n",
    "    for _ in range(max_len):\n",
    "        logits, _ = model.decode(ys, memory, src_mask)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "        if (next_token == EOS).all():\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def length_penalty(length, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Penalidade de comprimento (Google NMT):\n",
    "      LP = ((5 + length) / 6)^alpha\n",
    "    Ajuda o beam a não favorecer sequências muito curtas.\n",
    "    \"\"\"\n",
    "    return ((5 + length) / 6) ** alpha\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src_ids, beam_size=4, max_len=50, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Beam search didático para batch=1.\n",
    "    Mantém 'beam_size' hipóteses com melhores scores (neg-log prob),\n",
    "    aplicando penalidade de comprimento para normalizar a comparação.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    assert src_ids.size(0) == 1, \"Implementação didática assume batch=1\"\n",
    "    memory, src_mask, _ = model.encode(src_ids)\n",
    "\n",
    "    # Cada item do beam: (score_acumulado, sequência_gerada)\n",
    "    beams = [(0.0, torch.tensor([[BOS]], device=src_ids.device))]\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for score, seq in beams:\n",
    "            # Se já finalizou com EOS, movemos para 'completed'\n",
    "            if seq[0, -1].item() == EOS:\n",
    "                completed.append((score, seq))\n",
    "                continue\n",
    "\n",
    "            # Expande um passo a partir da sequência atual\n",
    "            logits, _ = model.decode(seq, memory, src_mask)\n",
    "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)  # (1, V)\n",
    "\n",
    "            # Seleciona top-k candidatos\n",
    "            topk = torch.topk(log_probs, beam_size, dim=-1)\n",
    "            for lp, tok in zip(topk.values[0], topk.indices[0]):\n",
    "                new_seq = torch.cat([seq, tok.view(1,1)], dim=1)\n",
    "                new_score = score - lp.item()  # acumulamos -log p (menor é melhor)\n",
    "                new_beams.append((new_score, new_seq))\n",
    "\n",
    "        # Ordena por score normalizado com length penalty e mantém top-k\n",
    "        def norm_key(item):\n",
    "            s, seq = item\n",
    "            L = seq.size(1)\n",
    "            return s / length_penalty(L, alpha)\n",
    "\n",
    "        beams = sorted(new_beams, key=norm_key)[:beam_size]\n",
    "        if len(beams) == 0:\n",
    "            break\n",
    "\n",
    "    # Considera também os beams restantes não finalizados\n",
    "    completed.extend(beams)\n",
    "\n",
    "    # Escolhe a melhor sequência pelo score normalizado\n",
    "    best = min(completed, key=lambda x: x[0] / length_penalty(x[1].size(1), alpha))\n",
    "    return best[1]\n",
    "\n",
    "\n",
    "print(\"Bootstrap do modelo carregado ✔\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d5759",
   "metadata": {},
   "source": [
    "## 1) Por que **label smoothing**?\n",
    "Sem smoothing, a *cross-entropy* empurra probabilidade 100% para o rótulo correto (one-hot). Isso pode causar **overconfidence** e prejudicar generalização.  \n",
    "**Label smoothing** (ε) “distribui” uma pequena massa nos demais rótulos:\n",
    "- Distribuição-alvo vira: `1−ε` no rótulo correto, `ε/(|V|−1)` nos demais.\n",
    "- Efeito: **regularização**; melhora BLEU/accuracy mesmo **piorando perplexidade** (o modelo fica menos “certo” numericamente, mas erra menos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação de cross-entropy com label smoothing.\n",
    "    Suaviza a distribuição-alvo para reduzir overfitting e calibrar probabilidades.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=0.1, ignore_index=0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        Parâmetros:\n",
    "            logits: (B, T, V) — scores não normalizados do decoder\n",
    "            target: (B, T)    — IDs verdadeiros (alvos)\n",
    "        Retorna:\n",
    "            Escalar de loss (por padrão, média sobre tokens não-pad).\n",
    "        \"\"\"\n",
    "        B, T, V = logits.shape\n",
    "        logits = logits.view(B*T, V)\n",
    "        target = target.view(B*T)\n",
    "\n",
    "        # Ignora posições com <pad>\n",
    "        mask = (target != self.ignore_index)\n",
    "        if not mask.any():\n",
    "            # Evita NaN se tudo for pad\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "        valid_logits = logits[mask]\n",
    "        valid_target = target[mask]\n",
    "\n",
    "        # Constrói distribuição-alvo suavizada\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(valid_logits)\n",
    "            true_dist.fill_(self.eps / (V - 1))\n",
    "            true_dist.scatter_(1, valid_target.unsqueeze(1), 1.0 - self.eps)\n",
    "\n",
    "        # CE com distribuição suavizada\n",
    "        log_probs = F.log_softmax(valid_logits, dim=-1)\n",
    "        loss = -(true_dist * log_probs).sum(dim=-1)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed2433",
   "metadata": {},
   "source": [
    "## 2) Otimizador e **Noam LR Schedule**\n",
    "O paper usa **Adam** com **β₁=0.9, β₂=0.98, ε=1e−9** e um agendamento:\n",
    "$$\n",
    "\\text{lrate} = d_{model}^{-0.5} \\cdot \\min(\\text{step}^{-0.5},\\ \\text{step}\\cdot \\text{warmup}^{-1.5})\n",
    "$$\n",
    "- **Aquecimento (warmup)**: cresce linearmente até `warmup_steps` e depois decai ∝ `1/√step`.  \n",
    "- **Valores do paper**: warmup=4000 (modelo base). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim import Adam\n",
    "\n",
    "class NoamScheduler:\n",
    "    \"\"\"\n",
    "    Agendador de taxa de aprendizado do Transformer original:\n",
    "      lrate = d_model^-0.5 * min(step^-0.5, step * warmup^-1.5)\n",
    "    Cresce linearmente até warmup_steps e depois decai como 1/sqrt(step).\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup = warmup_steps\n",
    "        self.step_num = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Avança um passo no agendador e atualiza a LR do otimizador.\n",
    "        Retorna a LR atual (útil para logging).\n",
    "        \"\"\"\n",
    "        self.step_num += 1\n",
    "        lr = (self.d_model ** -0.5) * min(\n",
    "            self.step_num ** -0.5,\n",
    "            self.step_num * (self.warmup ** -1.5)\n",
    "        )\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "# Exemplo de uso:\n",
    "# optimizer = Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "# scheduler = NoamScheduler(optimizer, d_model=512, warmup_steps=4000)\n",
    "# for ...:\n",
    "#     loss.backward(); optimizer.step(); optimizer.zero_grad()\n",
    "#     lr = scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956a006",
   "metadata": {},
   "source": [
    "## 3) Dataset sintético: **tarefa de cópia**\n",
    "Para demonstrar o pipeline completo sem depender de corpus grande, usamos uma tarefa simples:  \n",
    "**Entrada = sequência aleatória de tokens** (sem <pad> dentro), **Saída = a mesma sequência deslocada** (teacher forcing).\n",
    "\n",
    "- `<bos>` e `<eos>` delimitam início/fim no target.\n",
    "- Mostramos *masking* do `<pad>` na perda.\n",
    "- Em poucas iterações dá pra ver a perda caindo e o modelo **memorizando** a cópia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bc9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "PAD, BOS, EOS = 0, 1, 2\n",
    "\n",
    "def gen_sequence(vocab_size=50, min_len=4, max_len=12):\n",
    "    L = random.randint(min_len, max_len)\n",
    "    # tokens em [3, vocab_size-1], evitando PAD/BOS/EOS\n",
    "    seq = [random.randint(3, vocab_size-1) for _ in range(L)]\n",
    "    return seq\n",
    "\n",
    "def make_example(vocab_size=50, min_len=4, max_len=12):\n",
    "    src = gen_sequence(vocab_size, min_len, max_len)\n",
    "    tgt_in = [BOS] + src[:]         # teacher forcing input\n",
    "    tgt_out = src[:] + [EOS]        # o que queremos prever\n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "def pad_batch(batch, pad_id=PAD):\n",
    "    # batch: list of (src, tgt_in, tgt_out)\n",
    "    max_src = max(len(s) for s,_,__ in batch)\n",
    "    max_tin = max(len(ti) for _,ti,__ in batch)\n",
    "    max_tout= max(len(to) for _,__,to in batch)\n",
    "\n",
    "    src_ids = []\n",
    "    tgt_in_ids = []\n",
    "    tgt_out_ids = []\n",
    "\n",
    "    for s, ti, to in batch:\n",
    "        src_ids.append(s + [pad_id]*(max_src - len(s)))\n",
    "        tgt_in_ids.append(ti + [pad_id]*(max_tin - len(ti)))\n",
    "        tgt_out_ids.append(to + [pad_id]*(max_tout - len(to)))\n",
    "\n",
    "    return (torch.tensor(src_ids, dtype=torch.long),\n",
    "            torch.tensor(tgt_in_ids, dtype=torch.long),\n",
    "            torch.tensor(tgt_out_ids, dtype=torch.long))\n",
    "\n",
    "def data_iter(num_batches=200, batch_size=32, vocab_size=50):\n",
    "    for _ in range(num_batches):\n",
    "        batch = [make_example(vocab_size=vocab_size) for _ in range(batch_size)]\n",
    "        yield pad_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9178c",
   "metadata": {},
   "source": [
    "## 4) Amarrando tudo: modelo + perda + otimizador + scheduler\n",
    "- Usaremos seu **MiniTransformer** (ou o da aula anterior), com:\n",
    "  - embeddings escaladas por `√d_model`,\n",
    "  - **PE senoidal** somada,\n",
    "  - pilha de **EncoderLayer**/**DecoderLayer**,\n",
    "  - *weight tying* entre embedding e projeção final.\n",
    "- Perda: **LabelSmoothingLoss(ε=0.1)**, ignorando `<pad>`.\n",
    "- Otimizador: **Adam(β₁=0.9, β₂=0.98, ε=1e−9)**, LR controlado pelo **NoamScheduler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cb4e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0020/200] loss=27.9931  lr=0.000221  (3.2s)\n",
      "[0040/200] loss=10.5540  lr=0.000442  (2.3s)\n",
      "[0060/200] loss=6.6749  lr=0.000663  (2.2s)\n",
      "[0080/200] loss=5.6498  lr=0.000884  (2.2s)\n",
      "[0100/200] loss=4.7260  lr=0.001105  (2.2s)\n",
      "[0120/200] loss=4.1890  lr=0.001326  (2.2s)\n",
      "[0140/200] loss=3.9550  lr=0.001547  (2.2s)\n",
      "[0160/200] loss=3.7974  lr=0.001768  (2.5s)\n",
      "[0180/200] loss=3.6977  lr=0.001989  (2.6s)\n",
      "[0200/200] loss=3.6150  lr=0.002210  (2.4s)\n",
      "Treino finalizado.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Hiperparâmetros didáticos (pequenos para rodar rápido)\n",
    "VOCAB_SIZE = 64\n",
    "D_MODEL    = 128\n",
    "N_HEADS    = 4\n",
    "D_FF       = 512\n",
    "N_LAYERS   = 2\n",
    "DROPOUT    = 0.1\n",
    "WARMUP     = 400   # menor que 4000 só para ver a curva mexer rápido\n",
    "BATCH_SIZE = 64\n",
    "NUM_BATCH  = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MiniTransformer(\n",
    "    vocab_size=VOCAB_SIZE, d_model=D_MODEL, num_heads=N_HEADS, d_ff=D_FF,\n",
    "    num_layers=N_LAYERS, dropout=DROPOUT, pad_token_id=PAD, max_len=128,\n",
    "    return_attn_weights=False, device=device\n",
    ").to(device)\n",
    "\n",
    "criterion = LabelSmoothingLoss(eps=0.1, ignore_index=PAD)\n",
    "optimizer = Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = NoamScheduler(optimizer, d_model=D_MODEL, warmup_steps=WARMUP)\n",
    "\n",
    "# Atenções retornando (out, attn) para bater com o desempacotamento do Encoder/Decoder\n",
    "\n",
    "# Encoder: ligar pesos nas self-attn\n",
    "for layer in model.encoder:\n",
    "    if hasattr(layer, \"mha\"):\n",
    "        layer.mha.return_attn_weights = True\n",
    "\n",
    "# Decoder: ligar pesos nas self-attn e cross-attn\n",
    "for layer in model.decoder:\n",
    "    if hasattr(layer, \"self_mha\"):\n",
    "        layer.self_mha.return_attn_weights = True\n",
    "    if hasattr(layer, \"cross_mha\"):\n",
    "        layer.cross_mha.return_attn_weights = True\n",
    "\n",
    "def shift_targets_for_loss(tgt_out):\n",
    "    \"\"\"\n",
    "    Garantimos que logits (B,T,|V|) alinhem com tgt_out (B,T).\n",
    "    Nosso decode já produz logits alinhados com tgt_in, então ok.\n",
    "    \"\"\"\n",
    "    return tgt_out\n",
    "\n",
    "losses = []\n",
    "lrs = []\n",
    "\n",
    "start = time.time()\n",
    "model.train()\n",
    "for i, (src, tgt_in, tgt_out) in enumerate(data_iter(num_batches=NUM_BATCH, batch_size=BATCH_SIZE, vocab_size=VOCAB_SIZE), 1):\n",
    "    src = src.to(device)\n",
    "    tgt_in = tgt_in.to(device)\n",
    "    tgt_out = shift_targets_for_loss(tgt_out.to(device))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = model(src, tgt_in)       # (B, T, V)\n",
    "    loss = criterion(logits, tgt_out)    # mascara PAD internamente\n",
    "    loss.backward()\n",
    "\n",
    "    # (Opcional) clipping para estabilidade\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    lr = scheduler.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    lrs.append(lr)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        dt = time.time() - start\n",
    "        print(f\"[{i:04d}/{NUM_BATCH}] loss={loss.item():.4f}  lr={lr:.6f}  ({dt:.1f}s)\")\n",
    "        start = time.time()\n",
    "\n",
    "print(\"Treino finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0405ee",
   "metadata": {},
   "source": [
    "## 5) Visualizando perda e learning rate\n",
    "Vamos plotar:\n",
    "- **Perda por batch** (espera-se queda clara na tarefa de cópia)\n",
    "- **Learning rate** ao longo dos passos (Noam schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5d7d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Perda por batch\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lrs)\n",
    "plt.title(\"Learning rate (Noam schedule)\")\n",
    "plt.xlabel(\"Passo\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520209bd",
   "metadata": {},
   "source": [
    "## 6) Decodificação: **greedy** e **beam search** com length penalty\n",
    "\n",
    "- **Greedy**: escolhe o argmax a cada passo; rápido, porém pode “miopizar” a sequência.\n",
    "- **Beam search**: mantém os `k` melhores **prefixos**; controla **exploração** vs **exploração local**.\n",
    "- **Length penalty (α)**: corrige o viés a sequências curtas/longas. O paper usou **α=0.6**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c08a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model, src_ids, max_len=50):\n",
    "    \"\"\"\n",
    "    src_ids: (B, n)\n",
    "    Retorna sequência decodificada para cada item do batch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    memory, src_mask, _ = model.encode(src_ids)\n",
    "    B = src_ids.size(0)\n",
    "    ys = torch.full((B, 1), BOS, dtype=torch.long, device=src_ids.device)  # começa com <BOS>\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, _ = model.decode(ys, memory, src_mask)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # último passo\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "        # parada antecipada se todos emitirem EOS\n",
    "        if (next_token == EOS).all():\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675c61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "\n",
    "def length_penalty(length, alpha=0.6):\n",
    "    # Como no GNMT / paper (α=0.6), penaliza/prolonga adequadamente\n",
    "    return ((5 + length) / 6) ** alpha\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src_ids, beam_size=4, max_len=50, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Implementação simples de beam search (batch=1 para didática).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    assert src_ids.size(0) == 1, \"Implementação didática assume batch=1\"\n",
    "    memory, src_mask, _ = model.encode(src_ids)\n",
    "\n",
    "    # cada hipótese: (score_negativo, seq_tensor)\n",
    "    # usamos log-prob acumulada negativa para usar heapq como min-heap\n",
    "    init = torch.tensor([[BOS]], device=src_ids.device)\n",
    "    beams = [(0.0, init)]\n",
    "    completed = []\n",
    "\n",
    "    for step in range(max_len):\n",
    "        new_beams = []\n",
    "        for score, seq in beams:\n",
    "            if seq[0, -1].item() == EOS:\n",
    "                completed.append((score, seq))\n",
    "                continue\n",
    "            logits, _ = model.decode(seq, memory, src_mask)\n",
    "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)  # (1, V)\n",
    "            topk = torch.topk(log_probs, beam_size, dim=-1)\n",
    "            for lp, tok in zip(topk.values[0], topk.indices[0]):\n",
    "                new_seq = torch.cat([seq, tok.view(1,1)], dim=1)\n",
    "                new_score = score - lp.item()  # lembrando: score = - Σ log p\n",
    "                new_beams.append((new_score, new_seq))\n",
    "\n",
    "        # seleciona os K melhores pelo score normalizado por length penalty\n",
    "        def normalized_key(item):\n",
    "            score, seq = item\n",
    "            L = seq.size(1)\n",
    "            return score / length_penalty(L, alpha)\n",
    "\n",
    "        beams = sorted(new_beams, key=normalized_key)[:beam_size]\n",
    "\n",
    "        # critério: se todos encerraram\n",
    "        if len(beams) == 0:\n",
    "            break\n",
    "\n",
    "    completed.extend(beams)\n",
    "    best = min(completed, key=lambda x: x[0] / length_penalty(x[1].size(1), alpha))\n",
    "    return best[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6ba71",
   "metadata": {},
   "source": [
    "## 7) Teste rápido de decodificação\n",
    "\n",
    "Vamos criar uma sequência de entrada (aleatória), treinar mais algumas iterações (se preciso),\n",
    "e comparar **greedy** vs **beam search** (α=0.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881ffcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: [1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "Beam  : [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Uma fonte ao acaso\n",
    "src_seq, tgt_in_seq, tgt_out_seq = make_example(vocab_size=VOCAB_SIZE)\n",
    "src = torch.tensor([src_seq], device=device)\n",
    "\n",
    "# Greedy\n",
    "out_greedy = greedy_decode(model, src, max_len=64)\n",
    "print(\"Greedy:\", out_greedy[0].tolist())\n",
    "\n",
    "# Beam\n",
    "out_beam = beam_search_decode(model, src, beam_size=4, max_len=64, alpha=0.6)\n",
    "print(\"Beam  :\", out_beam[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b92d28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[texto 020/200] loss=15.1084  lr=0.000221  (2.4s)\n",
      "[texto 040/200] loss=4.6911  lr=0.000442  (2.1s)\n",
      "[texto 060/200] loss=2.4095  lr=0.000663  (2.2s)\n",
      "[texto 080/200] loss=1.3448  lr=0.000884  (2.1s)\n",
      "[texto 100/200] loss=1.0756  lr=0.001105  (2.1s)\n",
      "[texto 120/200] loss=0.9646  lr=0.001326  (2.1s)\n",
      "[texto 140/200] loss=0.9317  lr=0.001547  (2.1s)\n",
      "[texto 160/200] loss=0.8387  lr=0.001768  (2.3s)\n",
      "[texto 180/200] loss=0.8133  lr=0.001989  (2.3s)\n",
      "[texto 200/200] loss=0.7566  lr=0.002210  (2.4s)\n",
      "Treino (texto) finalizado.\n",
      "\n",
      "================= DEMO (TEXTO) =================\n",
      "Entrada:          o gato preto correu .\n",
      "Saída esperada:   o gato preto correu .\n",
      "Saída gerada:     o gato preto correu .\n",
      "------------------------------------------------\n",
      "Entrada:          a gata branca correu .\n",
      "Saída esperada:   a gata branca correu .\n",
      "Saída gerada:     a gata branca correu .\n",
      "------------------------------------------------\n",
      "Entrada:          o cachorro branco dormiu .\n",
      "Saída esperada:   o cachorro branco dormiu .\n",
      "Saída gerada:     o cachorro branco dormiu .\n",
      "------------------------------------------------\n",
      "Entrada:          o gato dormiu .\n",
      "Saída esperada:   o gato dormiu .\n",
      "Saída gerada:     o gato dormiu .\n",
      "------------------------------------------------\n",
      "Entrada:          o cachorro pulou .\n",
      "Saída esperada:   o cachorro pulou .\n",
      "Saída gerada:     o cachorro pulou .\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Treinar e comparar esperado vs gerado\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 1) Corpus de texto simples (use o que quiser aqui; só mantenha tokens no vocabulário)\n",
    "TEXT_CORPUS = [\n",
    "    \"o gato preto correu .\",\n",
    "    \"o cachorro branco dormiu .\",\n",
    "    \"a gata preta pulou .\",\n",
    "    \"o gato dormiu .\",\n",
    "    \"o cachorro correu .\",\n",
    "    \"a gata branca correu .\",\n",
    "    \"o gato preto pulou .\",\n",
    "    \"a gata dormiu .\",\n",
    "    \"o cachorro pulou .\",\n",
    "    \"o gato correu .\",\n",
    "    \"o melhor time do mundo é o cruzeiro .\",\n",
    "    \"santos é nome de cidade .\",\n",
    "]\n",
    "\n",
    "# 2) Construir vocabulário (com seus especiais PAD/BOS/EOS)\n",
    "#    Usa tokenização por espaço, mantendo '.' como token\n",
    "def _tok(s): \n",
    "    return s.strip().lower().split()\n",
    "\n",
    "unique_tokens = set()\n",
    "for s in TEXT_CORPUS:\n",
    "    unique_tokens.update(_tok(s))\n",
    "\n",
    "# Garante que '.' esteja presente mesmo se não aparecer nos exemplos\n",
    "unique_tokens.update([\".\", \",\"])\n",
    "\n",
    "# Monta vocab/ivocab incluindo especiais\n",
    "# Se você já tem 'vocab' e 'ivocab' globais para OUTROS usos, aqui criamos versões\n",
    "# específicas deste demo de texto, para não conflitar: vocab_txt / ivocab_txt.\n",
    "vocab_txt = {\"<pad>\": PAD, \"<bos>\": BOS, \"<eos>\": EOS}\n",
    "next_id = max(vocab_txt.values()) + 1\n",
    "for w in sorted(unique_tokens):\n",
    "    if w not in vocab_txt:\n",
    "        vocab_txt[w] = next_id\n",
    "        next_id += 1\n",
    "ivocab_txt = {i:w for w,i in vocab_txt.items()}\n",
    "\n",
    "def encode_text_txt(text, add_bos=True, add_eos=True, device=None):\n",
    "    toks = _tok(text)\n",
    "    ids = []\n",
    "    if add_bos: ids.append(BOS)\n",
    "    oov = []\n",
    "    for t in toks:\n",
    "        if t in vocab_txt:\n",
    "            ids.append(vocab_txt[t])\n",
    "        else:\n",
    "            oov.append(t)\n",
    "            ids.append(PAD)  # fallback didático; melhor seria tratar <unk> separado\n",
    "    if add_eos: ids.append(EOS)\n",
    "    if oov:\n",
    "        print(f\"[Aviso] Tokens fora do vocabulário: {oov}\")\n",
    "    dev = device if device is not None else next(model.parameters()).device\n",
    "    return torch.tensor([ids], dtype=torch.long, device=dev)\n",
    "\n",
    "def decode_ids_txt(ids_tensor):\n",
    "    ids = ids_tensor.squeeze(0).tolist()\n",
    "    toks = []\n",
    "    for i in ids:\n",
    "        if i == BOS: \n",
    "            continue\n",
    "        if i == EOS:\n",
    "            break\n",
    "        toks.append(ivocab_txt.get(i, \"<unk>\"))\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# 3) Dataset de cópia (texto -> texto)\n",
    "def make_text_batch(batch_size=32, device=None):\n",
    "    dev = device if device is not None else next(model.parameters()).device\n",
    "    src_list, tgt_in_list, tgt_out_list = [], [], []\n",
    "    for _ in range(batch_size):\n",
    "        s = random.choice(TEXT_CORPUS)\n",
    "        src_ids = encode_text_txt(s, add_bos=True, add_eos=True, device=dev)      # (1, n)\n",
    "        tgt_in  = encode_text_txt(s, add_bos=True, add_eos=False, device=dev)      # <bos> + tokens\n",
    "        tgt_out = encode_text_txt(s, add_bos=False, add_eos=True, device=dev)      # tokens + <eos>\n",
    "        src_list.append(src_ids.squeeze(0))\n",
    "        tgt_in_list.append(tgt_in.squeeze(0))\n",
    "        tgt_out_list.append(tgt_out.squeeze(0))\n",
    "    src = pad_sequence(src_list, batch_first=True, padding_value=PAD)\n",
    "    tgt_in  = pad_sequence(tgt_in_list, batch_first=True, padding_value=PAD)\n",
    "    tgt_out = pad_sequence(tgt_out_list, batch_first=True, padding_value=PAD)\n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "# 4) Instanciar um novo modelo para TEXTO (não sobrescreve seu 'model' anterior)\n",
    "#    Importante: return_attn_weights=True para bater com o Encoder/Decoder atuais.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_TXT = len(vocab_txt)\n",
    "\n",
    "model_txt = MiniTransformer(\n",
    "    vocab_size=VOCAB_TXT,\n",
    "    d_model=D_MODEL,           # reutiliza seus hyperparams definidos acima\n",
    "    num_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_token_id=PAD,\n",
    "    max_len=128,\n",
    "    return_attn_weights=True,  # <- necessário dado o desempacotamento (out, attn)\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "criterion_txt = LabelSmoothingLoss(eps=0.1, ignore_index=PAD)\n",
    "optimizer_txt = torch.optim.Adam(model_txt.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler_txt = NoamScheduler(optimizer_txt, d_model=D_MODEL, warmup_steps=WARMUP)\n",
    "\n",
    "# 5) Treino curto no corpus textual (tarefa de cópia)\n",
    "NUM_BATCH_TXT = 200\n",
    "BATCH_SIZE_TXT = 64\n",
    "model_txt.train()\n",
    "losses_txt, lrs_txt = [], []\n",
    "t0 = time.time()\n",
    "for step in range(1, NUM_BATCH_TXT+1):\n",
    "    src, tgt_in, tgt_out = make_text_batch(BATCH_SIZE_TXT, device=device)\n",
    "    optimizer_txt.zero_grad()\n",
    "    logits, _ = model_txt(src, tgt_in)   # (B, T, V)\n",
    "    loss = criterion_txt(logits, tgt_out)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_txt.parameters(), 1.0)\n",
    "    optimizer_txt.step()\n",
    "    lr = scheduler_txt.step()\n",
    "    losses_txt.append(loss.item()); lrs_txt.append(lr)\n",
    "    if step % 20 == 0:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[texto {step:03d}/{NUM_BATCH_TXT}] loss={loss.item():.4f}  lr={lr:.6f}  ({dt:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "print(\"Treino (texto) finalizado.\")\n",
    "\n",
    "# 6) Predição (greedy) e comparação: Entrada x Esperada x Gerada\n",
    "@torch.no_grad()\n",
    "def greedy_decode_txt(model, src_ids, max_len=40):\n",
    "    model.eval()\n",
    "    memory, src_mask, _ = model.encode(src_ids)\n",
    "    ys = torch.tensor([[BOS]], device=src_ids.device, dtype=torch.long)\n",
    "    for _ in range(max_len):\n",
    "        logits, _ = model.decode(ys, memory, src_mask)  # (B, t, V)\n",
    "        next_id = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        ys = torch.cat([ys, next_id], dim=1)\n",
    "        if (next_id == EOS).all():\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def demo_compare(sentences):\n",
    "    print(\"\\n================= DEMO (TEXTO) =================\")\n",
    "    for s in sentences:\n",
    "        src = encode_text_txt(s, add_bos=True, add_eos=True, device=device)\n",
    "        out_ids = greedy_decode_txt(model_txt, src, max_len=40)\n",
    "        out_text = decode_ids_txt(out_ids)\n",
    "        print(f\"Entrada:          {s}\")\n",
    "        print(f\"Saída esperada:   {s}\")     # tarefa de cópia\n",
    "        print(f\"Saída gerada:     {out_text}\")\n",
    "        print(\"-\"*48)\n",
    "\n",
    "# 7) Rodar a comparação em várias entradas\n",
    "TESTE = [\n",
    "    \"o gato preto correu .\",\n",
    "    \"a gata branca correu .\",\n",
    "    \"o cachorro branco dormiu .\",\n",
    "    \"o gato dormiu .\",\n",
    "    \"o cachorro pulou .\",\n",
    "]\n",
    "demo_compare(TESTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020ba443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= PREDICT (TEXTO) =================\n",
      "Entrada:         o gato preto correu .\n",
      "Saída gerada:    o gato preto correu .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.8977\n",
      "  gato         p=0.0077\n",
      "  ,            p=0.0066\n",
      "  santos       p=0.0063\n",
      "  branca       p=0.0056\n",
      "------------------------------------------------\n",
      "Entrada:         a gata branca correu .\n",
      "Saída gerada:    a gata branca correu .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.9033\n",
      "  ,            p=0.0071\n",
      "  branca       p=0.0068\n",
      "  santos       p=0.0067\n",
      "  gata         p=0.0056\n",
      "------------------------------------------------\n",
      "Entrada:         o cachorro branco dormiu .\n",
      "Saída gerada:    o cachorro branco dormiu .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.9022\n",
      "  santos       p=0.0078\n",
      "  ,            p=0.0066\n",
      "  branca       p=0.0054\n",
      "  gata         p=0.0053\n",
      "------------------------------------------------\n",
      "Entrada:         o gato dormiu .\n",
      "Saída gerada:    o gato dormiu .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.8890\n",
      "  gato         p=0.0093\n",
      "  ,            p=0.0085\n",
      "  santos       p=0.0069\n",
      "  dormiu       p=0.0062\n",
      "------------------------------------------------\n",
      "Entrada:         o cachorro pulou .\n",
      "Saída gerada:    o cachorro pulou .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.8980\n",
      "  santos       p=0.0075\n",
      "  branca       p=0.0071\n",
      "  cidade       p=0.0062\n",
      "  ,            p=0.0061\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PREDICT-ONLY\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Checagens mínimas (evita redefinir qualquer coisa)\n",
    "if \"model_txt\" not in globals():\n",
    "    raise RuntimeError(\"Não encontrei 'model_txt'. Rode o bloco de treino/instanciação anterior primeiro.\")\n",
    "\n",
    "for name in (\"encode_text_txt\", \"decode_ids_txt\", \"greedy_decode_txt\", \"vocab_txt\", \"ivocab_txt\", \"BOS\", \"EOS\", \"PAD\"):\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Dependência ausente: {name}. Rode o bloco anterior para definir.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_text(inputs, max_len=40, topk=None):\n",
    "    \"\"\"\n",
    "    inputs: str ou list[str] com frases já no vocabulário 'vocab_txt'.\n",
    "    max_len: limite de geração (tokens incluindo <eos>).\n",
    "    topk: se int (ex. 5), mostra top-k do último passo para cada frase.\n",
    "    \"\"\"\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "\n",
    "    dev = next(model_txt.parameters()).device\n",
    "    print(\"\\n================= PREDICT (TEXTO) =================\")\n",
    "    for s in inputs:\n",
    "        # encode -> greedy -> decode\n",
    "        src_ids = encode_text_txt(s, add_bos=True, add_eos=True, device=dev)\n",
    "        out_ids = greedy_decode_txt(model_txt, src_ids, max_len=max_len)\n",
    "        out_text = decode_ids_txt(out_ids)\n",
    "\n",
    "        print(f\"Entrada:         {s}\")\n",
    "        print(f\"Saída gerada:    {out_text}\")\n",
    "\n",
    "        # opcional: top-k do último passo\n",
    "        if isinstance(topk, int) and topk > 0:\n",
    "            # recalcula só o último passo para extrair probs\n",
    "            memory, src_mask, _ = model_txt.encode(src_ids)\n",
    "            ys = torch.tensor([[BOS]], device=dev, dtype=torch.long)\n",
    "            last_logits = None\n",
    "            for _ in range(max_len):\n",
    "                last_logits, _ = model_txt.decode(ys, memory, src_mask)  # (1, t, V)\n",
    "                step_logits = last_logits[:, -1, :]                       # (1, V)\n",
    "                next_id = step_logits.argmax(dim=-1, keepdim=True)        # (1,1)\n",
    "                ys = torch.cat([ys, next_id], dim=1)\n",
    "                if (next_id == EOS).all():\n",
    "                    break\n",
    "            if last_logits is not None:\n",
    "                probs = torch.softmax(step_logits, dim=-1)[0]             # (V,)\n",
    "                vals, idxs = torch.topk(probs, k=min(topk, probs.numel()))\n",
    "                toks = [ivocab_txt.get(i.item(), \"<unk>\") for i in idxs]\n",
    "                print(\"Top-{} do último passo:\".format(len(toks)))\n",
    "                for t, p in zip(toks, vals.tolist()):\n",
    "                    print(f\"  {t:<12} p={p:.4f}\")\n",
    "        print(\"-\" * 48)\n",
    "\n",
    "# =========================\n",
    "# EXEMPLOS DE USO\n",
    "# =========================\n",
    "testes = [\n",
    "    \"o gato preto correu .\",\n",
    "    \"a gata branca correu .\",\n",
    "    \"o cachorro branco dormiu .\",\n",
    "    \"o gato dormiu .\",\n",
    "    \"o cachorro pulou .\",\n",
    "]\n",
    "predict_text(testes, max_len=40, topk=5)\n",
    "\n",
    "# Ou testar uma única frase:\n",
    "# predict_text(\"o gato preto pulou .\", max_len=40, topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194475b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= PREDICT (TEXTO) =================\n",
      "Entrada:         o gato\n",
      "Saída gerada:    o gato preto pulou .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.9140\n",
      "  santos       p=0.0077\n",
      "  branca       p=0.0058\n",
      "  ,            p=0.0057\n",
      "  cruzeiro     p=0.0050\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "predict_text(\"o gato\", max_len=40, topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674596a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= PREDICT (TEXTO) =================\n",
      "[Aviso] Tokens fora do vocabulário: ['quem', 'mundo?']\n",
      "Entrada:         quem é o melhor time do mundo?\n",
      "Saída gerada:    o melhor time do mundo é o cruzeiro .\n",
      "Top-5 do último passo:\n",
      "  <eos>        p=0.9158\n",
      "  santos       p=0.0090\n",
      "  do           p=0.0060\n",
      "  branca       p=0.0049\n",
      "  cidade       p=0.0046\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "predict_text(\"quem é o melhor time do mundo?\", max_len=40, topk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fad1f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
