{"cells":[{"cell_type":"markdown","id":"c61fd253","metadata":{"id":"c61fd253"},"source":["# Aula 4.4 — Pré-treinamento (Pretraining)\n","\n","Nosso próximo modelo a ser observado é o **BERT** — _Bidirectional Encoder Representations from Transformers_ — o modelo que redefiniu o estado da arte em Processamento de Linguagem Natural (PLN) ao introduzir **pré-treinamento bidirecional profundo** baseado no encoder do Transformer.\n","\n","---\n","\n","## 1. Mas primeiro... Transfer Learning e Pré-Treinamento em Modelos de Linguagem\n","\n","**Transfer Learning (Aprendizado por Transferência)** é uma técnica em que o conhecimento adquirido em uma tarefa é **reaproveitado** para resolver **outra tarefa diferente, mas relacionada**.\n","\n","Em vez de treinar um modelo do zero (o que exige milhões de exemplos e recursos computacionais elevados), podemos **começar de um modelo já treinado em uma grande base de dados** e adaptá-lo para uma nova tarefa com poucos dados.\n","\n","### Analogia intuitiva\n","\n","Imagine que você aprende inglês e depois decide aprender espanhol.  \n","Você já tem um conhecimento prévio de estrutura gramatical, tempos verbais e alfabeto — então o aprendizado é **muito mais rápido**.  \n","O mesmo acontece com os modelos de linguagem: um modelo pré-treinado em grandes corpora já entende o “idioma” e o contexto geral do mundo, facilitando o aprendizado de tarefas específicas.\n","\n","---\n","\n","## 2. Por que o Transfer Learning é essencial em PLN?\n","\n","Antes do Transfer Learning, cada tarefa (tradução, classificação, resposta a perguntas) exigia um **modelo treinado do zero**.  \n","Isso era **ineficiente** e **custoso**.\n","\n","A revolução começou com o **Word2Vec** e o **GloVe**, que introduziram embeddings reutilizáveis de palavras.  \n","Mas esses métodos geravam vetores **estáticos** — ou seja, “banco” tinha o mesmo vetor em “banco de madeira” e “banco de dados”.\n","\n","O BERT e outros modelos baseados em Transformers levaram isso adiante, produzindo **representações contextuais dinâmicas**.\n","\n","> O mesmo token pode ter **significados diferentes**, dependendo do contexto.\n","\n","---\n","\n","## 3. O conceito de Pré-Treinamento (Pretraining)\n","\n","O **pré-treinamento** é a primeira etapa do aprendizado por transferência.\n","\n","Nesta fase, o modelo aprende **representações gerais da linguagem** sem depender de tarefas específicas.  \n","Ele é treinado com **objetivos auto-supervisionados**, ou seja, usa o próprio texto como “rótulo”.\n","\n","Exemplos de objetivos de pré-treinamento:\n","- **Linguagem mascarada (MLM)** — prever palavras ocultas.\n","- **Previsão de próxima sentença (NSP)** — entender a relação entre duas sentenças.\n","- **Causal Language Modeling (CLM)** — prever a próxima palavra (como no GPT).\n","\n","### Intuição\n","\n","Durante o pré-treinamento, o modelo aprende:\n","- Que tipo de palavras tendem a aparecer juntas.\n","- Que estrutura sintática as frases têm.\n","- Que relações semânticas existem entre conceitos (por exemplo, “rei” → “rainha”, “paris” → “frança”).\n","\n","Essas representações são **transferíveis** para novas tarefas.\n","\n","---\n","\n","## 4. Fine-Tuning: a etapa de especialização\n","\n","Depois de pré-treinar um modelo em uma base gigantesca (ex: Wikipedia, BooksCorpus), fazemos o **fine-tuning**, ajustando os pesos do modelo em uma tarefa específica, com poucos dados.\n","\n","| Etapa | Objetivo | Dados | Exemplo |\n","|:------|:----------|:-------|:--------|\n","| **Pré-treinamento** | Aprender linguagem geral | Bilhões de palavras | Wikipedia, Common Crawl |\n","| **Fine-tuning** | Adaptar a tarefa específica | Milhares de exemplos | Classificação, QA, NER |\n","\n","Essa combinação é poderosa porque:\n","- Reaproveita conhecimento linguístico universal.\n","- Reduz drasticamente custo e tempo de treinamento.\n","- Aumenta a precisão mesmo com datasets pequenos.\n","\n","---\n","\n","## 5. Transfer Learning no contexto de Transformers\n","\n","O artigo *Attention Is All You Need* (Vaswani et al., 2017) mostrou que o Transformer podia aprender dependências de longo alcance com eficiência.\n","\n","O BERT (Devlin et al., 2018) pegou essa arquitetura e **pré-treinou apenas o encoder** em uma tarefa de linguagem geral.  \n","Assim, o modelo aprendeu a capturar **relações bidirecionais** (olhando para a esquerda e para a direita) e **transferir esse conhecimento** para diferentes aplicações de PLN.\n","\n","---\n","\n","## 6. Benefícios práticos do pré-treinamento\n","\n","1. **Generalização**: o modelo entende padrões linguísticos universais.  \n","2. **Eficiência**: o fine-tuning é rápido e requer poucos dados.  \n","3. **Desempenho**: melhora significativa em benchmarks (GLUE, SQuAD, etc.).  \n","4. **Reuso**: o mesmo modelo base pode ser adaptado a múltiplas tarefas.  \n","5. **Aprendizado contínuo**: novos dados podem expandir o conhecimento sem recomeçar do zero.\n","\n","             ┌───────────────────────────┐\n","             │     Pré-Treinamento       │\n","             │ (Auto-supervisão, MLM/NSP)│\n","             └────────────┬──────────────┘\n","                          │\n","                          ▼\n","             ┌───────────────────────────┐\n","             │        Fine-Tuning         │\n","             │ (Tarefas específicas)      │\n","             └────────────┬──────────────┘\n","                          │\n","                          ▼\n","            Classificação | QA | NER | Tradução"]},{"cell_type":"markdown","id":"05c17e4e","metadata":{"id":"05c17e4e"},"source":["## 7. Estratégias de Pré-Treinamento em Modelos de Linguagem\n","\n","---\n","\n","### 7.1. Por que existem diferentes formas de pré-treino?\n","\n","Os modelos de linguagem podem aprender de maneiras distintas, dependendo **da direção da previsão**, **da unidade de predição** (palavra, subpalavra, token), e **do tipo de sinal supervisionado** disponível.\n","\n","O objetivo do pré-treinamento é sempre o mesmo:\n","> Aprender representações linguísticas gerais e contextuais, **sem precisar de rótulos humanos**.\n","\n","Mas existem **diversas formas** de fazer isso, cada uma com características próprias.  \n","A seguir, veremos as mais importantes, em ordem histórica e conceitual.\n","\n","#### 7.1.1 O que é auto-supervisão?\n","\n","**Aprendizado auto-supervisionado (Self-Supervised Learning)** é uma forma de aprendizado de máquina em que o próprio modelo **gera seus próprios rótulos** a partir dos dados brutos.\n","\n","Em vez de precisar de um conjunto de dados anotado manualmente (como em aprendizado supervisionado), o modelo cria **tarefas auxiliares artificiais**, que permitem aprender padrões e representações **de forma autônoma**.\n","\n","Imagine que você tenha milhões de frases sem rótulos.  \n","Você pode transformar esse texto em uma tarefa de aprendizado assim:\n","\n","> “Remova uma palavra aleatória e peça para o modelo adivinhar qual era.”\n","\n","O modelo agora tem:\n","- **Entrada:** frase com lacuna → “O gato ___ no sofá.”\n","- **Saída (rótulo):** palavra removida → “dorme”\n","\n","Nenhum humano precisou rotular nada — o dado forneceu o próprio sinal de aprendizado.  \n","Isso é **auto-supervisão**.\n","\n","#### 7.1.2 Por que a auto-supervisão é tão poderosa?\n","\n","Antes dela, os modelos de NLP dependiam de dados rotulados — algo escasso e caro.  \n","A auto-supervisão aproveita o **texto abundante** da internet (livros, artigos, Wikipedia, etc.),  \n","criando tarefas de aprendizado como:\n","\n","- **Prever palavras mascaradas** (BERT)\n","- **Prever a próxima palavra** (GPT)\n","- **Detectar sentenças incoerentes** (BERT)\n","- **Reconstruir texto corrompido** (T5, BART)\n","- **Distinguir tokens substituídos** (ELECTRA)\n","\n","Essas tarefas forçam o modelo a **entender a estrutura da linguagem** — gramática, semântica, coocorrência, relações sintáticas — sem que ninguém precise ensinar explicitamente.\n","\n","#### 7.1.3 Relação com outras formas de aprendizado\n","\n","| Tipo de aprendizado | Precisa de rótulos humanos? | Exemplo |\n","|----------------------|-----------------------------|----------|\n","| **Supervisionado** | ✅ Sim | Classificação de sentimentos, NER |\n","| **Não supervisionado** | ❌ Não | Clustering, autoencoders |\n","| **Auto-supervisionado** | ❌ Não (gera rótulos automaticamente) | MLM (BERT), CLM (GPT), RTD (ELECTRA) |\n","| **Reforço (RL)** | ⚙️ Recompensas, não rótulos diretos | RLHF no ChatGPT |\n","\n","#### 7.1.4 Auto-supervisão como fundação dos LLMs\n","\n","Modelos como **BERT**, **GPT**, **T5**, **RoBERTa** e **LLaMA** são todos treinados com **tarefas auto-supervisionadas**.  \n","O modelo observa texto cru, cria um “jogo” interno (prever partes ocultas, continuar o texto etc.) e, ao vencer esse jogo repetidas vezes, **aprende o idioma humano**.\n","\n","> Em outras palavras: o modelo aprende **sem que ninguém precise explicitar o que é “correto”** — o próprio texto fornece as pistas.\n","\n","\n","\n","---\n","\n","### 7.2. Linguagem causal (unidirecional)\n","#### *Causal Language Modeling (CLM)*\n","\n","Essa é a forma mais tradicional, usada em modelos como o **GPT**, **Transformer Decoder** e modelos de geração de texto.\n","\n","A ideia é simples:\n","> O modelo prevê a **próxima palavra**, dado o contexto anterior.\n","\n","#### Exemplo\n","\n","| Tipo | Entrada | Saída esperada |\n","|------|----------|----------------|\n","| CLM | “O gato dorme no” | “sofá” |\n","\n","O modelo aprende uma distribuição $ P(w_t \\mid w_1, w_2, ..., w_{t-1}) $.\n","\n","**Características:**\n","- É **autoregressivo** (gera um token por vez).\n","- Aprende a produzir texto coerente da esquerda para a direita.\n","- Ideal para **geração de texto**, **tradução** e **chatbots**.\n","\n","**Modelos que usam CLM:**  \n","GPT, GPT-2, GPT-3, LLaMA, Falcon, Mistral, etc.\n","\n","---\n","\n","### 7.3. Linguagem mascarada (bidirecional)\n","#### *Masked Language Modeling (MLM)*\n","\n","Introduzida pelo **BERT**, essa abordagem mascara aleatoriamente alguns tokens da entrada e pede que o modelo os reconstrua, usando **contexto dos dois lados** (esquerda e direita).\n","\n","#### Exemplo\n","\n","| Tipo | Entrada | Saída esperada |\n","|------|----------|----------------|\n","| MLM | “O gato [MASK] no sofá.” | “dorme” |\n","\n","**Características:**\n","- Permite **aprendizado bidirecional**, pois o modelo vê toda a sentença ao mesmo tempo.\n","- Excelente para tarefas de **compreensão de texto**, mas **não** para geração sequencial.\n","- Normalmente, 15% dos tokens são mascarados:\n","  - 80% viram `[MASK]`\n","  - 10% são substituídos por uma palavra aleatória\n","  - 10% ficam inalterados (para estabilidade)\n","\n","**Modelos que usam MLM:**  \n","BERT, RoBERTa, ALBERT, ELECTRA (em parte).\n","\n","---\n","\n","### 7.4. Modelos de substituição discriminativa\n","#### *Replaced Token Detection (RTD)*\n","\n","Usado no **ELECTRA**, é uma alternativa mais eficiente ao MLM.\n","\n","Em vez de prever diretamente a palavra mascarada, o modelo é treinado como um **discriminador**:\n","> Ele tenta identificar quais tokens foram **substituídos por palavras falsas** geradas por outro modelo.\n","\n","#### Exemplo\n","\n","| Tipo | Entrada | Tarefa |\n","|------|----------|--------|\n","| RTD | “O gato dorme no sofá.” (onde “sofá” foi trocado por “carro”) | Dizer se cada token é original ou substituído |\n","\n","**Características:**\n","- O modelo aprende com **todos os tokens**, e não só os mascarados.\n","- O treinamento é mais **eficiente** e **estável**.\n","- A saída é **binária** por token: verdadeiro (original) ou falso (substituído).\n","\n","**Modelo que usa RTD:**  \n","ELECTRA.\n","\n","---\n","\n","### 7.5. Treinamento de pares de sentenças\n","#### *Next Sentence Prediction (NSP)* e variantes\n","\n","Introduzido também no **BERT original**, o NSP ensina o modelo a **entender relações entre sentenças**.\n","\n","#### Exemplo\n","\n","| Entrada | Rótulo |\n","|----------|--------|\n","| (A) “O gato subiu na árvore.”<br>(B) “Ele não conseguia descer.” | É sequência (IsNext) |\n","| (A) “O gato subiu na árvore.”<br>(B) “O carro é azul.” | Não é sequência (NotNext) |\n","\n","**Características:**\n","- Útil para tarefas envolvendo **coerência entre sentenças** (ex: QA, inferência textual).\n","- Mas pesquisas posteriores (ex: RoBERTa) mostraram que o NSP **nem sempre melhora** o desempenho — e pode até atrapalhar.\n","\n","**Modelos que usam NSP:**  \n","BERT (usa), RoBERTa (remove), ALBERT (substitui por “Sentence Order Prediction”).\n","\n","---\n","\n","### 7.6. Linguagem permutada\n","#### *Permutation Language Modeling (PLM)*\n","\n","Introduzida pelo **XLNet**, combina o melhor dos dois mundos (CLM e MLM).\n","\n","> O modelo aprende a prever tokens mascarados, mas em **ordens aleatórias** — assim, mantém a bidirecionalidade **sem usar [MASK]**.\n","\n","#### Exemplo\n","\n","> Frase: “O gato dorme no sofá.”\n","\n","> Permuta possível: prever “no” a partir de “O, gato, dorme, sofá”.\n","\n","**Características:**\n","- Modelo bidirecional e generativo ao mesmo tempo.\n","- Evita o problema do `[MASK]`, que não aparece em textos reais.\n","- Mais complexo de treinar, mas muito poderoso.\n","\n","**Modelo que usa PLM:**  \n","XLNet.\n","\n","---\n","\n","### 7.7. Modelos seq2seq de reconstrução\n","#### *Denoising Autoencoding (DAE)*\n","\n","Usado no **T5** (Text-to-Text Transfer Transformer).\n","\n","> O modelo recebe uma entrada corrompida e tenta reconstruí-la **como uma sequência de saída completa**.\n","\n","#### Exemplo\n","\n","| Entrada (corrompida) | Saída |\n","|----------------------|-------|\n","| “O [MASK] preto correu [MASK].” | “O gato preto correu rápido.” |\n","\n","**Características:**\n","- Treina o modelo no formato “texto → texto”.\n","- Permite fine-tuning direto em várias tarefas textuais.\n","- Extremamente flexível (tradução, QA, sumarização, etc.).\n","\n","**Modelos que usam DAE:**  \n","T5, BART.\n","\n","---\n","\n","### 7.8. Comparativo entre abordagens\n","\n","| Tipo de pré-treino | Direcionalidade | Exemplo de modelo | Ideal para |\n","|---------------------|-----------------|-------------------|-------------|\n","| CLM (causal) | Unidirecional | GPT, LLaMA | Geração de texto |\n","| MLM (mascarado) | Bidirecional | BERT, RoBERTa | Compreensão |\n","| RTD (discriminativo) | Bidirecional | ELECTRA | Eficiência |\n","| NSP / SOP | Inter-sentença | BERT, ALBERT | Coerência textual |\n","| PLM (permuta) | Bidirecional + sequencial | XLNet | Contexto + geração |\n","| DAE (reconstrução) | Seq2Seq | T5, BART | Tradução, sumarização |\n","\n","---\n","\n","### Em geral...\n","\n","Não existe uma única “melhor” forma de pré-treino — a escolha depende da **tarefa final** e do **tipo de representação desejada**.\n","\n","- Para **compreender texto** → preferimos **MLM/RTD**.  \n","- Para **gerar texto** → usamos **CLM ou DAE**.  \n","- Para **reconhecer relações entre sentenças** → incluímos **NSP/SOP**.\n","\n","O BERT introduziu o **MLM + NSP**, consolidando o pré-treinamento bidirecional como paradigma dominante em modelos de compreensão textual."]},{"cell_type":"code","execution_count":null,"id":"79b422d1","metadata":{"id":"79b422d1"},"outputs":[],"source":["# ============================================\n","# BERT (MLM) — Pré-treinamento simples\n","# ============================================\n","\n","# ===== (Colab) Instalar libs =====\n","try:\n","    import google.colab  # type: ignore\n","    IN_COLAB = True\n","except Exception:\n","    IN_COLAB = False\n","\n","if IN_COLAB:\n","    !pip -q install -U \"transformers>=4.39\" \"accelerate>=0.28\" \"datasets>=2.14\" \"pyarrow>=14\"\n","\n","import os, math, random, inspect\n","import torch\n","\n","SEED = 42\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","# ---------------------------\n","# Configurações gerais\n","# ---------------------------\n","MODEL_NAME  = \"bert-base-uncased\"    # ponto de partida (continual pretraining)\n","OUTPUT_DIR  = \"bert-mlm-output\"\n","BLOCK_SIZE  = 128\n","MLM_PROB    = 0.15\n","EPOCHS      = 1\n","BATCH_TRAIN = 16\n","BATCH_EVAL  = 16\n","USE_RANDOM_INIT = False  # True = treinar do zero (didático; mais lento)\n","\n","# ---------------------------\n","# Utilitários de compatibilidade\n","# ---------------------------\n","from transformers import TrainingArguments, Trainer\n","import transformers\n","\n","print(\"transformers version:\", transformers.__version__)\n","\n","def build_training_arguments(**kwargs):\n","    \"\"\"\n","    Cria TrainingArguments aceitando somente os parâmetros suportados\n","    na versão instalada do transformers.\n","    \"\"\"\n","    sig = inspect.signature(TrainingArguments.__init__)\n","    allowed = set(sig.parameters.keys()); allowed.discard(\"self\")\n","    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n","    dropped = [k for k in kwargs if k not in allowed]\n","    if dropped:\n","        print(\"[Aviso] Parâmetros ignorados (não suportados nesta versão):\", dropped)\n","    return TrainingArguments(**filtered)\n","\n","# ---------------------------\n","# Tokenizer e Modelo\n","# ---------------------------\n","from transformers import (\n","    BertConfig, BertTokenizerFast, BertForMaskedLM,\n","    DataCollatorForLanguageModeling\n",")\n","\n","tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n","\n","if USE_RANDOM_INIT:\n","    print(\">> Inicializando BERT do zero (pesos aleatórios).\")\n","    config = BertConfig.from_pretrained(MODEL_NAME)\n","    model  = BertForMaskedLM(config)\n","else:\n","    print(\">> Carregando BERT pré-treinado para continual pretraining.\")\n","    model  = BertForMaskedLM.from_pretrained(MODEL_NAME)\n","\n","model.to(device)\n","\n","# ============================================================\n","# TENTATIVA 1: usar datasets (WikiText-2)\n","# ============================================================\n","USE_FALLBACK = False\n","train_dataset = None\n","eval_dataset  = None\n","data_collator = None\n","\n","try:\n","    from datasets import load_dataset\n","    raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","    print(raw)\n","\n","    def tokenize_function(examples):\n","        return tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n","\n","    tokenized = raw.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","\n","    def group_texts(examples):\n","        concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n","        total_length = len(concatenated[\"input_ids\"])\n","        total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n","        result = {\n","            k: [t[i: i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n","            for k, t in concatenated.items()\n","        }\n","        return result\n","\n","    lm_datasets = tokenized.map(group_texts, batched=True)\n","    train_dataset = lm_datasets[\"train\"]\n","    eval_dataset  = lm_datasets[\"validation\"]\n","\n","    print(\"Tamanhos — train:\", len(train_dataset), \" | eval:\", len(eval_dataset))\n","\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB\n","    )\n","\n","except Exception as e:\n","    print(\"\\n[AVISO] Falha ao usar `datasets`/pyarrow:\", repr(e))\n","    print(\"Ativando FALLBACK para um corpus simples em memória.\")\n","    USE_FALLBACK = True\n","\n","# ============================================================\n","# FALLBACK: corpus simples em memória (sem datasets/pyarrow)\n","# ============================================================\n","if USE_FALLBACK:\n","    corpus_text = \"\"\"\n","    Deep learning has significantly advanced the state of the art in natural language processing.\n","    Transformers enable parallel training and capture long-range dependencies effectively.\n","    Self-supervised pretraining allows models to learn from raw text without human labels.\n","    BERT uses masked language modeling to learn bidirectional context.\n","    Large corpora such as Wikipedia and BooksCorpus are commonly used for pretraining.\n","    Neural networks trained on vast amounts of data acquire powerful representations.\n","    \"\"\".strip().splitlines()\n","    corpus_text = [c.strip() for c in corpus_text if c.strip()]\n","\n","    enc = tokenizer(corpus_text, return_special_tokens_mask=False)\n","\n","    def chunkify(encodings, block_size):\n","        ids = sum(encodings[\"input_ids\"], [])\n","        att = sum(encodings[\"attention_mask\"], [])\n","        total = (len(ids) // block_size) * block_size\n","        ids = [ids[i:i+block_size] for i in range(0, total, block_size)]\n","        att = [att[i:i+block_size] for i in range(0, total, block_size)]\n","        samples = []\n","        for x, a in zip(ids, att):\n","            samples.append({\n","                \"input_ids\": torch.tensor(x, dtype=torch.long),\n","                \"attention_mask\": torch.tensor(a, dtype=torch.long)\n","            })\n","        return samples\n","\n","    chunks = chunkify(enc, BLOCK_SIZE)\n","\n","    class ListDataset(torch.utils.data.Dataset):\n","        def __init__(self, data): self.data = data\n","        def __len__(self): return len(self.data)\n","        def __getitem__(self, idx): return self.data[idx]\n","\n","    # usa 80/20 como split simples\n","    split = int(0.8 * len(chunks)) if len(chunks) > 1 else 1\n","    train_dataset = ListDataset(chunks[:split])\n","    eval_dataset  = ListDataset(chunks[split:])\n","\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB\n","    )\n","\n","# ============================================================\n","# TrainingArguments + Trainer\n","# ============================================================\n","args = build_training_arguments(\n","    output_dir=OUTPUT_DIR,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_steps=50,\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=BATCH_TRAIN,\n","    per_device_eval_batch_size=BATCH_EVAL,\n","    gradient_accumulation_steps=1,\n","    fp16=torch.cuda.is_available(),\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    max_grad_norm=1.0,\n","    report_to=\"none\",\n","    push_to_hub=False,\n","    seed=SEED,\n","    save_steps=500,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset if len(eval_dataset) > 0 else None,\n","    data_collator=data_collator,\n",")\n","\n","# ============================================================\n","# Treino + Avaliação\n","# ============================================================\n","train_out = trainer.train()\n","\n","eval_out = {}\n","try:\n","    eval_out = trainer.evaluate() if (trainer.eval_dataset is not None) else {}\n","except Exception as e:\n","    print(\"[Aviso] Avaliação automática indisponível nesta versão:\", e)\n","\n","if \"eval_loss\" in eval_out:\n","    eval_loss = float(eval_out[\"eval_loss\"])\n","    ppl = math.exp(eval_loss) if eval_loss < 20 else float(\"inf\")\n","    print(f\"\\nEval loss: {eval_loss:.4f}\")\n","    print(f\"Perplexity: {ppl:.2f}\")\n","else:\n","    print(\"\\n[Info] Sem eval_loss reportado pela API/execução atual.\")\n","\n","# ============================================================\n","# Salvar modelo/tokenizer\n","# ============================================================\n","try:\n","    trainer.save_model(OUTPUT_DIR)\n","    tokenizer.save_pretrained(OUTPUT_DIR)\n","    print(f\"\\nModelo salvo em: {OUTPUT_DIR}\")\n","except Exception as e:\n","    print(\"[Aviso] Não foi possível salvar modelo/tokenizer:\", e)\n","\n","# ============================================================\n","# Teste rápido: fill-mask\n","# ============================================================\n","from transformers import pipeline\n","device_idx = 0 if torch.cuda.is_available() else -1\n","\n","try:\n","    fillmask = pipeline(\"fill-mask\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR, device=device_idx)\n","except Exception as e:\n","    print(\"[Aviso] fallback: usando instância em memória (sem salvar/carregar).\", e)\n","    fillmask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer, device=device_idx)\n","\n","tests = [\n","    \"The cat [MASK] on the mat.\",\n","    \"Deep learning has [MASK] the state of the art.\",\n","    \"Paris is the [MASK] of France.\"\n","]\n","\n","print(\"\\n== Fill-Mask examples ==\")\n","for s in tests:\n","    try:\n","        preds = fillmask(s)\n","        print(f\"\\nInput: {s}\")\n","        for p in preds[:5]:\n","            print(f\"  -> {p['sequence']}  (p={p['score']:.4f})\")\n","    except Exception as e:\n","        print(f\"\\nInput: {s}\")\n","        print(\" Erro:\", e)"]},{"cell_type":"code","execution_count":null,"id":"62992429","metadata":{"id":"62992429"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"env-misc","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}